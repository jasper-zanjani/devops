{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Automation using Azure DevOps and Ansible","text":"<p>Azure Pipelines are used to automate the building of source code, including executing associated tasks like unit tests and packaging. Every execution of a pipeline, or a run, produces an artifact.</p> <p>Pipelines makes use of several concepts.</p> <ul> <li>A pipeline is started by a trigger </li> <li>A pipeline is made up of one or more stages, each of which can have one or more jobs</li> <li>Each job runs on an agent and can be made of one or more steps or tasks which are run sequentially.</li> <li>A task refers to a predefined action</li> </ul> <p>The simplest possible pipeline is a YAML file with a single line defining a trigger.  Creating a new repo with a single YAML file with only the trigger keyword defined will allow a Pipeline to then be created. But attempting to run it will produce an error reading \"The pipeline must contain at least one stage with no dependencies.\"</p> pipeline.yml<pre><code>trigger: none\n</code></pre> <p>Simple pipelines can omit the stages and jobs container and directly specify the steps keyword. In this case the pipeline is said to have a single implicit stage, as well as a single implicit job.</p> <p>This single-stage, single-job pipeline will place a short message in the user directory of the service account of a self-hosted agent.</p> <pre><code>trigger: none\npool: Hyper-V # (1)!\nsteps:\n- checkout: self  \n- bash: | # (2)!\n    echo \"Hello, World!\" &gt; ~/hello\n</code></pre> <ol> <li>Without the pool property, ADO will allocate a VM from the cloud.</li> <li>The pipe symbol here represents the block style indicator, one of many formats supported by YAML for multiline strings.</li> </ol> <p>Here the stages and jobs lists are made explicit. </p> <pre><code>trigger: none\npool: Hyper-V\nstages:\n- stage: helloWorldStage # (1)\n  jobs:\n  - job: helloWorldJob\n    steps:\n    - checkout: self  \n    - bash: |\n        echo \"Hello, World!\" &gt; ~/hello\n</code></pre> <ol> <li>Values for stages.stage and jobs.job must be an alphanumeric string with no spaces. Both also expose an optional displayName property that appears in the web interface.</li> </ol> <p>This example can be developed further to provide the ability to choose between agent pools using templates, which define reusable content such as parameters.</p> <p>Parameters must contain a name and data type, for example this string. Parameters are references using the ${{ ... }} syntax. </p> <pre><code>trigger: none\nparameters:\n- name: name\n  type: string\n  default: World\njobs:\n- job: helloWorldJob\n  steps:\n  - checkout: self  \n  - bash: echo \"Hello, ${{ parameters.name  }}!\" &gt; ~/hello\n</code></pre> <p>Enums, rendered as dropdowns, are defined using a list of values placed under the values key.</p> <pre><code>trigger: none\nparameters:\n- name: pool\n  default: Home\n  values:\n  - Home\n  - Work\n- name: name\n  type: string\n  default: World\njobs:\n- job: helloWorld\n  pool: ${{ parameters.pool }} # (1)!\n  steps:\n  - checkout: self  \n  - bash: |\n      echo \"Hello, ${{ parameters.name }}!\" &gt; ~/hello\n</code></pre> <ol> <li>It is also possible to specify the agent pool at stages.stage: <pre><code>stages:\n- stage: helloWorldStage\n  pool: ${{ parameters.pool }}\n  jobs:\n# ...\n</code></pre></li> </ol> <p>/etc/motd contains a message that is displayed to users who login for the first time in that day.  If the ADO agent is also an Ansible control host, with properly defined sudo permissions, it can be used to set the motd on a managed host.</p> <p>Ensure that the control node has privilege escalation enabled.</p> <pre><code>trigger: none\nparameters:\n- name: pool\n  default: Home\n  values:\n  - Home\n  - Work\n- name: name\n  type: string\n  default: World\njobs:\n- job: helloWorldJob\n  pool: ${{ parameters.pool }}\n  steps:\n  - checkout: self\n  - bash: |\n      ansible all -m copy -a 'dest=/etc/motd content=\"Hello, ${{ parameters.name }}!\"'\n</code></pre> <p>Instead of running an Ansible ad-hoc command, we can create a role, which groups content in a way that allows Ansible content to be shared.</p> <p>Here, the pipeline executes the motd-role role which is specified in the requirements file and defined in a separate repo. The pipeline parameter is passed to the playbook via the --extra-vars option.</p> <pre><code>trigger: none\nparameters:\n- name: pool\n  default: Home\n  values:\n  - Home\n  - Work\njobs:\n- job: HelloWorldJob\n  pool: ${{ parameters.pool }}\n  steps:\n  - checkout: self\n  - bash: |\n      ansible-galaxy role install -r ansible/requirements.yml -p ansible/roles -f \n      ansible-playbook ansible/playbook.yml \\\n        --extra-vars \"greet_name=${{parameters.name}}\"\n# (1)!\n</code></pre> <ol> <li>ansible/playbook.yml<pre><code>- name: Running motd role\n  hosts: all\n  roles:\n  - role: 'motd-role'\n</code></pre> ansible/requirements.yml<pre><code>roles:\n- src: git+https://jasperzanjani@dev.azure.com/jasperzanjani/NewDevOpsProject/_git/motd-role\n</code></pre></li> </ol> <p>The extends keyword can be used to remove complexity from a pipeline. Parameters defined in the parent must be passed to the child explicitly, and they must be defined again within the child to make them available to any template experssions. This makes it possible to abstract the frontend of parameter definitions from the backend of build logic.</p> pipeline.yml<pre><code>trigger: none\nparameters:\n- name: pool\n  default: Home\n  values:\n  - Home\n  - Work\n- name: name\n  type: string\n  default: World\nextends:\n  template: jobs.yml\n  parameters:\n    pool: ${{ parameters.pool }}\n    name: ${{ parameters.name }}\n</code></pre> jobs.yml<pre><code>parameters:\n- name: pool\n- name: name\njobs:\n- job: helloWorldJob\n  pool: ${{ parameters.pool }}\n  steps:\n  - checkout: self\n  - bash: |\n      ansible-galaxy role install -r ansible/requirements.yml -p ansible/roles -f \n      ansible-playbook ansible/playbook.yml \\\n        --extra-vars \"greet_name=${{parameters.name}}\"\n</code></pre> <p>Secure files are one of two types of files that can be made available via the Library. Secure files provide a way to store files that can be shared across pipelines, especially security-related items like signatures and keys.</p> <p>In order to consume a secure file in a pipeline, the DownloadSecureFile task task is used. Here it is used to make a private SSH key available on the agent.</p> <pre><code>- task: DownloadSecureFile@1\n  name: sshkey\n  inputs:\n    secureFile: ansible@hyperv-ubuntu2004\n</code></pre> <p>The secure file can then be used in the following line to make the key available to Ansible. Note that the template syntax $( ... ) differs from the template syntax used for pipelines parameters.</p> <pre><code>- bash: |\n    eval $(ssh-agent); ssh-add &lt;(cat \"$(sshkey.secureFilePath)\")\n    ansible-galaxy role install -r ansible/requirements.yml -p ansible/roles -f \n    ansible-playbook ansible/playbook.yml \\\n      --extra-vars \"greet_name=${{parameters.name}}\"\n    pkill ssh-agent \n# (1)!\n</code></pre> <ol> <li>Repeated runs of the pipeline will continue to start new instances of ssh-agent, so an additional line killing the process at the end of the pipline is good form.</li> </ol> <p>An Ansible vault password file can be placed in Pipelines as a secure file.  This file is downloaded to the agent using the DownloadSecureFile task and can be used in downstream tasks, such as the argument to the --vault-password-file option, using the secureFilePath output variable.</p> pipeline.yaml<pre><code>trigger: none\nparameters:\n- name: pool\n  displayName: Agent pool\n  default: Home\n  values:\n  - Home\n  - Work\njobs:\n- task: DownloadSecureFile@1\n  name: sshkey\n  inputs:\n    secureFile: ansible@hyperv-ubuntu2004\n- task: DownloadSecureFile@1\n  name: vaultpw\n  inputs:\n    secureFile: vault-pw\n- job: HelloWorldJob\n  pool: ${{ parameters.pool }}\n  steps:\n  - checkout: self\n  - bash: |\n      eval $(ssh-agent); ssh-add &lt;(cat \"$(sshkey.secureFilePath)\")\n      ansible-galaxy role install -r ansible/requirements.yml -p ansible/roles\n      ansible-playbook ansible/playbook.yml \\\n        --extra-vars \"greet_name=${{parameters.name}}\" \\\n        --vault-password-file=$(vaultpw.secureFilePath)  \n      pkill ssh-agent \n# (1)!\n</code></pre> <ol> <li>The magic variable role_path points to the path of the currently running role. roles/motd-role/tasks/main.yml<pre><code>---\n# tasks file for motd\n- include_vars:\n   file: \"{{ role_path }}/defaults/main.yml\" # (1)\n- copy:\n    dest: /etc/motd\n    content: Hello, {{ greet_name }}!\n  notify: Confirm motd\n</code></pre> Here, the encrypted value for the greet_name variable is encrypted inline in an otherwise unencrypted vars file. roles/motd-role/defaults/main.yml<pre><code>---\n# defaults file for motd\ngreet_name: !vault |\n  $ANSIBLE_VAULT;1.1;AES256\n  32306266363035376539336165613665393533653331363063303630353737633965646634356233\n  3761346166386235336362623435653264336435623261610a313864346535343534616530313461\n  61656438633862333038376239343132616537623664633536306264653636333835633735353531\n  6331623962383261340a666330666438613764636162353831356432623461386437313963663333\n  3137\n</code></pre></li> </ol>"},{"location":"#trigger","title":"trigger","text":"<p>A push trigger specifies which branches cause a continuous integration build to run. trigger: none disables CI triggers.</p> <p>Scheduled triggers can be defined with the  schedules key.</p> <pre><code>trigger: none\nschedules:\n- cron: \"0 3 * * *\"\n  displayName: Daily 3 AM scan\n  branches:\n    include:\n    - main\n  always: true # (1)\n</code></pre> <ol> <li>always ensures that the pipeline runs even when there are no code changes.</li> </ol>"},{"location":"#resources","title":"resources","text":"<pre><code>resources:\n  repositories:\n  - repository: network_bootstrap\n    type: git\n    name: devops_network-pipeline-templates\n    ref: refs/heads/master\n</code></pre> <p>Files from this repository can now be referenced by the repository name after @</p> <p><code>yaml stages: - stage: KickstartISO   displayName: Generate Kickstart   variables:   - template: .connect_vars.yml@network_bootstrap     parameters:       deploy_net: ${{ parameters['deploy_net'] }}</code>?</p>"},{"location":"ADO/","title":"Azure Devops","text":""},{"location":"ADO/#agent","title":"Agent","text":"<p>An agent represents compute infrastructure with installed agent software. Agents can be Microsoft-hosted (i.e. Azure VMs created specifically for the job and discarded after use) or self-hosted. Agents are organized into pools; an agent instance can only belong to a single pool, unless more than one agent is installed.</p> <p>The agent software package contains several shell scripts that provide various ways of running and managing the agent.</p> <ul> <li>config.sh must be run to configure the agent after installation by providing the server URL and PAT token.</li> <li>run.sh allows manual, interactive execution of the agent software</li> <li>svc.sh allows management of the agent software as a SystemD service. The service itself is named according to the pattern vsts.agent.[ORGANIZATION].[AGENTPOOL].[AGENTNAME].service.</li> </ul> Self-hosted agent setup <p>Because the agent software itself is based on .NET Core 3.1, some operating systems (such as Ubuntu 22.04) are not compatible. Some like CentOS 9 have an unsupported version of OpenSSL installed, which results in the configuration script producing a libssl error. CentOS 9 provides OpenSSL 1.1.1k libraries in a separate package:</p> <pre><code>dnf install compat-openssl11\n</code></pre> <p>Also note it appears that the git utility needs to be installed on Red Hat derivatives like CentOS, although it doesn't appear to be explicitly installed by the installdependencies.sh script. This may be because git is assumed to exist on Ubuntu.</p> <p>The agent software package must be downloaded from ADO, and a personal access token must be created with the Agent Pools (read, manage) scope.</p> <pre><code>wget \"https://vstsagentpackage.azureedge.net/agent/3.218.0/vsts-agent-linux-x64-3.218.0.tar.gz\"\nmkdir agent; cd agent\ntar xfz \"vsts-agent-linux-x64-3.218.0.tar.gz\"\n\n# Configure agent\n./config.sh\n\n# Install systemctl service\nsudo ./svc.sh install\n\n# Start systemctl service\nsudo ./svc.sh run\n</code></pre>"},{"location":"ADO/#azure-cli","title":"Azure CLI","text":"<p>There is an Azure DevOps extension for the Azure CLI.</p> <pre><code># Installation and configuring defaults\naz extension add --name azure-devops\naz devops configure --defaults organization=$ORG\naz devops configure --defaults project=$PROJECT\n\n# Display users\naz devops user list --organization $ORG\n\n# Display a single user\naz devops user show --organization $ORG --user $USER\n\n# Upgrade Azure CLI\naz upgrade # (1)!\n</code></pre> <ol> <li><pre><code># The extension used to need to be updated separately from the core Azure CLI\naz extension list-versions --name azure-devops\naz extension update --name azure-devops \n</code></pre></li> </ol> <p>ADO offers the opportunity to create wikis for repos (\"codewiki\") or projects (\"projectwiki\"). These can also be done through the CLI.</p> ADO wikis<pre><code>az devops wiki list\n</code></pre> <p>A separate command group for Azure Pipelines is installed with the azure-devops extension, with limited functionality.</p> <pre><code>az pipelines runs list --query '[*].result' # (1)\naz pipelines run --id 1 --parameters \"name=Dgiapusccu pool=Work\" # (2)\naz pipelines delete --id 1\n</code></pre> <ol> <li>Like other JSON output from the Azure CLI, JMESPATH queries can be passed to the --query option to filter results.</li> <li>The --parameters options appears to be relatively new and buggy. The help indicates that multiple space-delimited key-value pairs should be able to be passed, however this appears not to be the case.</li> </ol>"},{"location":"ADO/#force-push","title":"Force push","text":"<p>Git force-push is specifically disabled for the main branch by default. This setting can only be changed by setting branch permissions that take effect for the main branch across the entire organization.</p>"},{"location":"Ansible/","title":"Ansible","text":"Projects for learning <p>There are [several areas][https://opensource.com/article/19/8/ops-tasks-ansible] where Ansible can be used in personal projects for learning purposes. </p> <ol> <li>Use the users module to manage users, assign groups, and define custom aliases in the <code>profile</code> property.</li> <li>Put a time limit on the availability of the <code>sudo</code> command</li> <li>Use Ansible Tower to produce a GUI interface to restart certain services.</li> <li>Use Ansible Tower to look for files larger than a particular size in a directory.</li> <li>Debug a system performance problem. </li> </ol> <p>Ansible is an automation tool used for configuration management using human-readable YAML templates.  Ansible is distinguished for being agentless, meaning no special software is required on the nodes it manages.</p> <p>Ansible can be used in one of two ways:</p> <ul> <li>Running ad hoc commands, executed in realtime by an administrator working at the terminal using the ansible command</li> <li>Running playbooks, YAML documents that represent a sequence of scripted actions which apply changes uniformly over a set of hosts, using the ansible-playbook  command. </li> </ul> <p>A playbook is a YAML document that represents a sequence of scripted actions called tasks which apply changes uniformly over a set of hosts. Any ad hoc command can be rewritten as a playbook, but some modules can only be used effectively as playbooks.</p> playbooks/motd.yml<pre><code>- hosts: all\n  tasks:\n  - copy:\n    dest: /etc/motd\n    content: \"Hello, World!\"\n</code></pre> <p>Ansible host management relies on an inventory file, an INI or YAML format file containing a list of servers or nodes that can be organized in groups. Inventories are conventionally organized as a file named hosts at the root of a project directory, although a system hosts file can also be defined at /etc/ansible/hosts.</p>"},{"location":"Ansible/#variables","title":"Variables","text":"<p>Variables can be defined under vars (as properties), and they are referenced using Jinja2-style double braces: {{ }}.  YAML syntax requires a value starting with double braces to be quoted.</p> playbooks/motd.yml<pre><code>- hosts: all\n  vars:\n    name: World\n  tasks:\n  - command: echo \"Hello, {{ name }}!\"\n</code></pre> <p>Variables can also be defined in variables files, YAML-format dictionaries conventionally placed in the vars directory, and referenced using the vars_files property. The path for vars files appears to be interpreted relative to the location of the playbook.</p> <p>playbooks/motd.yml<pre><code>- hosts: all\n  vars_files:\n  - vars/name.yml\n  tasks:\n  - copy:\n    dest: /etc/motd\n    content: Hello, {{ greet_name }}!\n</code></pre> playbooks/vars/name.yml<pre><code>greet_name: World\n</code></pre></p> <p>Variables can also be defined at runtime using the --extra-vars/-e option. Variables can be passed as space-delimited or JSON format.</p> <pre><code>ansible-playbook release.yml -e \"version=1.23.45 other_variable=foo\"\n</code></pre> <p>Variables cane be encrypted inline in an otherwise cleartext vars file.</p>"},{"location":"Ansible/#loops","title":"Loops","text":"<p>Here is an unusual example of a loop that uses the control variable to successively delete, then create a directory</p> <pre><code>- name: Regen Kickstart dir\n  file:\n    path: \"{{ isopath }}/ks\"\n    state: \"{{ stmp }}\"\n  loop:\n    - absent\n    - directory\n  loop_control:\n    loop_var: stmp\n</code></pre>"},{"location":"Ansible/#jinja2","title":"Jinja2","text":"<p>Various effects are possible using Jinja2 templates:</p> <p>Jinja2 control structures support control flow features like loops and conditionals inside {% ... %} blocks.</p> <pre><code>- name: Find any YUM/DNF variables\n  find:\n    paths: \"/etc/{% 'dnf' if ansible_distribution_major_version == '8' else 'yum' %}/vars\"\n  register: _repository_vars_files\n</code></pre> <p>Filters follow a pipe in the template.</p> Capitalize a value<pre><code>line: \" {{ hypervisor | upper }}\n</code></pre> <p>Ansible provides additional filters. Here both the basename, b64decode, and combine Ansible filters are used as well as trim which is native to Jinja2. <pre><code>- set_fact:\n    repository_vars: \"{{ (repository_vars | default({})) | combine({ (_file.source | basename): _file.content | b64decode | trim }) }}\"\n  loop: \"{{ _repository_vars_slurped_files.results }}\"\n  loop_control:\n    loop_var: _file\n</code></pre></p>"},{"location":"Ansible/#roles","title":"Roles","text":"<p>Ansible roles group content in a way that allows it to be shared. They typically correspond to the service offered (web servers or databases, etc).</p> <p>Roles have a highly standardized directory structure.</p> <ul> <li>defaults: default values for variables with low perecedence that can be overriden by inventory variables</li> <li>files: static files referenced by role tasks</li> <li>handlers: handler definitions</li> <li>meta: metadata about the role, such as author, license, platforms, dependencies, etc</li> <li>tasks: task definitions</li> <li>vars: role variables with high precedence that cannot be overriden by inventory variables</li> </ul> <p>A skeleton directory can be created with ansible-galaxy. </p> <pre><code>ansible-galaxy init $ROLENAME\n</code></pre> <p>Roles can be called in a playbook under the roles property. The value of the role is interpreted as a path, appended to the project directory or various other potential locations. Because roles are meant to be reused by many playbooks, a central location is recommended:</p> <ul> <li>~/.ansible/roles</li> <li>/etc/ansible/roles</li> <li>/usr/share/ansible/roles</li> </ul> playbooks/motd.yml<pre><code>- hosts: all\n  roles:\n  - role: roles/motd\n    vars:\n      greet_name: Dgiapusccu # (1)\n</code></pre> <ol> <li>Without providing an overriding variable value: <pre><code>- hosts: all\n  roles: \n  - roles/motd\n</code></pre></li> </ol> motd role<pre><code># motd/tasks/main.yml\n- copy:\n    content: Hello, {{ greet_name }}!\n    dest: /etc/motd\n\n# motd/defaults/main.yml\ngreet_name: World # (1)\n</code></pre> <ol> <li>Role variables defined in vars have a high precedence and cannot be overriden. Only values defined in defaults can be overriden.</li> </ol> <p>It appears that variables with values defined in the main.yml file located vars or defaults are automatically picked up. But if variables are defined in additional files they must be explicitly imported.</p> motd/tasks/main.yml<pre><code>- include_vars:\n    file: \"{{ role_path }}/defaults/secure.yml\"\n- copy:\n    content: Hello, {{ greet_name }}!\n    dest: /etc/motd\n</code></pre> <p>Normally, tasks in a role execute before the other tasks of a playbook. pre_tasks and post_tasks can be defined as well. </p> <p>Roles can have dependencies on other dependencies, as defined in the meta directory.</p> <pre><code>dependencies:\n- { role: apache, port: 80 }\n- { role: mariadb, dbname: addresses, admin_user: bob }\n</code></pre>"},{"location":"Ansible/#collections","title":"Collections","text":"<p>Ansible collections comprise a standardized format for Ansible content distribution, allowing it to be delivered asynchronously and on-demand separately from Ansible Automation Platform releases. Ansible content can include playbooks, modules, roles, documentation, tests, plugins. Ansible collections are delivered using Ansible Galaxy and each collection needs a galaxy.yml file that describes the collection.</p> <p>Collections can be installed from a YAML-format requirements file:</p> <pre><code>ansible-galaxy collection install -r ansible/requirements.yml\n</code></pre> ansible/requirements.yml<pre><code>roles:\n- src: git@ssh.dev.azure.com:v3/PODS-LLC/SWE/devops_inventory_role\n  version: master # (2)\n  scm: git\n  name: inventory # (1)\n</code></pre> <ol> <li>This apparently determines how the directory is renamed.</li> <li>Branch name</li> </ol>"},{"location":"Ansible/#handlers","title":"Handlers","text":"<p>Handlers are tasks that are executed when notified by a task. They are only run once, and only if the notifying task has made a change to the system. </p> <p>Here, Enable Apache will be called if Install Apache makes a change.  If apache2 is already installed, the handler is not called. </p> <pre><code>- hosts: webservers\n  become: yes\n  tasks:\n  - name: Install Apache\n    apt: name=apache2 update_cache=yes state=latest\n    notify: enable apaches\n\n  handlers:\n  - name: Enable Apache\n    service: name=apache2 enabled=yes state=started\n</code></pre> <pre><code>- hosts: all\n  vars:\n    package_name: apache2\n  tasks:\n  - name: this installs a package\n    apt: \"name={{ package_name }} update_cache=yes state=latest\"\n    notify: enable apache\n  handlers:\n  - name: enable apache\n    service: \"name={{ package_name }} enabled=yes state=started\" \n</code></pre> <p>Conditional logic is implemented on each task by defining a value for the when statement:</p> <pre><code>- hosts: all\n  vars:\n    startme: true\n  tasks:\n  - command: echo Hello, World!\n    when: startme\n</code></pre>"},{"location":"Ansible/#vault","title":"Vault","text":"<p>Ansible Vault is a place to safely keep passwords. There are two types of vaulted content:</p> <ul> <li>Vaulted files, where the full file, which can contain Ansible variables or other content, is encrypted</li> <li>Single encrypted variables, where only specific variables within a normal \"variable file\" are encrypted.</li> </ul> Run a playbook providing a vault password file<pre><code>ansible-playbook --vault-password-file $pwfile playbooks/motd.yml\n</code></pre>"},{"location":"Ansible/#tasks","title":"Tasks","text":""},{"location":"Ansible/#setup","title":"Setup","text":"<p>The Ansible control node needs to be configured to elevate privileges. This is done by modifying ansible.cfg in the relevant project directory or the system config at /etc/ansible/ansible.cfg</p> ansible.cfg<pre><code>[privilege_escalation]\nbecome=yes\n</code></pre> <p>An Ansible service account is created on each managed node.</p> <pre><code>useradd ansible -s /usr/bin/bash -mG wheel # sudo\npasswd ansible\nsu - ansible\nssh-keygen -A\n</code></pre> <p>Now the service account is given the ability to sudo any command without a password.</p> /etc/sudoers.d/ansible<pre><code># Without this line, plays will fail with the message \"Missing sudo password\" \nansible ALL=(ALL) NOPASSWD: ALL\n</code></pre> <p>The system inventory is an INI-format config located at etc/ansible/hosts and defines the clients which are to be controlled by the server.</p> <p>The group name all is implicitly defined, and the following command will display all defined hosts.</p> Display all available hosts<pre><code>ansible all --list-hosts\n</code></pre>"},{"location":"Ansible/#apache","title":"Apache","text":"<pre><code>- hosts: all\n  tasks:\n  - package:\n      name: httpd\n      state: latest\n  - service:\n      name: httpd\n      enabled: true\n      state: started\n  - ansible.posix.firewalld:\n      immediate: true\n      permanent: true\n      service: http\n      state: enabled\n</code></pre> <p>Using vars<pre><code>- name: Install Apache\n  hosts: all\n  vars:\n    apache_package: httpd\n  tasks:\n  - name: Install {{ apache_package }} package\n    package:\n      name: \"{{ apache_package }}\"\n      state: latest\n  - name: Enable and start {{ apache_package }} service\n    service:\n      name: \"{{ apache_package }}\"\n      enabled: true\n      state: started\n  - name: Open firewall\n    ansible.posix.firewalld:\n      immediate: true\n      permanent: true\n      service: http\n      state: enabled\n</code></pre></p>"},{"location":"Ansible/#files","title":"Files","text":"Create file<pre><code>- copy:\n  dest: /etc/motd\n  content: \"Hello, World!\\n\" # (1)\n</code></pre> <ol> <li>Alma Linux 9 additionally requires the libselinux-python package to handle SELinux contexts. It is incorrectly identified as \"libselinux-python\" in the Ansible error message.</li> </ol> Delete file<pre><code>- file:\n  path: /etc/motd\n  state: absent\n</code></pre> Create directory<pre><code>- file:\n  path: /home/ansible/.vim/autoload\n  state: directory\n  mode: 0755\n</code></pre>"},{"location":"Ansible/#user-creation","title":"User creation","text":"<pre><code>- hosts: all\n  vars:\n  - username: newuser\n  - password: password\n  tasks:\n  - user:\n      name: \"{{ username }}\"\n      password: \"{{ password }}\"\n</code></pre> <p>More secure is using a separate vaulted variables file to keep the credential secure.</p> <p>playbooks/user.yml<pre><code>- hosts: all\n  vars_files:\n  - vars/user.yml\n  tasks:\n  - user:\n      name: \"{{ username }}\"\n      password: \"{{ password }}\"\n</code></pre> playbooks/vars/user.yml<pre><code>username: newuser\npassword: password\n</code></pre> Run playbook providing a password file<pre><code>ansible-playbook --vault-password-file vault-pw playbooks/user.yml\n</code></pre></p>"},{"location":"Ansible/#commands","title":"Commands","text":"ansible<pre><code># Ad-hoc commands\"\nansible all -m shell -a env\n\n# The **command** module is default and does not have to be made explicit\nansible all -a env # (1)\n\n# Delineate hosts\nansible all --list-hosts\n\n# Display specific groups from a provided inventory\nansible --list-hosts rhel7:rhel8 -i pods/inventory_role/pd/hosts\n</code></pre> ansible-config<pre><code># Display non-default settings\nansible-config dump --only-changed\n</code></pre> ansible-doc<pre><code># List currently installed modules\nansible-doc -l\n\n# Get module-specific information\nansible-doc $MODULE\n\n# Get example code\nansible-doc -s $MODULE\n</code></pre> ansible-playbook<pre><code># Verify YAML syntax\nansible-playbook --syntax-check $FILE\n</code></pre> ansible-vault<pre><code># Create an encrypted file, providing password interactively\nansible-vault create $file\n\n# Use a cleartext password file\nansible-vault view --vault-password-file=vault-pw $file\n\n# Encrypt/decrypt a file in-place, overwriting original file\nansible-vault encrypt $file\nansible-vault decrypt $file\n\nansible-vault edit secret.yml\n</code></pre>"},{"location":"Ansible/#ansible-galaxy","title":"ansible-galaxy","text":""},{"location":"Ansible/#modules","title":"Modules","text":"apt<pre><code># devops_solarwinds_role\n- name: Install apt-transport-https\n  apt:\n    name: apt-transport-https\n    state: present\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n- name: Run update for apt\n  apt:\n    update_cache: yes\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n- name: Install swiagent for apt\n  apt:\n    name:\n      - swiagent\n      - perl\n      # - linux-tools\n    state: latest\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n</code></pre> archive<pre><code>- name: Compress directory /path/to/foo/ into /path/to/foo.tgz\n  archive:\n    path: /path/to/foo\n    dest: /path/to/foo.tgz\n- name: Create a bz2 archive of multiple files, rooted at /path\n  archive:\n    path:\n    - /path/to/foo\n    - /path/wong/foo\n    dest: /path/file.tar.bz2\n    format: bz2\n</code></pre> cli_config<pre><code># Platform-agnostic way of pushing text-based configurations to network devices over the network_cli_connection plugin.\n# Source: https://opensource.com/article/19/9/must-know-ansible-modules\n- name: Set hostname for a switch and exit with a commit message\n  cli_config:\n    config: set system host-name foo\n    commit_comment: this is a test\n- name: Back up a config to a different destination file\n  cli_config:\n    config: \"{{ lookup('template', 'basic/config.j2') }}\"\n    backup: yes\n    backup_options:\n      filename: backup.cfg\n      dir_path: /home/user\n</code></pre> command<pre><code>- name: Return motd to registered var\n  command: cat /etc/motd\n  register: mymotd\n- name: Change the working directory to somedir/ and run the command as db_owner if /path/to/database does not exist.\n  command: /usr/bin/make_database.sh db_user db_name\n  become: yes\n  become_user: db_owner\n  args:\n    chdir: somedir/\n    creates: /path/to/database\n</code></pre> copy<pre><code>- name: Copy a new \"ntp.conf file into place, backing up the original if it differs from the copied version\n  copy:\n    src: /mine/ntp.conf\n    dest: /etc/ntp.conf\n    owner: root\n    group: root\n    mode: '0644'\n    backup: yes\n- name: Copy file with owner and permission, using symbolic representation\n  copy:\n    src: /srv/myfiles/foo.conf\n    dest: /etc/foo.conf\n    owner: foo\n    group: foo\n    mode: u=rw,g=r,o=r\n</code></pre> debug<pre><code>- name: Display all variables/facts known for a host\n  debug:\n    var: hostvars[inventory_hostname]\n    verbosity: 4\n\n# Display content of copy module only when verbosity of 2 is specified\n- name: Write some content in a file /tmp/foo.txt\n  copy:\n    dest: /tmp/foo.txt\n    content: |\n    Good Morning!\n      Awesome sunshine today.\n    register: display_file_content\n- name: Debug display_file_content\n  debug:\n    var: display_file_content\n    verbosity: 2\n</code></pre> file<pre><code>- name: Change file ownership, group and permissions\n  file:\n    path: /etc/foo.conf\n    owner: foo\n    group: foo\n    mode: '0644'\n- name: Create a directory if it does not exist\n  file:\n    path: /etc/foo\n    state: directory\n    mode: '0755'\n</code></pre> file<pre><code># Create a symlink\nansible $CLIENT -b -m file -a \"src=/etc/ntp.conf dest=/home/user/ntp.conf owner=user group=user state=link\"\n\n# Create a folder using an ad hoc command\nansible $CLIENT -b -m file -a \"path=/etc/newfolder state=directory mode=0755\"\n</code></pre> firewalld<pre><code># pods.redhat_subscriptions/roles/cockpit_setup\n- name: Permit the cockpit service\n  ansible.posix.firewalld:\n    service: cockpit\n    permanent: yes\n    immediate: yes\n    state: enabled    \n</code></pre> git<pre><code>- git:\n    name: Create git archive from repo\n    repo: https://github.com/ansible/ansible-examples.git\n    dest: /src/ansible-examples\n    archive: /tmp/ansible-examples.zip\n- git:\n    repo: https://github.com/ansible/ansible-examples.git\n    dest: /src/ansible-examples\n    separate_git_dir: /src/ansible-examples.git\n</code></pre> group<pre><code># pods.redhat_subscriptions/roles/tasks/main.yml\n- name: Ensure group \"docker\" exists\n  group:\n    name: docker\n    state: present\n    system: yes\n</code></pre> ini_file<pre><code>- name: Syncronize \"simple_allow_users\"\n  community.general.ini_file:\n    path: /etc/sssd/sssd.conf\n    section: \"{{ default_domain }}\"\n    option: simple_allow_users\n    value: \"{% for ag in (ad['allowedusers'] | default([])) %}{{ ag.name }}{% if not loop.last %}, {% endif %}{% endfor %}\"\n    mode: '0600'\n    state: present\n    exclusive: true\n  when: \"(ad['allowedusers'] | default([])) | length\"\n- name: Remove \"simple_allow_users\" when no users are listed\n  community.general.ini_file:\n    path: /etc/sssd/sssd.conf\n    section: \"{{ default_domain }}\"\n    option: simple_allow_users\n    state: absent\n    exclusive: true\n  when: \"((ad['allowedusers'] | default([])) | length) &lt; 1\"\n</code></pre> lineinfile<pre><code>- name: Ensure SELinux is set to enforcing mode\n  lineinfile:\n    path: /etc/selinux/config\n    regexp: '^SELINUX='\n    line: SELINUX=enforcing\n- name: Add a line to a file if the file does not exist, without passing regexp\n  lineinfile:\n    path: /etc/resolv.conf\n    line: 192.168.1.99 foo.lab.net foo\n    create: yes\n</code></pre> package<pre><code>- name: Install Apache and MariaDB\n  dnf:\n    name:\n    - httpd\n    - mariadb-server\n    state: latest\n- name: Install PostgreSQL and NGINX\n  yum:\n    name:\n    - nginx\n    - postgresql\n    - postgresql-server\n    state: present\n</code></pre> redhat_subscription<pre><code># pods.redhat_subscriptions/subscribe_vm/roles/tasks/main.yml\n- name: Subscribe RHEL System with RedHat\n  redhat_subscription:\n    state: present\n    username: \"{{ rhsm['redhat_subscription_manager']['username'] }}\"\n    password: \"{{ rhsm['redhat_subscription_manager']['password'] }}\"\n    auto_attach: true\n    # force_register: \"{{ sm_status.stdout.find('Invalid') != -1 }}\"\n  register: subscribe_results\n  when: ansible_os_family == 'RedHat' and ansible_distribution == 'RedHat'\n</code></pre> replace<pre><code>- name: Comment out a line in a config\n  ansible.builtin.replace:\n    path: /etc/motd\n    regexp: '^Hello, (.*)'\n    replace: '# Hello, \\1'\n</code></pre> service<pre><code>- name: Start service foo, based on running process /usr/bin/foo\n  service:\n    name: foo\n    pattern: /usr/bin/foo\n    state: started\n- name: Restart network service for interface eth0\n  service:\n    name: network\n    state: restarted\n    args: eth0\n</code></pre> setup<pre><code># Display all available information about a system\nansible $CLIENT -b -m setup\n\n# Filter results to ansible_os_family, which indicates if the OS is Debian or Red Hat\nansible $CLIENT -b -m setup -a \"filter=*family*\"\n</code></pre> shell<pre><code># From PODS\n- name: Remove Grub Defaults\n  shell: |\n    awk '/### BEGIN \\/etc\\/grub.d\\/10_linux ###/{stop=1} stop==0{print}' &lt; ks/EFI/BOOT/grub.cfg &gt; grub.cfg\n    cat grub.cfg\n  args:\n    chdir: \"{{ isopath }}\"\n</code></pre> snap<pre><code>- name: Install VS Code\n  snap:\n    name: code\n    state: present\n    classic: yes   \n</code></pre> systemd<pre><code># pods.redhat_subscriptions/roles/cockpit_setup/tasks/main.yml\n- name: Enable and start cockpit\n  ansible.builtin.systemd:\n    name: cockpit.socket\n    state: started\n    enabled: yes\n\n# pods.redhat_subscriptions/podman_setup/tasks/main.yml\n- name: Enable Podman socket\n  systemd:\n    name: podman.socket\n    state: restarted\n    daemon_reload: yes\n    enabled: true\n</code></pre> template<pre><code>- name: install index\n  template:\n    src: index.html.j2\n    dest: /var/www/html/index.html\n# (1)!\n\n# devops_solarwinds_role\n- name: Add mirrorlist\n  template:\n    src: \"{{ role_path }}/templates/swiagent-amzn-2015.mirrors.j2\"\n    dest: /etc/yum.repos.d/swiagent-amzn-2015.mirrors\n  when: ansible_os_family == \"RedHat\"  \n# (2)!\n\n- name: Add deb repo\n  template:\n    src: \"{{ role_path }}/templates/swiagent-ubuntu-14.list.j2\"\n    dest:  /etc/apt/sources.list.d/swiagent-ubuntu-14.list\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n# (3)!\n</code></pre> <ol> <li>Jinja2 template file<pre><code>&lt;html&gt;\n  &lt;h1&gt;This computer is running {{ ansible_os_family }},\n  and its hostname is:&lt;/h1&gt;\n  &lt;h3&gt;{{ ansible_hostname }}&lt;/h3&gt;\n  {# this is a comment, which won't be copied to the index.html file #}\n&lt;/html&gt;\n</code></pre></li> <li><pre><code>{% for host in groups['solarwinds'] %}\nhttps://{{ host }}/Orion/AgentManagement/LinuxPackageRepository.ashx?path=/dists/amzn-2015/$basearch\nhttps://{{ host }}:443/Orion/AgentManagement/LinuxPackageRepository.ashx?path=/dists/amzn-2015/$basearch\n{% endfor %}\n</code></pre></li> <li><pre><code>{% for host in groups['solarwinds'] %}\ndeb [trusted=yes] https://{{ host }}/Orion/AgentManagement/LinuxPackageRepository.ashx?path= ubuntu-14 swiagent\ndeb [trusted=yes] https://{{ host }}:443/Orion/AgentManagement/LinuxPackageRepository.ashx?path= ubuntu-14 swiagent\n{% endfor %}\n</code></pre></li> </ol> user<pre><code># pods.redhat_subscriptions/roles/podman_setup/tasks/main.yml\n- name: \"Add podman_nonroot_user to docker group\"\n  user:\n    name: \"{{ podman_nonroot_user }}\"\n    groups: docker\n    append: yes\n    system: yes\n  ignore_errors: yes\n</code></pre> yum<pre><code># pods.redhat_subscriptions/roles/cockpit_setup/tasks/main.yml\n- name: Install Cockpit\n  ansible.builtin.yum:\n    name: cockpit\n    state: latest\n\n# pods.redhat_subscriptions/roles/insights_setup/tasks/main.yml\n- name: Setup syncinsights package\n  yum:\n    name: \"pods-syncinsights-el{{ ansible_distribution_major_version }}\"\n    state: latest\n    update_cache: yes\n  when: \n    - \"'nexus' not in group_names\"\n\n# devops_solarwinds_role/tasks/main.yml\n- name: Install SolarWinds Agent\n  yum: \n    name:\n      - swiagent\n      - perl\n      - perf\n    state: latest\n  when: ansible_os_family == \"RedHat\"  \n</code></pre> yum_repository<pre><code># devops_solarwinds_role/tasks/main.yml\n- name: Add Solarwinds repository\n  yum_repository:\n    name: swiagent\n    description: SolarWinds Agent\n    mirrorlist: file:///etc/yum.repos.d/swiagent-amzn-2015.mirrors\n    file: swiagent-amzn-2015\n    enabled: yes\n    gpgcheck: no\n  when: ansible_os_family == \"RedHat\"  \n</code></pre>"},{"location":"Ansible/#glossary","title":"Glossary","text":"<ul> <li>Ad Hoc: type of command run in realtime by an administrator working at the terminal</li> <li>Ansible Galaxy: online portal where a gallery of roles made by the Ansible community can be found</li> <li>Ansible Tower: web-based RESTful API endpoint that provides the officially supported GUI frontend to Ansible configuration management, available in two versions: standard ($13,000/yr) and premium ($17,500/yr)</li> <li>Ansible Vault: place to keep encrypted passwords</li> <li>AWX: Open-source project upon which Ansible Tower was built</li> <li>Fact: System property gathered by Ansible when it executes a playbook on a node</li> <li>Module: standalone scripts that enable a particular task across many OSes, services, applications, etc.  Predefined modules are available in the module library, and new ones can be defined via Python or JSON.</li> <li>Play: script or instruction that defines the task to be carried out in a server</li> <li>Role: organize components of playbooks, allowing them to be reused</li> <li>Task: A single scripted action in a playbook, equivalent to an ad hoc command</li> <li>Vault: feature of Ansible that allows you to keep sensitive data such as passwords or keys protected at rest, rather than as plaintext in playbooks or roles.</li> </ul>"},{"location":"github/","title":"GitHub Actions","text":"<p>Actions are YAML files located under .github/workflows.</p>"},{"location":"Cloud/","title":"Overview","text":"<p>History</p> <p>Beginning in 2000, Amazon began developing Merchant.com, a planned e-commerce service that was intended to be the base upon which other enterprises would develop online shopping sites. At the time, Amazon's development environment was a jumbled mess, and in the effort to consolidate and organize the enterprise into a set of well-documented APIs.</p> <p>Despite these changes, software development remained sluggish, and an investigation discovered that individual teams were procuring storage, compute, and database resources independently. AWS originated out of the effort to consolidate these resources across the enterprise and remove this bottleneck.</p> <p>Azure was announced in 2008 and publicly released in 2010 after earlier experiments in cloud computing like Whitehorse and RedDog.  In fact, references to the \"classic\" model predating the Azure Resource Manager (ARM) actually refer to RedDog: the \"classic\" portal was also known as \"RedDog Front-End\".</p>"},{"location":"Cloud/#cli-tools","title":"CLI tools","text":"<p>Azure CLI</p> accountadaksconfigdeploymentgroupinteractivenetworkstoragevm config <pre><code># Get details for default subscription\naz account show\n\n# List subscriptions for the logged-in account\naz account list --output table\n\n# Set specific subscription as default\naz account set --subscription $SUBSCRIPTION\n\n# Display available locations\naz account list-locations\n\n# Logout\naz account clear\n</code></pre> <pre><code>\n</code></pre> <pre><code># List managed Kubernetes clusters\naz aks list\n\n# Inspect a single cluster\naz aks show -g $RG -n $CLUSTER_NAME -o table\n\n# Get Kubernetes context, saving it in $HOME/.kube/config\naz aks get-credentials -g $RG -n $CLUSTER_NAME\n\n# Display available AKS versions in the location\naz aks get-versions -l eastus\n\n# Get upgrade versions available for a specific cluster\naz aks get-upgrades -g $RG -n $CLUSTER_NAME\n\n# Upgrade an AKS cluster\naz aks upgrade -g $RG -n $CLUSTER_NAME  \\\n    --kubernetes-version \"1.28.5\"\n\n# Upgrade only the control plane of an AKS cluster\naz aks upgrade -g $RG -n $CLUSTER_NAME  \\\n    --control-plane-only                \\\n    --kubernetes-version \"1.28.5\"\n\n# Upgrade node pool of a cluster\naz aks nodepool upgrade -g $RG -n $NODEPOOL_NAME \\\n    --kubernetes-version \"1.28.5\"\n</code></pre> <pre><code># Settings are saved to $HOME/.azure/config by default\n\n# Set default location to East US 2\naz config defaults.location=eastus2\n</code></pre> <pre><code># Deploy to a resource group\naz deployment group create -g $GROUP --template-file $TEMPLATE_FILE_PATH\n\n# Deploy to a subscription\naz deployment sub create -l $LOCATION --template-file $TEMPLATE_FILE_PATH\n</code></pre> <pre><code># Create new resource group\naz group create -n $RG_NAME -l $LOCATION\n\n# Check if a specified resource group exists\naz group exists -n $RG_NAME\n</code></pre> <p>Enter an interactive environment to run Azure CLI commands.</p> vnet <pre><code>az network vnet list\n\n# Create a vnet, using default address prefix of 10.0.0./16\naz network vnet create -n $VNET_NAME -g $RG_NAME -l $LOCATION\n\n# Inspect specified vnet\naz network vnet show -n $VNET_NAME -g $RG_NAME\n\n# Display subnets of a vnet\naz network vnet subnet list -g $RG_NAME --vnet-name $VNET_NAME\n\n# Create a subnet, providing name, resource group, and vnet name\naz network vnet subnet create -n $SUBNET_NAME -g $RG_NAME --vnet-name $VNET_NAME\n\n\n# Create a vnet and subnets at the same time (using default address prefix)\naz network vnet create -n $VNET_NAME -g $RG_NAME -l $LOCATION \\\n    --subnet-name $SUBNET_NAME --subnet-prefixes $SUBNET_PREFIXES\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>Settings are saved to $HOME/.azure/config by default</p> <p>IBM Cloud CLI</p> accountiamisloginpipluginregionsresourcesltarget Config Links <pre><code># Get details of current account\nibmcloud account show\n\n# List accounts\nibmcloud account list\n</code></pre> <pre><code># Create an API key, saved to the current working directory\nibmcloud iam api-key-create $NAME --file $FILENAME --action-if-leaked DELETE\n\n# List API keys\nibmcloud iam api-keys\n</code></pre> <p>Virtual Private Cloud infrastructure service</p> subnetvpc <pre><code># List subnets\nibmcloud is subnets\n\n# Create subnet\nibmcloud is subnet-create\n</code></pre> <pre><code>ibmcloud is vpcs\n\nibmcloud is vpc-create --address-prefix-management auto\n\n# List address prefixes of a VPC\nibmcloud is vpc-addrs $VPC\n</code></pre> <pre><code>ibmcloud login\n\n# Logging in via one-time passcode is available from within the management portal\nibmcloud login -a https://cloud.ibm.com -u passcode -p wP9SrQx476\n</code></pre> iamimginsvolws <pre><code># Create an API key, saved to the current working directory\nibmcloud iam api-key-create $NAME --file $FILENAME --action-if-leaked DELETE\n\n# List API keys\nibmcloud iam api-keys\n</code></pre> <pre><code># List images in a workspace\nibmcloud pi img ls\n\n# List images available in the regional image catalog\nibmcloud pi img lc\n</code></pre> <pre><code># List instances (note that a workspace target must be set)\nibmcloud pi instance ls\n\n# Create an instance\n...\n</code></pre> Virtual machine operations<pre><code>\n</code></pre> <pre><code># List all storage volumes in a workspace\nibmcloud pi vol ls\n\n# Create a storage volume\n# Default values:\n#   --storage-pool: General-Flash-8\n#   --storage-tier: tier3\nibmcloud pi vol cr --size 1\n\n# Delete a storage volume\nibmcloud pi vol del $VOLUME_ID\n</code></pre> Related<pre><code># List storage pools for the targeted region\nibmcloud pi spools\n\n# List storage tiers for the targeted region\nibmcloud pi stiers\n</code></pre> <pre><code># List Power Virtual Server workspaces\nibmcloud pi ws ls\n\n# Create a workspace\nibmcloud pi ws cr -d $DATACENTER -p public -g $RG_ID # (1)\n\n# Extract workspace ID of first displayed workspace\nWORKSPACE_ID=$(ibmcloud pi ws ls --json | jq '.Payload.workspaces.[0].id' -r)\n\n# Get details of a workspace\nibmcloud pi ws get $WORKSPACE_ID\n\n# Target a workspace (requires a Cloud Resource Name (2) )\nibmcloud pi ws tg $WORKSPACE_CRN\n\n# Delete a workspace\nibmcloud pi ws del $WORKSPACE_ID\n</code></pre> <ol> <li> <p>This command requires the ID, not name, of the resource group:</p> <pre><code># Display ID of a named resource group\nibmcloud resource group $RG --id\n</code></pre> datacenter<pre><code># List datacenters\nibmcloud pi dat ls\n</code></pre> </li> <li> <p>For some commands, a cloud ID is not accepted but only a Cloud Resource Name (CRN).     These can be found from within the Resource List page of the IBM Cloud console.</p> </li> </ol> <pre><code># Install IBM Power Virtual Server CLI plug-in\nibmcloud plugin install power-iaas\n\n# Install the Classic Infrastructure Services (CIS) plugin\nibmcloud plugin install sl\n\n# Confirm\nibmcloud plugin show power-iaas\n</code></pre> <pre><code># List available regions\nibmcloud regions\n</code></pre> <pre><code># List resource groups\nibmcloud resource groups\n\n# Show details of a resource group\nibmcloud resource group $GROUP\n\n# Create a resource group\nibmcloud resource group-create $GROUP\n</code></pre> <pre><code>\n</code></pre> <pre><code># View current account and region\nibmcloud target\n</code></pre> <p>Configuration data for the ibmcloud CLI application is at $HOME/.bluemix/config.json (IBM Cloud used to be known as Bluemix). Plugins are also installed under this directory.</p> <ul> <li> <p>IBM Cloud CLI Quick Reference (pdf)</p> </li> <li> <p>Documentation</p> </li> </ul> <p>runpodctl</p> <p>Configuration</p> <p>Configuration is saved in $HOME/.runpod/config.toml</p> <pre><code># Save API key\nrunpodctl config --apiKey $APIKEY\n</code></pre> <p>Create pod</p> <ul> <li> <p><code>--gpuType</code>: Prepend \"NVIDIA GeForce\" to the GPU names provided on the Runpod website</p> </li> <li> <p><code>--imageName</code>: Accepts either link (or even relative path) to template description on Runpod's Explore page (i.e. \"runpod-torch-v240\" for https://www.runpod.io/console/explore/runpod-torch-v240)</p> </li> </ul> <pre><code>runpodctl create pod --gpuType 'NVIDIA GeForce RTX 3090' --imageName 'fedirz/faster-whisper-server:latest-cuda'\n</code></pre> <p>Handle existing pod</p> <pre><code># Display running pods (output is similar to docker ps)\nrunpodctl get pod\n\nrunpodctl start pod $POD_ID\nrunpodctl stop pod $POD_ID\nrunpodctl remove pod $POD_ID\n</code></pre> <p>Transfer files</p> Local<pre><code># Output displays an alphanumeric codephrase\nrunpodctl send $FILE\n</code></pre> Destination pod<pre><code>runpod receive $CODEPHRASE\n</code></pre> <ul> <li> <p>Ultimate Runpod tutorial</p> </li> <li> <p>GitHub repo</p> </li> </ul>"},{"location":"Cloud/#administration","title":"Administration","text":""},{"location":"Cloud/#cost-management","title":"Cost management","text":"<p>Azure quotas apply to subscriptions and are implemented with tags.</p> <ul> <li>Resource quotas trigger alarms when resource creation and consumption hit a threshold. These are not to be confused with resource limits which can stop resources from being created, whereas quotas can not.</li> <li>Spending quotas trigger alarms when spending has reached a threshold.</li> </ul> <p>Azure budgets can be viewed and administered in the Cost Management + Billing blade.  Users must have at least the Reader role at the subscription scope to view, and Contributor to create and manage, budgets. </p> <p>Resource locks  are used to apply restrictions across all users and roles and can be applied at subscription, resource group, or resource scopes. </p> <ul> <li>CanNotDelete</li> <li>ReadOnly effectively restricts all authorized users to the permissions granted by the Reader role<ul> <li>Storage account keys of a locked storage account cannot be listed because the list keys operation is handled through a POST request</li> <li>Visual Studio Server Explorer will not be able to display files for a locked App Service resource, because that interaction requires write access</li> <li>VMs in a locked resource group will not be able to be started or restarted, because those operations require a POST request</li> </ul> </li> </ul> <p>All child resources of the scope at which a lock is applied inherit the lock. A CanNotDelete lock applied to a DNS A record would also prevent the deletion of the DNS zone that the record resides in, as well as the resource group the zone resides in.</p> <p>Of the builtin roles, only two have access to the <code>Microsoft.Authorization/*</code> or <code>Microsoft.Authorization/locks/*</code> actions required to create or delete locks:</p> <ul> <li>Owner</li> <li>User Access Administrator</li> </ul> <p>Resource locks apply to the management plane of Azure, specifically operations sent to https://management.azure.com</p> <p>Managed applications create two resource groups to implement locks:</p> <ul> <li>One resource group to contain an overview of the service, which isn't locked</li> <li>Another resource group containing the infrastructure for the service, which is locked</li> </ul> <p>Sources:</p> <ul> <li> Move resources to a new resource group or subscription</li> <li>Some services have limitations or requirements when moving resources between groups (src)</li> <li>Source and destination subscriptions must be within the same [AAD][Azure AD] tenant</li> <li>Destination subscription must be registered for the resource provider of the resource being moved</li> <li>Account moving the resources must have at least the following permissions:<ul> <li>Microsoft.Resources/subscriptions/resourceGroups/moveResources/action</li> <li>Microsoft.Resources/subscriptions/resourceGroups/write</li> </ul> </li> </ul>"},{"location":"Cloud/#iam","title":"IAM","text":"<p>All cloud providers offer IAM systems that are used to control access to resources, all of which establish a similar taxonomy of concepts.</p> <p>The type of user that is granted access to resources is referred to variously as a member  or a security principal .</p> <p>Bundles of specific permissions that can be assigned to users are called roles. All cloud providers offer the ability to define custom roles and come with many ready-to-use role definitions: predefined roles  or built-in roles . Roles form the basis of RBAC, which is the recommended model used by all cloud providers.</p> <p>Roles are associated to users by policies  and role assignments .</p>  gcloud <pre><code>gcloud projects get-iam-policy $project\n</code></pre> <p>The Cloud providers also still support legacy IAM systems which are deprecated.</p> <ul> <li>Classic  administrator roles included Account Administrator, Service Administrator and Co-Administrator</li> <li>Primitive roles  included Owner, Editor, and Viewer can still be applied to most GCP resources.</li> </ul>"},{"location":"Cloud/#infrastructure","title":"Infrastructure","text":"<p>All cloud providers divide their global services into a hierarchy of geographically defined regions, each of which is in turn divided into availability zones (what AWS calls its Global Infrastructure).</p> <p>Azure datacenters contain multiple availability zones, and every Azure region has at least three availability zones.</p> <p>Azure services are also divided into geographies , generally coterminous with countries. Azure geographies are further divided into regional pairs . Each regional pair receives rolling updates one member at a time.</p> <p>Most services are regionally based, meaning the underlying hardware of that service's instance will exist in only a single Region. Some regions, like AWS GovCloud, have restricted access.</p> <p>Some AWS resources, however, are technically running on hardware that exists in a single Region, but presented as global. </p> <ul> <li>GCP regions </li> </ul> <p>Resources</p> <ul> <li>Services available on Free Tier</li> </ul>"},{"location":"Cloud/#monitoring","title":"Monitoring","text":""},{"location":"Cloud/#resources","title":"Resources","text":"<p>Cloud providers exhibit some variety in how resources can be organized.</p> <p>All cloud providers support key-value tags, many of which can be applied to the same resource.</p> <p>Any Azure resource can only exist in a single resource group, which can contain resources from any region or subscription. However, resource groups may not contain other resource groups.</p> <p>GCP projects are equivalent to Azure resource groups, in that they are containers for and direct parents to resources. However, projects can be placed within folders, which do support nested hierarchies.</p> <p>AWS does not have an equivalent method of organizing resources.</p> <p>Azure subscriptions can be organized into Management Groups, and they can be nested in a hierarchy of management groups up to a maximum depth of six levels. In AWS the Organizational Unit (OU), which can organize user accounts (subscriptions) and the resources they contain in a nested hierarchy, appears to be equivalent.</p> <p>A pattern common to Azure is that of a service being implemented in two resource types, one of which determines important configuration settings shared by all instances of the service which are contained within it. This is the case for storage accounts, [App Service][App Service], Azure Data Explorer clusters, etc.</p> <p>The resource hierarchy organizes GCP resources in 3 levels below Domain</p> <ul> <li>Domain<ul> <li>Organization corresponds to a company or organization. A single cloud identity is associated with a single organization and can have super admins</li> </ul> </li> </ul> <p>Billing Account tracks charges and billing account admins can set budgets.</p> <p>Payments Profile is a Google-level resource that is used to pay for all Google services.</p>"},{"location":"Cloud/#support","title":"Support","text":"<p>AWS offers various support plan tiers that provide 24/7 email, chat, and phone access to AWS cloud support engineers.</p> <ul> <li>Basic Support Plan</li> <li>Developer Support Plan (greater of $29 or 3% of monthly account usage)</li> <li>Business Support Plan</li> <li>Enterprise Support Plan (&gt;$15,000/mo.) offers a Technical Account Manager (TAM), a dedicated guide and advocate</li> </ul> <p>AWS documentation is available in several places:</p> <ul> <li>AWS documentation</li> <li>AWS Knowledge Center is a sprawling FAQ</li> <li>AWS security resources</li> <li>AWS forums</li> <li>Professional Services team makes white papers and webinars publicly available</li> </ul>"},{"location":"Cloud/#tags","title":"Tags","text":"<p>Azure tags:</p> <ul> <li>Tag names have a limit of 512 characters (128 characters for storage accounts) </li> <li>Tag values have a limit of 256 characters. </li> <li>Resources and resource groups are limited to 15 tags. </li> <li>VMs cannot exceed 2048 characters for all tag names and values combined. </li> </ul>"},{"location":"Cloud/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>All cloud providers support ways of provisioning resources declaratively.  Azure [ARM][ARM] templates are JSON, but [Bicep][Bicep] is a domain-specific language and command-line utility that can be used to generate templates from simpler, YAML-like syntax.</p>"},{"location":"Cloud/#compute","title":"Compute","text":""},{"location":"Cloud/#iaas","title":"IaaS","text":"<p>All cloud providers offer Infrastructure as a Service (IaaS), whereby virtual machines can be provisioned with specific compute resources and base operating systems.</p> <p>AWS also offers configuration management services like [OpsWorks][OpsWorks] and [Systems Manager][Systems Manager]</p> <p>GCP virtual machines are referred to as instances, and are available in three general machine family types: general-purpose, memory-optimized, and compute-optimized. Machine type describes the different packaged configurations representing allocated compute resources, or what is called a SKU in Azure.</p>"},{"location":"Cloud/#containers","title":"Containers","text":"<ul> <li> Build and package container artifacts</li> <li> Private container registry</li> </ul>"},{"location":"Cloud/#serverless","title":"Serverless","text":""},{"location":"Cloud/#storage","title":"Storage","text":""},{"location":"Cloud/#archive","title":"Archive","text":""},{"location":"Cloud/#backups","title":"Backups","text":"<p>Azure Backup are integrated into Portal and clickable from the VM blade.  You have to specify a Recovery Services vault and a Backup policy.  The policy can specify frequency of backups, and other settings. Using Backup service costs $10 per VM plus the cost of used storage.</p> <p>2 methods to restore data after backing up a VM to Azure Backup:</p> <ol> <li>Restore a recovery point as a new VM</li> <li>Restore access to files only</li> </ol>"},{"location":"Cloud/#physical-media","title":"Physical media","text":"<p> Uploading files to GCS</p>"},{"location":"Cloud/#networking","title":"Networking","text":"<p>All cloud providers offer an implementation of software-defined networking (SDN) that allows a logically isolated network to be defined as a block of IP addresses allocated from one of the private ranges (10.0.0.0/8, 192.168.0.0/16, or 172.16.0.0/12), what in AWS and GCP is referred to as a VPC and in Azure a VNet. In all providers, the network is confined to a single region and must have at least one IP segment called a subnet defined within it which must be a subset of the range used to define the virtual network itself.</p> <p>The smallest possible CIDR range for a subnet in Azure is 29, which provides 3 addresses for use (Azure reserves 5). In AWS, the smallest possible CIDR range is 28. In AWS, VPCs have a default range of 172.31.0.0/16 and subnets have a default subnet mask of /20.</p> <p>In Azure, subnets span Availability Zones, can only be deleted if empty, and their names, which are immutable, must be unique. In AWS, a subnet exists only within a single Availability Zone.</p> <p>VNet peering allows VMs in two separate virtual networks to communicate directly.  In all cloud providers, this is a one-way process which must be repeated in both directions in order to have two-way communication.</p> <p>In Azure, before the introduction of peering, virtual networks were connected using S2S VPN or by connecting to the same ExpressRoute circuit. It is not required for the peered networks to be in the same region (Global VNet peering), subscription, or tenant, although cross-tenant peering is not available in the Portal but must be configured from the command-line or ARM templates. VNet peering has to be disabled before moving a VNet, and a VNet can only be moved within the same subscription.</p> <ul> <li>There is a maximum of 100 peering connections per VNet</li> <li>Peerings cannot be moved to another resource group or subscription, so they must be disabled before moving peered VNets.</li> </ul> <p>Service endpoints facilitate restricting traffic from Azure services.  Service endpoint policies allow restricting traffic to the granularity of individual Azure service instances.</p> <p>An internet gateway is a VPC resource that allows EC2 instances to obtain a public IP address and access the Internet. In order to access the Internet, instances must be in a public subnet, one that contains a default route to the VPC's internet gateway.</p> <ul> <li>ExpressRoute is the main service used to connect Azure to on-premises networks, although P2S and S2S VPNs are also options.</li> <li>Direct Connect provides dedicated network connectivity to an AWS VPC through links offered through APN partners.</li> </ul> <p>In GCP, in addition to peering, a shared VPC can be created that is associated with multiple projects.</p> <p>Resources:</p> <ul> <li> Migrating to GCP? First Things First: VPCs</li> </ul>"},{"location":"Cloud/#user-defined-routes","title":"User-defined routes","text":"<p>In Azure, a virtual appliance refers to a VM running a network application like a load-balancer, firewall, or router.  Service chaining refers to the process of deploying a network virtual appliance (NVA) into a hub network to route traffic between spokes using user-defined routes (UDR).  This is a method of reducing the complexity of pairing between individual spoke networks in complex hub-and-spoke architectures. <sup>AZ-103: 309</sup></p> <ul> <li>In such a deployment, the peerings must be set to Allow Forwarded Traffic.</li> </ul> <p>Alternatively, two peered networks can share a single virtual network gateway, say to connect to an external network.</p> <ul> <li>The pairing connection to the network that contains the gateway must be set to Use Remote Gateways</li> <li>The pairing connection from the network containing the gateway must be set to Allow Gateway Transit</li> </ul>"},{"location":"Cloud/#network-security","title":"Network security","text":"<p>Azure Network Security Groups (NSGs) are associated with network interfaces and contain an arbitrary number of security rules. Each rule has the following properties:</p> <ul> <li>Name</li> <li>Priority: number between 100 and 4096, lower numbers indicate a higher priority</li> <li>Source or destination: IP address, CIDR block, service tag, or application security group</li> <li>Protocol: <code>TCP</code>, <code>UDP</code>, <code>ICMP</code>, or <code>Any</code></li> <li>Direction: Inbound or outbound</li> <li>Port range; </li> <li>Action: allow or deny</li> </ul> <p>Service tags represent a group of IP address prefixes managed by Microsoft available for use in NSG rules:</p> <ul> <li><code>VirtualNetwork</code>: all CIDR ranges defined for the virtual network, all connected on-premises address spaces, peered VNets or VNets connected to a VNET gateway</li> <li><code>AzureLoadBalancer</code>: Virtual IP address of the host where Azure's health probes originate</li> <li><code>Internet</code>: IP address space that is outside the virtual network</li> <li><code>AzureCloud*</code>: IP address space for Azure, including all datacenter public IP addresses</li> <li><code>AzureTrafficManager*</code>: IP address space for the Azure Traffic Manager probe IP addresses</li> <li><code>Storage</code>:</li> </ul> <p>NSG flow logging ,which saves the 5-tuple of all packets, is available as a low-cost way to monitor traffic. Flow logs record all IP flows going in and out of an NSG and are collected per NSG rule. They are charged per GB of logs collected and include a free tier of 5 GB/month.</p> <p>In AWS VPCs, Security Groups are similar to firewall rules that regulate inbound and outbound traffic of an instance. Outbound traffic is unrestricted by default, and every VPC contains a default security group.</p> <p>A network access control lists (NACLs), also like a firewall, contains inbound and outbound rules but operates on the subnet. By default, a NACL allows all inbound and outbound traffic.</p> <p>In GCP, each VPC has a set of firewall rules that control traffic not only into and out of the VPC, but between instances in the same VPC. Each rule can be tagged, and individual instances with the same tags inherit those rules.</p> <p>Resources:</p> <ul> <li> Protect your Google Cloud Instances with Firewall Rules</li> </ul>"},{"location":"Cloud/#dns","title":"DNS","text":""},{"location":"Cloud/#cdn","title":"CDN","text":"<p>Users can use Azure CDN as a cache, reducing load from website.  Content is cached by the CDN until its time-to-live (TTL) elapses, which can be controlled in the HTTP response from the origin server.</p> <p>Permanently removing content from the CDN requires it be first removed from the origin servers, meaning if the content is in a storage account it should be set to private or deleted from the storage, or the container itself should be deleted.  Cached copies may remain in the CDN endpoint until the TTL has expired, unless it is purged.</p> <p>There are 4 pricing tiers available within Azure CDN:</p> <ul> <li>Azure CDN Standard from Microsoft does not offer dynamic site acceleration (DSA) (cf. Azure Front Door Service)</li> <li>Azure CDN Standard from Akamai</li> <li>Azure CDN Standard from Verizon</li> <li> <p>Azure CDN Premium from Verizon, for which caching is configured using a rules engine.</p> </li> <li> <p>AWS</p> <ul> <li>CloudFront</li> </ul> </li> <li>GCP<ul> <li>CDN</li> </ul> </li> </ul>"},{"location":"Cloud/#load-balancing","title":"Load-balancing","text":"<ul> <li>Azure<ul> <li>Load Balancer</li> <li>Application Gateway</li> </ul> </li> <li>AWS<ul> <li>Elastic Load Balancer</li> </ul> </li> <li>GCP<ul> <li>Load balancing</li> </ul> </li> </ul>"},{"location":"Cloud/#development","title":"Development","text":""},{"location":"Cloud/#nosql","title":"NoSQL","text":"<p>NoSQL databases differ from relational databases in that they do not obey the principle of data normalization. That is, the same data can be stored in more than one place.</p> <p>This is an advantage for databases that are optimized for reads as opposed to writes, because fewer queries are needed to retrieve information. However, when changing information that is duplicated in several places, write operations will be more laborious and prone to error. NoSQL databases are also horizontally scalable because the information can be sharded horizontally more easily than relational database, which are only vertically scalable (meaning scaling them requires larger and larger computers) and can only be sharded vertically. (src)</p>"},{"location":"Cloud/Tasks/","title":"Tasks","text":"<p>Cloud APIs  are equivalent to Azure resource providers . Unlike Azure, which automatically registers resource providers on use, Cloud APIs must be enabled per project.</p> <p>GCP</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Azure</p> <pre><code>az provider register -n Microsoft.ContainerService\n</code></pre>"},{"location":"Cloud/Tasks/#display-all-available-regions","title":"Display all available regions","text":"<p>GCP</p> <pre><code>gcloud compute regions list\n</code></pre>"},{"location":"Cloud/Tasks/#administration","title":"\ud83d\udee0\ufe0f Administration","text":"<p>Display subscription ID</p> <pre><code>Get-AzSubscription\n</code></pre> <pre><code>az account show\n</code></pre>"},{"location":"Cloud/Tasks/#cli","title":"\ud83d\udda5\ufe0f CLI","text":"<p>Initialize CLI utility</p> <pre><code>gcloud init\n</code></pre>"},{"location":"Cloud/Tasks/#iam","title":"IAM","text":"<p>Add guest user</p> <pre><code>New-AzureADMSInvitation \n    -InvitedUserEmailAddress $EMAIL \n    -SendInvitationMessage $True \n    -InviteRedirectUrl \"http://myapps.onmicrosoft.com\"\n</code></pre> <p> </p> <p>Assign a role</p> <pre><code># At the organization level\ngcloud organizations add-iam-policy-binding \n    $ORG_ID\n    --member=\"user:$EMAIL\"\n    --role=\"roles/compute.xpnAdmin\"\n</code></pre> <pre><code># At the folder level\ngcloud beta resource-manager-folders add-iam-policy-binding \n    $FOLDER_ID\n    --member=\"user:$EMAIL\"\n    --role=\"roles/compute.xpnAdmin\"\n</code></pre>"},{"location":"Cloud/Tasks/#cost-management","title":"\ud83d\udcb0 Cost management","text":"<p>To view resource quotas for a subscription, go to the subscription in Azure Portal and open the Usage + quotas blade. From there you can select resources and then click the Request Increase button.</p> <p>View current usage of vCPU quotas</p> <pre><code>Get-AzVMUsage\n</code></pre> <p>View current usage of storage service</p> <pre><code>Get-AzStorageUsage\n</code></pre> <p>Create a budget</p> <p>To create a budget, open Cost Management + Billing, then Subscriptions, select a subscription, then click Budgets.  Then click + Add, which produces a Create budget blade.  The created budget can be seen in the Budgets blade.  PowerShell commands used with budgets:</p> <ul> <li><code>Get-AzResourceGroup</code> retrieve Resource Group object</li> <li><code>Set-AzResourceGroup</code> apply a tag to a resource group with no preexisting tags</li> <li><code>.Tags</code> method that retrieves Tag collection from a resource group</li> <li><code>.Add()</code> method used to add tags to a resource group that already has tags.</li> </ul>"},{"location":"Cloud/Tasks/#monitoring","title":"Monitoring","text":"<p>VM extension</p> <pre><code>Set-AzVMExtension -ResourceGroupName ExamRefRG -Location \"West Europe\" -VMName VM1 -Name networkWatcherAgent -Publisher Microsoft.Azure.NetworkWatcher -Type NetworkWatcherAgentWindows -TypeHandlerVersion 1.4\n</code></pre> <pre><code>az vm extension set --vm-name VM1 --resource-group ExamRefRG --publisher Microsoft.Azure.NetworkWatcher --version 1.4 --name NetworkWatcherAgentWindows --extension-instance-name NetworkWatcherAgent\n</code></pre> <p>Start packet capture</p> <pre><code>$nw = Get-AzResource | Where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"WestEurope\"\n$networkWatcher = Get-AzNetworkWatcher -Name $nw.Name -ResourceGroupName $nw.ResourceGroupName\n$storageAccount = Get-AzStorageAccount -Name examref-storage -ResourceGroupName ExamRefRG\n\n$filter1 = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.3\" -LocalPort \"1-65535\" -RemotePort \"20;80;443\"\n$filter2 = New-AzPacketCaptureFilterConfig -Protocol UDP\n$vm = Get-AzVM ` -Name VM1 -ResourceGroupName ExamRefRG\n\nNew-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -TargetVirtualMachineId $vm.Id -PacketCaptureName \"PacketCaptureTest\" -StorageAccountId $storageAccount.id -TimeLimitInSeconds 60 -Filter $filter1, $filter2\n</code></pre> <pre><code>filter='[ { \"protocol\": \"TCP\", \"remoteIPAddress\": \"1.1.1.1-255.255.255.255\", \"localIPAddress\":\"10.0.0.3\", \"remotePort\":\"20\" } ]'\naz network watcher packet-capture create --name PacketCaptureTest2 --resource-group ExamRefRG --vm VM1 --time-limit 300 --storage-account examref-storage --filters $filter\n</code></pre> <p>Check status of packet capture</p> <pre><code>Get-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\"\n</code></pre> <pre><code>az network watcher packet-capture show-status --name PacketCaptureTest --location WestEurope\n</code></pre> <p>Stop packet capture</p> <pre><code>Stop-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\"\n</code></pre> <pre><code>az network watcher packet-capture stop --name PacketCaptureTest --location WestEurope\n</code></pre> <p>Use IP Flow Verify to test outbound connectivity from source VM and port to destination. If any configured filtering rules block traffic between the endpoints, it will return the name of the offending NSG.</p> <pre><code>Test-AzNetworkWatcherIPFlow\n</code></pre> <pre><code>az network watcher test-ip-flow\n</code></pre> <p>Next Hop</p> <pre><code>Get-AzNetworkWatcherNextHop\n</code></pre> <pre><code>az network watcher show-next-hop\n</code></pre> <p>Use Network Topology</p> <pre><code>Get-AzNetworkWatcherTopology\n</code></pre> <pre><code>az network watcher show-topology\n</code></pre> <p>Capture SFTP traffic</p> <pre><code>$r = Get-AzResource | where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"EastUS\"\n$nw = Get-AzNetworkWatcher -Name $r.Name -ResourceGroupName $r.ResourceGroupName\n$s = Get-AzStorageAccount -ResourceGroupName \"Diagnostics-RG\" -Name \"Diagnostics-Storage\"\n$filter = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.4\" -LocalPort \"1-65535\" -RemotePort \"22\"\n\nNew-AzNetworkWatcherPacketCapture -NetworkWatcher $nw -TargetVirtualMachineId $vm.ID -PacketCaptureName \"Capture SFTP traffic\" -StorageAccountId $s.Id -TimeLimitInSeconds 60 -Filter $filter\n</code></pre>"},{"location":"Cloud/Tasks/#resources","title":"Resources","text":"<p>Create resource group</p> <pre><code>New-AzGroup -Location $location -Name $rgName\n</code></pre> <pre><code>az group create -l $location -n $rgName \n</code></pre> <p>Register resource provider in subscription <pre><code>az provider register --namespace 'Microsoft.PolicyInsights'\n</code></pre></p> <p>Move resources</p> <pre><code>$webapp = Get-AzResource -ResourceGroupName OldRG -ResourceName ExampleSite\n$plan = Get-AzResource -ResourceGroupName OldRG -ResourceName ExamplePlan\n\nMove-AzResource -DestinationResourceGroupName NewRG -ResourceId $webapp.ResourceId, $plan.ResourceId\n</code></pre> <pre><code>webapp=$(az resource show -g OldRG -n ExampleSite --resource-type \"Microsoft.Web/sites\" --query id --output tsv)\nplan=$(az resource show -g OldRG -n ExamplePlan --resource-type \"Microsoft.Web/serverfarms\" --query id --output tsv)\n\naz resource move --destination-group newgroup --ids $webapp $plan\n</code></pre> <p>Create lock on a resource</p> <pre><code>New-AzResourceLock \n    -LockName LockSite\n    -LockLevel CanNotDelete \n    -ResourceGroupName $rg \n    -ResourceName $r \n    -ResourceType Microsoft.Web/sites \n</code></pre> <pre><code>az lock create \n    --name LockSite\n    --lock-type CanNotDelete \n    --resource-group $rg \n    --resource-name $r \n    --resource-type Microsoft.Web/sites \n</code></pre> <p>Create lock on a resource group</p> <pre><code>New-AzResourceLock \n    -LockName LockGroup \n    -LockLevel CanNotDelete \n    -ResourceGroupName $rg\n</code></pre> <pre><code>az lock create \n    --name LockGroup \n    --lock-type CanNotDelete \n    --resource-group $rg\n</code></pre> <p>Display resource lock</p> <pre><code>Get-AzResourceLock -ResourceName $r -ResourceType Microsoft.Web/sites -ResourceGroupName $rg\n</code></pre> <pre><code>az lock list --resource-group $rg --resource-name $r --namespace Microsoft.Web --resource-type sites --parent \"\"\n</code></pre> <p>Delete resource lock</p> <pre><code>$lockId = (Get-AzResourceLock -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft.Web/sites).LockId\n\nRemove-AzResourceLock -LockId $lockId\n</code></pre> <pre><code>lockid=$(az lock show --name LockSite --resource-group $rg --resource-type Microsoft.Web/sites --resource-name $r --output tsv --query id)\naz lock delete --ids $lockid\n</code></pre> <p>Sources</p> <ul> <li>Manage Azure Resource Manager resource groups by using Azure PowerShell</li> <li>Manage Azure Resource Manager resource groups by using Azure CLI</li> <li>Resource providers</li> <li>Lock resources to prevent unexpected changes</li> <li>AZ-103: <code>1.3</code>, p. 76</li> </ul>"},{"location":"Cloud/Tasks/#tags","title":"Tags","text":"<p>List all resources by tag</p> <pre><code>(Get-AzResource -Tag @{ CostCode=\"1001\"}).Name\n# List all resources by tag name, with no value\n(Get-AzResource -TagName CostCode).Name\n</code></pre> <pre><code>az resource list --tag Dept=Finance\n</code></pre> <p>List resource groups by tag</p> <pre><code>(Get-AzResourceGroup -Tag @{ CostCode=\"1001\" }).ResourceGroupName\n</code></pre> <pre><code>az group list --tag CostCode=1001\n</code></pre> <p>Enumerate a resource's tags</p> <pre><code>$r = Get-AzResource -Name $resourceName -ResourceGroup rg\nGet-AzTag -ResourceId $r.id\n\n# Resource group\n$rg = Get-AzResourceGroup -Name $rgName\nGet-AzTag -ResourceId $rg.ResourceId\n\n# Subscription\n$s = (Get-AzSubscription -SubscriptionName \"Example Subscription\").Id\nGet-AzTag -ResourceId \"/subscriptions/$s\"\n</code></pre> <pre><code>az resource show -n $resourceName -g $rgName --query tags\n\n# Resource group\naz group show -n $rgName --query tags\n</code></pre> <p>Tag resource</p> <pre><code>$r = Get-AzResource -ResourceName hrvm1 -ResourceGroupName rg\n$r.Tags.Add(\"Owner\", \"user@contoso.com\")\nSet-AzResource -Tag $r.Tags -ResourceId $r.ResourceId -Force\n</code></pre> <p>Resource group</p> <pre><code>$tags = @{\"Dept\"=\"Finance\"; \"Status\"=\"Normal\"}\n$rg = Get-AzResourceGroup -Name demoGroup\nNew-AzTag -ResourceId $rg.ResourceId -tag $tags\n</code></pre> <pre><code>$tags = (Get-AzResourceGroup -Name rg).Tags\n$tags.Add(\"Owner\", \"user@contoso.com\")\nSet-AzResourceGroup -Tag $tags -Name rg\n</code></pre> <pre><code>jsonrtag=$(az group show -n rg --query tags)\nrt=$(echo $jsonrtag | tr -d '\"{},' | sed 's/: /=/g')\naz group update -n rg --tags $rt Owner=user@contoso.com\n</code></pre> <p>Remove specific tags</p> <pre><code>$tags = @{\"Project\"=\"ECommerce\"; \"Team\"=\"Web\"}\nUpdate-AzTag -ResourceId $resource.id -Tag $tags -Operation Delete\n</code></pre> <p>Remove all tags</p> <pre><code>$s = (Get-AzSubscription -SubscriptionName \"Example Subscription\").Id\nRemove-AzTag -ResourceId \"/subscriptions/$s\"\n\n# Alternatively\nSet-AzResourceGroup -Tag @{} -Name rg\n</code></pre> <p>Apply tags to resource, overwriting</p> <p><pre><code>$tags = @{\"Dept\"=\"Finance\"; \"Status\"=\"Normal\"}\nNew-AzTag -ResourceId $resource.id -Tag $tags\n</code></pre> <pre><code>Set-AzResource -ResourceId $r.ResourceId -Tag @{ CostCode=\"1001\"; Environment=\"Production\" } -Force\n</code></pre></p> <pre><code>az resource tag --tags 'Dept=IT' 'Environment=Test' -g $rgName -n examplevnet --resource-type \"Microsoft.Network/virtualNetworks\"\n</code></pre> <p>Apply tags to resource group</p> <pre><code>Set-AzResourceGroup -Name rg -Tag @{CostCode=1001; Environment=Production}\n</code></pre> <pre><code>az group update -n $rgName --tags 'Environment=Test' 'Dept=IT'\n\n# Alternatively\naz group update -n $rgName --set tags.Environment=Production tags.CostCode=1001\n</code></pre>"},{"location":"Cloud/Tasks/#compute","title":"Compute","text":""},{"location":"Cloud/Tasks/#kubernetes","title":"\u2693 Kubernetes","text":"<p>Create Kubernetes cluster</p> <p><pre><code>New-AzAKS -ResourceGroupName $g -Name $n\n    -NodeCount 2\n    -NetworkPlugin azure\n    -NodeVmSetType VirtualMachineScaleSets\n    -WindowsProfileAdminUserName azureuser\n    -WindowsProfileAdminUserPassword $Password\n    -KubernetesVersion 1.16.7 \n    # PowerShell does not offer an option to generate SSH keys for access to the cluster; `ssh-keygen` must be used.\n</code></pre> -  Create a Windows Server container on an AKS cluster</p> <p><pre><code>az aks create -g $g -n $n\n    --node-count 2 \n    --network-plugin azure \n    --vm-set-type VirtualMachineScaleSets \n    --windows-admin-username azureuser \n    --windows-admin-password $PASSWORD\n    --generate-ssh-keys \n    --enable-addons monitoring \n</code></pre> -  Create a Windows Server container on an AKS cluster </p> <p>Add a pool of nodes</p> <pre><code>New-AzAksNodePool -ResourceGroupName $rgName -Name npwin -ClusterName $clusterName \n    -OsType Windows \n    -KubernetesVersion 1.16.7\n</code></pre> <pre><code>az aks nodepool add -g $g -n $n --cluster-name $clusterName\n    --os-type Windows\n    --node-count 1\n</code></pre> <p>Persistent volume claim</p> <p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: azure-managed-disk\nspec:\n    accessModes:\n    - ReadWriteOnce\n    storageClassName: managed-premium\n    resources:\n        requests:\n            storage: 5Gi\n</code></pre> -  Source</p> <p>Provision Azure Disk</p> StandardPremium <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\nname: managed-disk-forapp\nprovisioner: kubernetes.io/azure-disk\nreclaimPolicy: Retain\nparameters:\nstorageaccounttype: default\nkind: Managed\n</code></pre> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\nname: managed-disk-forapp\nprovisioner: kubernetes.io/azure-disk\nreclaimPolicy: Retain\nparameters:\nstorageaccounttype: Premium_LRS\nkind: Managed\n</code></pre>"},{"location":"Cloud/Tasks/#functions","title":"Functions","text":"<p>Deploy <pre><code>gcloud functions deploy hello_get --runtime python37 --trigger-http\n</code></pre> Test <pre><code>gcloud functions describe hello_get\n</code></pre></p>"},{"location":"Cloud/Tasks/#storage","title":"Storage","text":"<p>Create storage account</p> Azure Portal <p>Click Create a resouce, then Storage, then Storage account. Choose a globally unique name for the account, containing lower-case characters and digits only.</p> <pre><code>New-AzStorageAccount -ResourceGroupName ExamRefRG -Name mystorage112300 -SkuName Standard_LRS -Location WestUS -Kind StorageV2 -AccessTier Hot\n</code></pre> <pre><code>az storage account create --name $accountName --resource-group $resourceGroup -location $location --sku $sku\n</code></pre> <p>Change access tier of storage account</p> <p>=== \"Azure PowerShell</p> <pre><code>```powershell\nSet-AzStorageAccount -ResourceGroupName RG -Name $accountName -AccessTier Cool -Force\n```\n</code></pre> <p>Change replication mode of storage account</p> <pre><code>Set-AzStorageAccount -ResourceGroupName $resourceGroup -Name $accountName -SkuName $type\n</code></pre> <p>Renew storage account keys</p> <p>=== \"Azure </p> <pre><code>```powershell\nNew-AzStorageAccountKey\n```\n</code></pre> <pre><code>az storage account keys renew\n</code></pre> <p>Create Azure Key Vault</p> <pre><code>New-AzKeyVault -VaultName $vaultName -ResourceGroupName $g -Location $location \n$key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' \n$storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount \n$secretvalue = ConvertTo-SecureString $storageKey[0].Value -AsPlainText -Force\n$secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue  $secretvalue\n</code></pre> <pre><code>az keyvault create --name $vaultName --resource-group $g --location $location\naz keyvault key create --vault-name \"$vaultName\" --name $keyName --protection \"software\"\naz keyvault secret set --vault-name \"$vaultName\" --name \"$secretName\" --value \"$secretValue\"\n</code></pre> <p>Create key in Azure Key Vault</p> <pre><code>$key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software'\n$storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount \n$secretvalue = ConvertTo-SecureString $storageKey[0].Value -AsPlainText -Force\n\n$secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue\n</code></pre> <pre><code>az keyvault key create --vault-name $vaultName --name $keyName --protection \"software\"\naz keyvault secret set --vault-name $vaultName --name $secretName --value $secretValue\n</code></pre> <p>Create Azure sync group</p> <p>Specify name of sync group in dialog after creating an Azure File Sync</p> <p>Change storage class</p> <p><code>$STORAGE_CLASS</code> can be <code>multi_regional</code>, <code>regional</code>, <code>nearline</code>, or <code>coldline</code> <pre><code>gsutil rewrite -s $STORAGE_CLASS gs://$PATH_TO_OBJECT\n</code></pre></p>"},{"location":"Cloud/Tasks/#file-shares","title":"File shares","text":"<p>Deploy Azure File Sync</p> <pre><code># Create Storage Sync Service\n$storageSync = New-AzStorageSyncService -ResourceGroupName $g -Name $storageSyncName -Location $l\n\n# Create Azure File Share\n$storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount \n$context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey.Value[0]\n\nNew-AzStorageShare -Name $shareName -Context $context\n</code></pre> <pre><code># Creating a Storage Sync Service resource is only possible in PowerShell or Portal\nconstring=$(az storage account show-connection-string -n $storageAccountName)\naz storage share create --name $shareName --quota 2048 --connection-string $constring\n</code></pre> <p>Create sync group</p> <pre><code>$syncgroup = New-AzStorageSyncGroup -Name $syncgroupname -ParentObject $storageSync\n</code></pre> <p>Create cloud endpoint</p> <pre><code>New-AzStorageSyncCloudEndpoint -Name $shareName -ParentObject $syncgroup -StorageAccountResourceId $storageAccount.Id -AzureFileShareName $shareName\n</code></pre>"},{"location":"Cloud/Tasks/#network-access","title":"Network access","text":"<p>Display the status of the default NetworkRule for a storage account</p> <pre><code>Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object DefaultAction\n</code></pre> <pre><code>az storage account show -$rgName -n $n --query networkRuleSet.defaultAction\n</code></pre> <p>Set default rule</p> <pre><code>Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Deny\nUpdate-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Allow\n</code></pre> <pre><code>az storage account update -g $g -n $n --default-action Deny\naz storage account update -g $g -n $n --default-action Allow\n</code></pre>"},{"location":"Cloud/Tasks/#networking","title":"Networking","text":"<p>Create virtual network with a specific prefix and subnet</p> <pre><code>$subnet = New-AzVirtualNetworkSubnetConfig \n    -Name $subnetName \n    -AddressPrefix \"10.0.0.0/24\"\n$vnet = New-AzVirtualNetwork -Name $name -ResourceGroupName $rgName -Location $l \n    -AddressPrefix \"10.0.0.0/16\" \n    -Subnet $subnet\n</code></pre> <pre><code>az network vnet create -g $rgName -n $name\n    --address-prefix \"10.0.0.0/16\"\n    --subnet-name $subnetName\n    --subnet-prefix \"10.0.0.0/24\"\n</code></pre> <pre><code>gcloud networks create $name --subnet-mode=custom\ngcloud beta compute networks subnets create $subnetName\n    --network=$name\n    --region=$l\n    --range=\"10.0.0.0/16\"\n    --enable-private-ip-google-access\n    --enable-flow-logs\n</code></pre> <p>Create peering</p> <pre><code>Add-AzVirtualNetworkPeering \n    -Name 'peering1' \n    -VirtualNetwork $net1 \n    -RemoteVirtualNetworkId $net2.Id\n\nAdd-AzVirtualNetworkPeering \n    -Name 'peering2' \n    -VirtualNetwork $net2 \n    -RemoteVirtualNetworkId $net1.Id\n</code></pre> <pre><code>az network vnet peering create \n    -n 'peering1' \n    -g $g \n    --vnet-name net1 \n    --allow-vnet-access \n    --remote-vnet net2\n\naz network vnet peering create \n    -n 'peering2' \n    -g $g \n    --vnet-name net2 \n    --allow-vnet-access \n    --remote-vnet net1\n</code></pre> <pre><code>gcloud compute networks peerings create \"peering1\"\n    --network net1\n    --peer-project $p\n    --peer-network net2\n    --auto-create-routes\n\ngcloud compute networks peerings create \"peering2\"\n    --network net1\n    --peer-project $p\n    --peer-network net1\n    --auto-create-routes\n</code></pre> <p>Check peering</p> <pre><code>Get-AzVirtualNetworkPeering -ResourceGroupName $rg -VirtualNetworkName $vnetName\n</code></pre> <pre><code>az network vnet peering list --resource-group $rg --vnet-name VNet1\naz network vnet peering list --resource-group $rg --vnet-name VNet2\n</code></pre> <p>User-defined routes</p> <pre><code># Create the route table resource\n$routeTable = New-AzRouteTable -Name $routeTableName -ResourceGroupName ExamRefRG\n\n# Add a route to route table object\nAdd-AzRouteConfig \n    -RouteTable $routeTable \n    -Name $routeName \n    -AddressPrefix 10.3.0.0/16 \n    -NextHopType VirtualAppliance \n    -NextHopIpAddress 10.2.20.4\nSet-AzRouteTable -RouteTable $routeTable\n\n# Associate route table with subnet\nSet-AzVirtualNetworkSubnetConfig -VirtualNetwork $vnet -Name Default -AddressPrefix $subnet.AddressPrefix \n    -RouteTable $routeTable\n\n# Commit changes\nSet-AzVirtualNetwork -VirtualNetwork $vnet\n\n# Get effective routes for a NIC\nGet-AzEffectiveRouteTable -NetworkInterfaceName $nicName -ResourceGroupName $rgName\n</code></pre> <pre><code># Create route table resource\naz network route-table create --name $routeTableName --resource-group $rgName \n\n# Add route to route table\naz network route-table route create \n    --resource-group $rgName \n    --route-table-name $routeTableName \n    --name $routeName \n    --address-prefix 10.3.0.0/16 \n    --next-hop-type VirtualAppliance \n    --next-hop-ip-address 10.2.20.4\n\n# Associate route table with subnet\naz network vnet subnet update --name defualt --vnet-name Vnet1 --resource-group $rgName \n    --route-table rt\n\n# Get effective routes for NIC\naz network nic show-effective-route-table --name $nicName --resource-group $rgName\n</code></pre> <p>Create NSG</p> <pre><code>$nsgRules = @()\n$nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTP\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 103 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5985\n$nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTPS\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 104 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5986\n$nsg = New-AzNetworkSecurityGroup -Name \"wscore-nsg\" -ResourceGroupName \"RG\" -Location \"East US\" -SecurityRules $nsgRules\n</code></pre> <p>View rules</p> <pre><code>Get-AzEffectiveNetworkSecurityGroup -NetworkInterfaceName $nicName -ResourceGroupName $rgName\n</code></pre> <pre><code>az network nic list-effective-nsg --name $nicName --resource-group $rgName\n</code></pre> <p>Create Bastion</p> <p>Connecting to a VM requires at least Reader role privileges on the VM, its NIC, and on the Bastion itself.</p> <pre><code>New-AzBastion -ResourceGroupName $rgName -Name $n -PublicIpAddress $pip -VirtualNetwork $vnet\n</code></pre> <pre><code>az network bastion create -g $rgName -n $n -l $l --public-ip-address $pip  --vnet-name $vnetName\n</code></pre> <p>Create virtual appliance</p> <p>IP forwarding must be enabled on the VM's NIC, then applications installed on the VM can begin accepting packets destined for other IP addresses.</p> <p> </p>"},{"location":"Cloud/Tasks/#cdn","title":"CDN","text":"<p>Create new profile</p> Azure Portal <p></p> <ol> <li>Click Create a resource </li> <li>Click Web</li> <li>Click CDN, opening the CDN profile blade</li> <li>Specify name for the profile, name of the resource group, region, and pricing tier.</li> <li>Click Create</li> </ol> <p>AZ-103: p. 140</p> <p>Create endpoint</p> Azure Portal <p></p> <p>Add an endpoint to a CDN profile (Portal) 1. Open the CDN Profile 2. Click + Endpoint button 3. Specify unique name, configuration for origin settings such as type, host header, and origin port for HTTP and HTTPS. 4. Click Add button</p> <p>AZ-103: p. 141</p> <p>Publish content in a CDN endpoint</p> Azure Portal <ol> <li>Create a new CDN profile</li> <li>Add an endpoint to the profile</li> </ol>"},{"location":"Cloud/Tasks/#dns","title":"DNS","text":"<p>Create DNS zone</p> <pre><code>New-AzDnsZone \n    -Name examref.com \n    -ResourceGroupName ExamRefRG\n</code></pre> <pre><code>az network dns zone create \n    --name examref.com \n    --resource-group ExamRefRG\n</code></pre> <p>Create empty A record</p> <pre><code>New-AzDnsRecordSet -Name www -RecordType A -ZoneName examref.com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords (New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\")\n</code></pre> <pre><code>az network dns record-set a create --name www --zone-name examref.com --resource-group ExamRefRG --ttl 3600\n</code></pre> <p>Create multiple records</p> <pre><code>$records = @()\n$records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\"\n$records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\"\nNew-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref.com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records\n</code></pre> <pre><code>az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1.2.3.4\naz network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 5.6.7.8\n</code></pre> <p>Remove record</p> <pre><code>$recordset = Get-AzDnsRecordSet -Name www -RecordType A -ZoneName examref.com -ResourceGroupName ExamRefRG\nAdd-AzdnsRecordConfig -RecordSet $recordset -IPv4Address \"5.6.7.8\"\nRemove-AzDnsRecordConfig -RecordSet $recordset -IPv4Address \"1.2.3.4\"\nSet-AzDnsRecordSet -RecordSet $recordset\n</code></pre> <pre><code>az network dns record-set a remove-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1.2.3.4\n</code></pre> <p>Read records</p> <pre><code>Get-AzDnsRecordSet -ZoneName examref.com -ResourceGroupName ExamRefRG\n</code></pre> <pre><code>az network dns record-set list --zone-name examref.com --resource-group ExamRefRG -o table \n</code></pre> <p>Create a virtual network with custom DNS settings</p> <pre><code>New-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rgName -Location $location \n    -AddressPrefix 10.1.0.0/16 -Subnet (New-AzVirtualNetworkSubnetConfig -Name Default -AddressPrefix 10.1.0.0/24)\n    -DNSServer 10.0.0.4,10.0.0.5 \n</code></pre> <pre><code>az network vnet create --name VNet1 --resource-group $rgName \n    --address-prefixes 10.0.0.0/16 \n    --dns-servers 10.0.0.4 10.0.0.5\n</code></pre> <p>Modify the DNS server configuration of an existing VNET</p> <pre><code>$vnet = Get-AzVirtualNetwork -Name $vnetName -ResourceGroupName $rgName\n$vnet.DhcpOptions.DnsServers.Clear()\n$vnet.DhcpOptions.DnsServers.Add(\"10.10.200.1\")\n$vnet.DhcpOptions.DnsServers.Add(\"10.10.200.2\")\nSet-AzVirtualNetwork -VirtualNetwork $vnet\n</code></pre> <pre><code>az network vnet update --name $vnetName --resource-group $rgName \n    --dns-servers 10.10.200.1 10.10.200.2\n</code></pre> <p>Restart the VMs in the VNet to pick up the DNS change</p> <pre><code>$vm = Get-AzVM -Name VNet1-VM -ResourceGroupName ExamRefRG\nRestart-AzVM -ID $vm.Id\n</code></pre> <p>Update the DNS settings on a NIC</p> <pre><code>$nic = Get-AzNetworkInterface -Name VM1-NIC -ResourceGroupName ExamRefRG\n$nic.DnsSettings.DnsServers.Clear()\n$nic.DnsSettings.DnsServers.Add(\"8.8.8.8\")\n$nic.DnsSettings.DnsServers.Add(\"8.8.4.4\")\n</code></pre> <p>Commit the DNS change, causing the VM to restart</p> <pre><code>Set-AzNetworkInterface -NetworkInterface $nic\n</code></pre> <p>Remove custom DNS servers from a VNET</p> <pre><code>az network vnet update --name VNet1 --resource-group ExamRefRG --remove DHCPOptions.DNSServers\n</code></pre> <p>Set custom DNS servers on a NIC</p> <pre><code>az network nic update --name VM1-NIC --resource-group ExamRefRG --dns-servers 8.8.8.8 8.8.4.4\n</code></pre>"},{"location":"Cloud/Tasks/#load-balancing","title":"Load balancing","text":"<p>Create public load balancer</p> <p>Creating a load balancer in PowerShell requires defining objects which are all passed to <code>New-AzLoadBalancer</code> as objects: - Frontend IP    - Public Ip Address resource (if public)   - Private IP address specified as a string (if internal) - Backend address pool - Health probe - Load balancing rule</p> <p>By contrast, in Azure CLI, the load balancer can be defined first with <code>az network lb create</code> before adding a probe and rule, passing the name of the load balancer to <code>--lb-name</code>.</p> <pre><code>$publicIP = New-AzPublicIpAddress -Name ExamRefLB-IP -ResourceGroupName $g -Location $location -AllocationMethod Static \n$frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PublicIpAddress $publicIP\n$beAddressPool = New-AzLoadBalancerBackendAddressPoolConfig -Name backend\n$healthProbe = New-AzLoadBalancerProbeConfig -Name -RequestPath '/' -Protocol http -Port 80\n\n$lbrule = New-AzLoadBalancerRuleConfig -Name -FrontendIpConfiguration $frontendIP -BackendAddressPool $beAddressPool -Probe $healthProbe -Protocol Tcp -FrontendPort 80 -BackendPort 80\n$lb = New-AzLoadBalancer -ResourceGroupName -Name -Location -FrontendIpConfiguration $frontendIP -LoadBalancingRule $lbrule -BackendAddressPool $beAddressPool -Probe $healthProbe\n</code></pre> <pre><code>az network public-ip create --name ExamRefLB-IP --resource-group ExamRefRG --location --allocation-method Static\naz network lb create --name ExamRefLB --resource-group ExamRefRG --location --backend-pool-name backend --frontend-ip-name frontend --public-ip-address ExamRefLB-IP\naz network lb probe create --resource-group ExamRefRG --name HealthProbe --lb-name ExamRefLB --protocol http --port 80 --path / --interval 5 --threshold \n\naz network lb rule create --name ExamRefRule --lb-name ExamRefLB --resource-group ExamRefRG --protocol Tcp --frontend-port 80 --backend-port 80 --frontend-ip-name ExamRefFrontEnd --backend-pool-name backend --probe-name HealthProbe\n</code></pre>"},{"location":"Cloud/gcp/","title":"Google Cloud Platform","text":""},{"location":"Cloud/AWS/","title":"Overview","text":"<p>Resources</p> <ul> <li>AWS CLI docs</li> </ul> <p>Global infrastructure:</p> <ul> <li>31 regions</li> <li>99 availability zones</li> </ul> <p>A region is a geographical area</p> <p>Availability zone is a datacenter.</p> <p>All availability zones are within 100 km of each other, although each is separated from another by a significant distance.</p> <p>Edge locations are endpoints used for caching content through CloudFront.</p>"},{"location":"Cloud/AWS/#aws-cli","title":"AWS CLI","text":""},{"location":"Cloud/AWS/#login","title":"Login","text":"<p>AWS CLI options, including login credentials, are managed with the configure command.</p> <pre><code># Interactively provide Access and Secure Keys\naws configure\n</code></pre> <p>The keys are stored in a cleartext INI config at $HOME/.aws/credentials; AWS CLI settings are stored in another INI config at $HOME/.aws./config.</p> <p>Configuration values, including the secret key, can be displayed using get</p> <pre><code>aws configure get aws_secret_access_key\n</code></pre> <p>Credentials can also be displayed or exported using export-credentials</p> <pre><code>aws configure export-credentials\n</code></pre>"},{"location":"Cloud/AWS/#profiles","title":"Profiles","text":"<p>Without specifying one, a profile named \"default\" is created on first providing access credentials.</p> <p>Additional profiles can be created to manage multiple accounts.</p> Profiles<pre><code># Start a new profile named corp for corporate access\naws configure --profile corp\n\n# Confirm\naws configure list-profiles\n</code></pre>"},{"location":"Cloud/AWS/#eks","title":"EKS","text":"<p>Elastic Kubernetes Service </p>"},{"location":"Cloud/AWS/#tasks","title":"Tasks","text":""},{"location":"Cloud/AWS/#clusters","title":"Clusters","text":"<pre><code>aws eks list-clusters\n\n# Add new context to .kube/config\naws eks --region $(terraform output -raw region) update-kubeconfig --name $(terraform output -raw cluster_name)\n</code></pre>"},{"location":"Cloud/AWS/#other-services","title":"Other services","text":"<ul> <li> <ul> <li>Backup</li> </ul> </li> </ul>"},{"location":"Cloud/AWS/#shared-storage","title":"Shared storage","text":"<ul> <li>EFS (Elastic File Store) is a managed NFS service that can be shared among thousands of EC2 instances for Linux</li> <li>FSx is a managed SMB file service for Windows</li> <li>FSx for Lustre </li> </ul>"},{"location":"Cloud/AWS/#systems-manager","title":"Systems Manager","text":"<p>Most Amazon Machine Images (AMI) come with the Systems Manager agent preinstalled. The IAM policy AmazonSSMManagedInstanceCore needs to be attached at a minimum.</p>"},{"location":"Cloud/AWS/ebs/","title":"EBS","text":"<p>Elastic Block Store offers block volumes for use with EC2 instances. Volumes are categorized by underlying technology (SSD or HDD) as well as by use case (IOPS or throughput).</p> <ul> <li>gp2 and gp3 are general-purpose SSD-backed volumes intended for use as boot volumes</li> <li>io1 and io2 are SSD-backed volumes optimized for IOPS and intended for use with transaction databases</li> <li>st1 or \"standard\" is a HDD-backed volume optimized for throughput which cannot be used as a boot volume</li> <li>sc1 is a cold volume intended for low-cost HDD-backed storage of data requiring infrequent access</li> </ul> <p>EBS volumes must be in the same region and AZ as the EC2 instance to which it is attached.</p> <p>EBS volumes are encrypted with AES-256 using KMS customer master keys.</p>"},{"location":"Cloud/AWS/ebs/#snapshots","title":"Snapshots","text":"<p>EBS volumes can be backed up by taking snapshots, which are stored in S3 buckets which cannot be accessed directly, although snapshots can be managed through the EC2 console. Snapshots are also how you would copy an EBS volume to another region.</p>"},{"location":"Cloud/AWS/ec2/","title":"EC2","text":"<p>Elastic Compute Cloud</p> <p>Security groups define permissible network communications in a VPS and are equivalent to firewalls in physical networks.</p> <p>Bootstrap scripts are run when an instance boots for the first time and allow for the automated installation of applications.</p>"},{"location":"Cloud/AWS/ec2/#tasks","title":"Tasks","text":""},{"location":"Cloud/AWS/ec2/#create-instance","title":"Create instance","text":"<pre><code>aws ec2 run-instances --image-id $AMI\n</code></pre>"},{"location":"Cloud/AWS/ec2/#metadata","title":"Metadata","text":"<p>Metadata can be retrieved from within the instance by curling 169.254.169.254:</p> <pre><code>TOKEN=$(curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")\nPUBLIC_IP=$(curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/public-ipv4)\n</code></pre> <p>Resources:</p> <ul> <li>EC2 metadata and user data</li> </ul>"},{"location":"Cloud/AWS/iam/","title":"IAM","text":"<p>IAM works at the Global level, and is not effective against specific regions.</p> <p>The root account is the email address used to sign up for AWS, which has full administrative access to AWS.</p> <p>Policy documents are JSON objects that define IAM permissions for principals or roles. The following JSON policy document describes unlimited access to all resource types, and is the output provided by the AWS-managed AdministratorAccess IAM policy.</p> Policy document <p>Identity Providers allow connections to solutions like Active Directory.</p> <p>An IAM role is an identity with specific permission policies. Instead of being uniquely and permanently associated with a single person, a role can be temporarily assumed by anyone who needs to use it. Roles can be assumed by people, AWS resources, or other system-level accounts.</p> <p>Resources:</p> <ul> <li>AWS Policy Generator</li> </ul>"},{"location":"Cloud/AWS/iam/#arn","title":"ARN","text":"<p>In addition to the friendly names provided to resources, the permissions policy language requires resources to be specified using the Amazon Resource Name (ARN) format.</p> ARN format<pre><code>arn:partition:service:region:account-id:resource-id\n</code></pre> Commands requiring an ARN<pre><code># Note that only hardware MFA devices are supported with this API\naws iam get-mfa-device \\\n    --serial-number 'arn:aws:iam::123456789012:mfa/Device'\n\n# Note that \"aws\" is provided as the account ID for AWS managed policies, \n# and there is no region specified because the resource is global.\naws iam attach-group-policy \\\n    --group-name Admins \\\n    --policy-arn 'arn:aws:iam::aws:policy/AdministratorAccess'\n</code></pre>"},{"location":"Cloud/AWS/iam/#tasks","title":"Tasks","text":""},{"location":"Cloud/AWS/iam/#iam-user-and-group","title":"IAM user and group","text":"AWS CLI Terraform <pre><code>aws iam create-user \\\n    --user-name Bob\n\naws iam create-group \\\n    --group-name Admins \\\n\n# Confirm\naws iam list-users\naws iam list-groups\n\naws iam add-user-to-group \\\n    --group-name Admins \\\n    --user-name Bob\n\n# Confirm\naws iam list-groups-for-user \\\n    --user-name Bob\n\naws iam attach-group-policy \\\n    --group-name Admins \\\n    --policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n\n# Confirm\naws iam list-attached-group-policies \\\n    --group-name Admins\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Cloud/AWS/iam/#create-administrator","title":"Create administrator","text":"<pre><code>aws iam create-access-key --user-name $USER\n</code></pre>"},{"location":"Cloud/AWS/ml/","title":"Machine learning","text":""},{"location":"Cloud/AWS/ml/#text-analysis","title":"Text analysis","text":"<ul> <li> <p>Comprehend     Amazon Comprehend uses NLP to extract insights about the contents of documents: entity extraction, sentiment analysis, document clustering, and syntax.</p> </li> <li> <p>Kendra     Amazon Kendra is an intelligent search service that uses NLP and machine learning to return specific answers to search questions.</p> </li> <li> <p>Textract: managed OCR</p> </li> </ul>"},{"location":"Cloud/AWS/ml/#speech","title":"Speech","text":"<ul> <li> <p>Lex: chatbots</p> </li> <li> <p>Polly: text-to-speech</p> </li> <li> <p>Transcribe: transcription</p> </li> </ul>"},{"location":"Cloud/AWS/ml/#sagemaker","title":"SageMaker","text":"<p>SageMaker is a service that enables data scientists and developers to quickly and easily build, train, and deploy ML models.</p> <p>Exploratory data analysis is facilitated by the ability to quickly create hosted Jupyter notebook instances which are preloaded with popular Python frameworks and libraries. Using pip install additional packages can be added to the notebook instance.</p> <p>SageMaker facilitates running training jobs by provisioning and tearing down EC2 clusters.</p> <p>One or more models can be deployed to an inference endpoint hosted on SageMaker.</p> <p>Resources:</p> <ul> <li>PluralSight: Introduction to Amazon SageMaker</li> </ul>"},{"location":"Cloud/AWS/ml/#others","title":"Others","text":"<ul> <li> <p>Forecast: predictions using time-series data.</p> </li> <li> <p>Fraud Detector</p> </li> <li> <p>Rekognition: computer vision</p> </li> </ul>"},{"location":"Cloud/AWS/s3/","title":"S3","text":"<p>Simple Storage Service (S3) was launched in 2006 and one of the first AWS services offered.</p> <p>S3 objects are stored in logical containers called buckets, all of which exist in a single global namespace. but any created bucket is created in a specified region.</p> S3 bucket URLs<pre><code>https://$BUCKET.s3.$REGION.amazonaws.com/\n</code></pre> <p>Bucket policies apply to the entire bucket. Object ACLs are specific to the object.</p>"},{"location":"Cloud/AWS/s3/#storage-classes","title":"Storage classes","text":"<ul> <li> <p>S3 Standard (2.3&amp;cent/GB/mo for first 50 TB)     Data that is regularly accessed by cloud-hosted applications.     The first 50 TB is charged at</p> </li> <li> <p>S3 Standard-IA (Infrequent Access) (1.25\u00a2/GB/mo)     Data that is less frequently accessed, offered at a cheaper rate than Standard.</p> </li> <li> <p>S3 One Zone-IA     Provides less durability than Standard-IA because it is stored in a single AZ instead of across 3 AZs, offered at price 20% less than Standard IA.</p> </li> </ul> <p></p> <p>Glacier is a cheap storage option for long-term archival storage. Costs are incurred per access. In the context of Glacier, objects are referred to as archives.</p> <ul> <li> <p>Glacier Instant Retrieval (0.4\u00a2/GB/mo)     Long-term data archiving with millisecond retrieval,      Intended for long-lived data that is accessed once per quarter.</p> </li> <li> <p>Glacier Flexible Retrieval (0.36\u00a2/GB/mo)     Retrieve large sets of data at no cost, i.e. backup or DR, but with no instant retrieval.     Retrieval can take minutes to hours.</p> </li> <li> <p>Glacier Deep Archive (0.099\u00a2/GB/mo)     Data storage on timescales of 7-10 years or longer to meet regulatory compliance requirements.     Retrieval time 12-48 hours.     Intended to be accessed less than once a year.     0.099\u00a2/GB/mo.</p> </li> </ul> <p>Read more:</p> <ul> <li>Getting Started with Amazon S3 Glacier</li> <li>AWS CLI reference for S3 Glacier</li> <li>Amazon S3 Glacier storage classes</li> </ul>"},{"location":"Cloud/AWS/s3/#lifecycle-management","title":"Lifecycle management","text":"<p>S3 Lifecycle configuration defines rules to objects to ensure they are stored economically throughout their lifecycle by transitioning them to cheaper storage classes with age. Lifecycle rules, which are applied to the bucket, are based on object creation date and are most useful for logs, i.e. S3 buckets with Server Access Logging which can accumulate large log files in another bucket.</p> <p>Lifecycle actions include:</p> <ul> <li>Transition actions which define when objects transition to another storage class.</li> <li>Expiration actions which define when objects are to be expired and deleted automatically.</li> </ul> <p>Resources:</p> <ul> <li>Working with S3 Lifecycle Policies</li> <li>Lifecycle Management with S3</li> <li>Managing your storage lifecycle</li> </ul>"},{"location":"Cloud/AWS/s3/#object-locks","title":"Object locks","text":"<p>S3 Object Lock implements the write once, read many (WORM) model, preventing objects from being deleted or modified.</p> <p>There are two ways to manage object retention, one or both of which may be used on any object version:</p> <ul> <li> <p>Retention period: A retention period defines a fixed amount of time for protecting an object version and can be applied on individual objects or the entire bucket.</p> </li> <li> <p>Legal Hold: A legal hold is a different type of object lock that remains in effect until it's removed and has no expiration date.</p> </li> </ul> <p>Object Lock offers two retention modes:</p> <ul> <li> <p>In governance mode, users require special permissions (s3:BypassGovernanceRetention as well as the x-amz-bypass-governance-retention:true as a request header on any API call) to overwrite or delete an object version or alter lock settings. </p> </li> <li> <p>In compliance mode, a protected object version can't be overwritten or deleted even by the root user.</p> </li> </ul> <p>S3 Glacier Vault Lock allows compliance controls to be deployed for individual vaults with a vault lock policy.</p> <p>Read more:</p> <ul> <li>Using S3 Object Lock</li> </ul>"},{"location":"Cloud/AWS/s3/#encryption","title":"Encryption","text":"<p>Types of encryption:</p> <ul> <li>Encryption in Transit<ul> <li>SSL/TL</li> <li>HTTPS</li> </ul> </li> <li>Encryption at Rest: server-side encryption (SSE)<ul> <li>SSE-S3: S3-managed keys, using AES 256-bit encryption</li> <li>SSE-KMS: AWS Key Management Service-managed keys</li> <li>SSE-C: Customer-provided keys</li> </ul> </li> <li>Encryption at Rest: client-side encryption<ul> <li>Encryption of files before upload to S3</li> </ul> </li> </ul> <p>Read more:</p> <ul> <li>Protecting data with encryption</li> </ul>"},{"location":"Cloud/AWS/s3/#replication","title":"Replication","text":"<p>S3 Replication can be used to backup data from one bucket to another, across regions or within the same region. Versioning must be enabled on both buckets, but delete markers are not replicated by default. Existing objects are not replicated automatically, but all subsequent updates to objects will be replicated.</p> <p>Read more:</p> <ul> <li>Amazon S3 User Guide: Replicating objects</li> <li>AWS CLI Command Reference: put-bucket replication</li> </ul>"},{"location":"Cloud/AWS/s3/#tasks","title":"Tasks","text":""},{"location":"Cloud/AWS/s3/#create-a-public-bucket","title":"Create a public bucket","text":"AWS CLI Terraform <pre><code># Create bucket\naws s3 mb s3://$BUCKET # (1)\n\n# Confirm\naws s3 ls # (2)\n\n# Allow public access by removing the PublicAccessBlock configuration on the bucket\naws s3api delete-public-access-block \\\n    --bucket $BUCKET\n\n# Apply a policy\naws s3api put-bucket-policy --bucket $BUCKET --policy file://policy.json # (3)\n</code></pre> <ol> <li>Alternatively, using the lower-level s3api subcommand. <pre><code># Create bucket\naws s3api create-bucket --bucket $BUCKET\n</code></pre></li> <li><pre><code># Confirm\naws s3api list-buckets\n</code></pre></li> <li>policy.json<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:GetObjectVersion\"\n      ],\n      \"Resource\": \"arn:aws:s3:::$BUCKET/*\"\n    }\n  ]\n}\n</code></pre></li> </ol> Upload file<pre><code># Upload $FILE to appear at $ROUTE\naws s3api put-object \\\n    --bucket $BUCKET \\\n    --key $ROUTE \\\n    --body $FILE\n</code></pre> <p>ACLs are deprecated and no longer enabled by default: bucket policies are preferred. Enabling them requires changing the bucket ownership rule.</p> Enable ACL<pre><code># Enable ACLs by changing the bucket's ownership controls setting\naws s3api put-bucket-ownership-controls \\\n    --bucket $BUCKET \\\n    --ownership-controls 'Rules=[{ObjectOwnership=BucketOwnerPreferred}]'\n\n# Grant read permission on the ACL\naws s3api put-object-acl \\\n    --bucket $BUCKET \\\n    --key $ROUTE \\\n    --grant-read uri='http://acs.amazonaws.com/groups/global/AllUsers'\n\n# Confirm\naws s3api get-object-acl \\\n    --bucket $BUCKET \\\n    --key $ROUTE # (3)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Cloud/AWS/s3/#s3-static-website","title":"S3 static website","text":"AWS CLI <pre><code># Sync contents of $PATH with a specified bucket. The s3:// URI can take the ARN or the bucket name and path.\naws s3 sync $PATH s3://$BUCKET\n\n# Allow public access\naws s3api delete-public-access-block --bucket $BUCKET\n\naws s3 website --index-document index.html --error-document error.html s3://$BUCKET\n</code></pre> Static website policy document<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": {\n        \"Sid\": \"PublicReadGetObject\",\n        \"Effect\": \"Allow\",\n        \"Principal\": \"*\",\n        \"Action\": [\"s3:GetObject\"],\n        \"Resource\": [ \"arm:aws:s3:::BUCKET_NAME/*\"]\n    }\n}\n</code></pre> <p>Note that there appears to be a step missing in these directions. Navigating to a nonexistent file fails to produce the error page, but rather produces a 403 error. </p>"},{"location":"Cloud/AWS/s3/#s3-versioning","title":"S3 versioning","text":"<p>Versioning is a bucket-level configuration that allows prior edits to files to be made available to administrators. Versioning cannot be disabled, only suspended.</p>  AWS CLI Terraform <pre><code>aws s3api put-bucket-versioning \\\n    --bucket $BUCKET \\\n    --versioning-configuration 'Status=Enabled'\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Cloud/AWS/s3/#delete-bucket","title":"Delete bucket","text":"<p>A bucket must be emptied of all contents before it can be deleted.</p> <pre><code># --force deletes all objects in the bucket\naws s3 rb $BUCKET --force # (1)\n</code></pre> <ol> <li>Using the s3api subcommand, the bucket must first be emptied of all objects. <pre><code>aws s3api delete-objects --bucket $BUCKET\n\naws s3api delete-bucket --bucket $BUCKET\n</code></pre></li> </ol>"},{"location":"Cloud/Azure/","title":"Azure","text":"<p>Azure CLI</p> accountadaksconfigdeploymentgroupinteractivenetworkstoragevm config <pre><code># Get details for default subscription\naz account show\n\n# List subscriptions for the logged-in account\naz account list --output table\n\n# Set specific subscription as default\naz account set --subscription $SUBSCRIPTION\n\n# Display available locations\naz account list-locations\n\n# Logout\naz account clear\n</code></pre> <pre><code>\n</code></pre> <pre><code># List managed Kubernetes clusters\naz aks list\n\n# Inspect a single cluster\naz aks show -g $RG -n $CLUSTER_NAME -o table\n\n# Get Kubernetes context, saving it in $HOME/.kube/config\naz aks get-credentials -g $RG -n $CLUSTER_NAME\n\n# Display available AKS versions in the location\naz aks get-versions -l eastus\n\n# Get upgrade versions available for a specific cluster\naz aks get-upgrades -g $RG -n $CLUSTER_NAME\n\n# Upgrade an AKS cluster\naz aks upgrade -g $RG -n $CLUSTER_NAME  \\\n    --kubernetes-version \"1.28.5\"\n\n# Upgrade only the control plane of an AKS cluster\naz aks upgrade -g $RG -n $CLUSTER_NAME  \\\n    --control-plane-only                \\\n    --kubernetes-version \"1.28.5\"\n\n# Upgrade node pool of a cluster\naz aks nodepool upgrade -g $RG -n $NODEPOOL_NAME \\\n    --kubernetes-version \"1.28.5\"\n</code></pre> <pre><code># Settings are saved to $HOME/.azure/config by default\n\n# Set default location to East US 2\naz config defaults.location=eastus2\n</code></pre> <pre><code># Deploy to a resource group\naz deployment group create -g $GROUP --template-file $TEMPLATE_FILE_PATH\n\n# Deploy to a subscription\naz deployment sub create -l $LOCATION --template-file $TEMPLATE_FILE_PATH\n</code></pre> <pre><code># Create new resource group\naz group create -n $RG_NAME -l $LOCATION\n\n# Check if a specified resource group exists\naz group exists -n $RG_NAME\n</code></pre> <p>Enter an interactive environment to run Azure CLI commands.</p> vnet <pre><code>az network vnet list\n\n# Create a vnet, using default address prefix of 10.0.0./16\naz network vnet create -n $VNET_NAME -g $RG_NAME -l $LOCATION\n\n# Inspect specified vnet\naz network vnet show -n $VNET_NAME -g $RG_NAME\n\n# Display subnets of a vnet\naz network vnet subnet list -g $RG_NAME --vnet-name $VNET_NAME\n\n# Create a subnet, providing name, resource group, and vnet name\naz network vnet subnet create -n $SUBNET_NAME -g $RG_NAME --vnet-name $VNET_NAME\n\n\n# Create a vnet and subnets at the same time (using default address prefix)\naz network vnet create -n $VNET_NAME -g $RG_NAME -l $LOCATION \\\n    --subnet-name $SUBNET_NAME --subnet-prefixes $SUBNET_PREFIXES\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>Settings are saved to $HOME/.azure/config by default</p>"},{"location":"Cloud/Azure/#networking","title":"Networking","text":""},{"location":"Cloud/Azure/#virtual-network","title":"Virtual network","text":"<p>A vnet can only be in a single Azure region and can only connect to other Azure resources in that region. Creating a vnet requires specifying an address prefix: one or more non-overlapping IPv4 address ranges specified in CIDR notation (10.0.0.0/16 is used by default if one is not provided. A subnet is a component of a vnet that takes up a portion of its address space. Different routing, traffic, and security policies can be applied to each subnet.</p> <ul> <li> <p>Subnet delegation (1) enables a subnet to be designated for a specific Azure PaaS service.     This gives the injected service explicit permissions to create service-specific resources in the subnet when deploying a service.</p> <ol> <li> Azure CLI <pre><code># Subnet delegation\naz network vnet subnet update -g $RG_NAME -n $SUBNET_NAME --vnet-name $VNET_NAME \\\n    --delegations Microsoft.Sql/managedInstances\n</code></pre> </li> </ol> </li> <li> <p>Bastion </p> </li> </ul>"},{"location":"Cloud/Azure/#storage","title":"Storage","text":"<p>Blob Storage allows large volumes of unstructured data to be stored in the cloud. A storage account is a unique namespace that can store various containers. Blobs are text or binary data, organized into an unlimited number of containers, which provide structure to a storage account similar to folders. Container names must be between 3 and 63 characters long, composed of numbers, lowercase characters, and dashes.</p>"},{"location":"Cloud/Azure/ARM/","title":"ARM","text":"<p>Azure Resource Manager (ARM) is the interface for managing and organizing cloud resources.</p> <p>An ARM template is a JSON file that precisely defines all ARM resources in a deployment. An ARM template can be deployed into a resource group as a single operation.</p> <p>Azure templates can be targeted to a resource group, subscription, management group, or tenant.</p> <p>ARM templates are typically adapted from existing Azure Quickstart templates, which are contributed by the community and hosted on a gallery. The Azure Resource Manager Visualizer assists users in seeing what the template will do before actually deploying.</p> <p>The Custom Script Extension is a way to run scripts on Azure VMs and represents one of the ways to automate configuration of new deployments. <sup>?</sup></p> <p>If you export a deployment to a template, only the resources deployed in that deployment will be templatized. In the case of a complex deployment that had several phases, the ultimate result of the deployment can be obtained by exporting the template from the resource group.</p>"},{"location":"Cloud/Azure/ARM/#structure","title":"Structure","text":"<p>A template must have at least the following sections.</p> <ul> <li><code>$schema</code></li> <li><code>contentVersion</code></li> <li><code>resources</code></li> </ul> <p>A template may have the following optional sections</p> <ul> <li><code>parameters</code></li> <li><code>variables</code></li> <li><code>functions</code></li> <li><code>outputs</code></li> </ul> <pre><code>{\n  \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"\",\n  \"parameters\": {  },\n  \"variables\": {  },\n  \"functions\": [  ],\n  \"resources\": [  ],\n  \"outputs\": {  }\n}\n</code></pre>"},{"location":"Cloud/Azure/ARM/#resources","title":"Resources","text":"<p>Create a public IP address.</p> <pre><code>{ \"type\": \"Microsoft.Network/publicIPAddresses\",\n  \"apiVersion\": \"2018-08-01\",\n  \"name\": \"[variables('publicIPAddressName')]\",\n  \"location\": \"[parameters('location')]\",\n  \"properties\": {\n    \"publicIPAllocationMethod\": \"Dynamic\",\n    \"dnsSettings\": {\n      \"domainNameLabel\": \"[parameters('dnsLabelPrefix')]\" } } }\n</code></pre> <p>Create a storage account <pre><code>{ \"type\": \"Microsoft.Storage/storageAccounts\",\n  \"apiVersion\": \"2019-06-01\",\n  \"name\": \"[variables('storageAccountName')]\",\n  \"location\": \"[parameters('location')]\",\n  \"sku\": {\n    \"name\": \"[parameters('storageAccountType')]\"\n  },\n  \"kind\": \"StorageV2\",\n  \"properties\": {} }\n</code></pre></p>"},{"location":"Cloud/Azure/ARM/#parameters","title":"Parameters","text":"<pre><code>\"adminUsername\": {\n  \"type\": \"string\",\n  \"metadata\": {\n    \"description\": \"Username for the Virtual Machine.\" }},\n\"adminPassword\": {\n  \"type\": \"securestring\",\n  \"metadata\": {\n    \"description\": \"Password for the Virtual Machine.\" }}\n</code></pre> <p>Simple storage account</p> <pre><code>{ \"storageAccountType\": {\n    \"type\": \"string\",\n    \"defaultValue\": \"Standard_LRS\",\n    \"allowedValues\": [\n      \"Standard_LRS\",\n      \"Standard_GRS\",\n      \"Standard_ZRS\",\n      \"Premium_LRS\" ],\n    \"metadata\": {\n      \"description\": \"Storage Account type\" }},\n  \"location\": {\n    \"type\": \"string\",\n    \"defaultValue\": \"[resourceGroup().location]\",\n    \"metadata\": {\n      \"description\": \"Location for all resources.\" }}}\n</code></pre>"},{"location":"Cloud/Azure/ARM/#variables","title":"Variables","text":"<p><pre><code>{ \"nicName\": \"myVMNic\",\n  \"addressPrefix\": \"10.0.0.0/16\",\n  \"subnetName\": \"Subnet\",\n  \"subnetPrefix\": \"10.0.0.0/24\",\n  \"publicIPAddressName\": \"myPublicIP\",\n  \"virtualNetworkName\": \"MyVNET\" }\n</code></pre> Simple storage account <pre><code>{ \"storageAccountName\": \"[concat('store', uniquestring(resourceGroup().id))]\" },\n</code></pre></p>"},{"location":"Cloud/Azure/ARM/#functions","title":"Functions","text":"<p>Create a globally unique name, useful for some resources that require it. <code>concat</code> is a built-in function. <pre><code>[ \n  { \"namespace\": \"contoso\",\n    \"members\": {\n      \"uniqueName\": {\n        \"parameters\": [\n          {\n            \"name\": \"namePrefix\",\n            \"type\": \"string\"\n          }\n        ],\n        \"output\": {\n          \"type\": \"string\",\n          \"value\": \"[concat(toLower(parameters('namePrefix')), uniqueString(resourceGroup().id))]\" \n        }\n      }\n    }\n  }\n]\n</code></pre></p>"},{"location":"Cloud/Azure/ARM/#outputs","title":"Outputs","text":"<pre><code>\"outputs\": {\n  \"hostname\": {\n    \"type\": \"string\",\n    \"value\": \"[reference(variables('publicIPAddressName')).dnsSettings.fqdn]\"\n  }\n}\n</code></pre> <p>Standard storage account</p> <pre><code>{\n  \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"\",\n  \"parameters\": {  },\n  \"variables\": {  },\n  \"functions\": [  ],\n  \"resources\": [  ],\n  \"outputs\": {\n    \"storageAccountName\": {\n      \"type\": \"string\",\n      \"value\": \"[variables('storageAccountName')]\"\n    }\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/ARM/#tasks","title":"Tasks","text":"<p> Deploy a VM quickstart template</p>"},{"location":"Cloud/Azure/ARM/#create-a-resource-group","title":"Create a resource group","text":"<p>Create a resource group  <pre><code>RESOURCEGROUP=learn-quickstart-vm-rg\nLOCATION=eastus\naz group create --name $RESOURCEGROUP --location $LOCATION\n</code></pre> Create template parameters</p>"},{"location":"Cloud/Azure/ARM/#validate-template","title":"Validate template","text":"<p><pre><code>USERNAME=azureuser\nPASSWORD=$(openssl rand -base64 32)\nDNS_LABEL_PREFIX=mydeployment-$RANDOM\n</code></pre> <pre><code>az deployment group validate --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername=$USERNAME --parameters adminPassword=$PASSWORD --parameters dnsLabelPrefix=$DNS_LABEL_PREFIX\n</code></pre></p>"},{"location":"Cloud/Azure/ARM/#deploy-template","title":"Deploy template","text":"<p><pre><code>USERNAME=azureuser\nPASSWORD=$(openssl rand -base64 32)\nDNS_LABEL_PREFIX=mydeployment-$RANDOM\n</code></pre> <pre><code>az deployment group create --name MyDeployment --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername=$USERNAME --parameters adminPassword=$PASSWORD --parameters dnsLabelPrefix=$DNS_LABEL_PREFIX\n</code></pre> Verify deployment <pre><code>az deployment group show --name MyDeployment --resource-group $RESOURCEGROUP\n</code></pre></p>"},{"location":"Cloud/Azure/ARM/#arm","title":"ARM","text":"<p>?</p> <pre><code>{\n  \"name\": \"[concat(variables('vmName'), '/', 'ConfigureIIS')]\",\n  \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n  \"apiVersion\": \"2018-06-01\",\n  \"location\": \"[parameters('location')]\",\n  \"properties\": {\n    \"publisher\": \"Microsoft.Compute\",\n    \"type\": \"CustomScriptExtension\",\n    \"typeHandlerVersion\": \"1.9\",\n    \"autoUpgradeMinorVersion\": true,\n    \"settings\": {\n      \"fileUris\": [\n        \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"\n      ]\n    },\n    \"protectedSettings\": {\n      \"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"\n    }\n  },\n  \"dependsOn\": [\n    \"[resourceId('Microsoft.Compute/virtualMachines/', variables('vmName'))]\"\n  ]\n}\n</code></pre>"},{"location":"Cloud/Azure/Azure-Backup/","title":"Azure Backup","text":"<p>Azure Backup can backup on-prem servers, cloud-based VMs, and virtualized workloads like SQL Server and Sharepoint.  However Azure SQL databases are already backed up by an automatic service by default. <sup>AZ-103 p. 159</sup></p> <p>On-prem machines can be backed up using several agents <sup>AZ-103 p. 162</sup></p> <ul> <li>MARS Agent</li> <li>System Center Data Protection Manager (DPM) or Microsoft Azure Backup Server (MABS) can be used as backup servers. The backup server can then be backed up to a Recovery Services vault</li> </ul> <p>Azure VMs can be backed up</p> <ul> <li>Directly using an extension on the Azure VM Agent, which comes preinstalled on Marketplace images</li> <li>Specific files and folders on a VM can be backed up by running the MARS agent</li> <li>To the MABS running in Azure, which can then be backed up to a Recovery Services vault</li> </ul> <p>Storage accounts can be backed up, but not blob storage. Blob storage is already replicated locally, which provides fault-tolerance. Instead, you can use snapshots.</p> <p>When installed, the <code>Get-AzVM</code> command exposes a <code>ProvisionVMAgent</code> property with a boolean value under <code>OSProfile.WindowsConfiguration</code>.</p> <ul> <li></li> </ul>"},{"location":"Cloud/Azure/Azure-Backup/#containers","title":"Containers","text":"<p>There appear to be resources that house items to be protected that can be enumerated.</p>"},{"location":"Cloud/Azure/Azure-Backup/#reports","title":"Reports","text":"<p>Log Analytics workspaces must be located in the same region as the Recovery Services vault in order to store Backup reports.</p>"},{"location":"Cloud/Azure/Azure-Backup/#pre-checks","title":"Pre-Checks","text":"<p>Azure Backup pre-checks complete with various statuses that indicate potential problems</p> <ul> <li>Passed: VM configuration is conducive for successful backups</li> <li>Warning: Issues that might lead to backup failures</li> <li>Critical: Issues that will lead to backup failures</li> </ul>"},{"location":"Cloud/Azure/Azure-Backup/#tasks","title":"Tasks","text":"<p>Create Recovery Services Vault</p> Azure PortalAzure PowerShellAzure CLI <p></p> <pre><code>New-AzRecoveryServicesVault -Name $n -ResourceGroupName $rgName -Location $l\n</code></pre> <pre><code>az backup vault create --name $n --resource-group $rgName --Location $l\n</code></pre> <p>Enable MFA</p> <p>This requires MFA to be enabled.</p> Azure Portal <p> </p> <p>Enable multi-factor authentication for the Recovery services vault by going to the vault in the Portal, then Properties &gt; Security settings: Update &gt; Choose Yes in the dropdown. An option to generate a security PIN will appear in this same blade.</p> <p>Recover files</p> Azure Portal <p> </p> <p>Download the executable (for Windows VMs) or PowerShell script (for Linux VMs). A Python script is generated when downloading to a Linux machine.</p>"},{"location":"Cloud/Azure/Azure-Backup/#configure-backup-reports","title":"Configure Backup reports","text":"<p>Sources - Configure Azure Backup reports</p> <p>A Log Analytics workspace must exist.</p> <ol> <li>Turn on diagnostics in the Recovery Services vault</li> <li>Select Archive to a storage account (NOT Send to Log Analytics), providing a storage account to store information needed for report.</li> <li>Select <code>AzureBackupReport</code> under log section, which will collect all needed data models and information for the backup report.</li> <li>Connect to Azure Backup in PowerBI using a service content pack.</li> </ol> <p></p> <p>Define new backup protection policy</p> Azure PowerShell <pre><code>$SchPol = Get-AzRecoveryServicesBackupSchedulePolicyObject -WorkloadType \"AzureVM\" \n$SchPol.ScheduleRunTimes.Clear()\n$Dt = Get-Date\n$SchPol.ScheduleRunTimes.Add($Dt.ToUniversalTime())\n$RetPol = Get-AzRecoveryServicesBackupRetentionPolicyObject -WorkloadType \"AzureVM\" \n$RetPol.DailySchedule.DurationCountInDays = 365\nNew-AzRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\" -WorkloadType AzureVM -RetentionPolicy $RetPol -SchedulePolicy $SchPol\n</code></pre> <p>Configure VM backup</p> Azure PowerShellAzure CLI <pre><code>$policy = Get-AzRecoveryServicesBackupProtectionPolicy -Name \"DefaultPolicy\"\nEnable-AzRecoveryServicesBackupProtection -ResourceGroupName $g -Name $n -Policy $policy\n</code></pre> <pre><code># GRS by default\naz backup protection enable-for-vm -g $g -v $v --vm vm --policy-name DefaultPolicy\n\n# LRS\naz backup vault backup-properties set -n $v -g $g --backup-storage-redundancy \"LocallyRedundant\"\n</code></pre> <p>Initiate VM backup</p> Azure PowerShell <pre><code>$backupcontainer = Get-AzRecoveryServicesBackupContainer\n    -ContainerType \"AzureVM\"\n    -FriendlyName \"myVM\"\n\n$item = Get-AzRecoveryServicesBackupItem\n    -Container $backupcontainer\n    -WorkloadType \"AzureVM\"\n\nBackup-AzRecoveryServicesBackupItem -Item $item\n</code></pre> <p><code>--container-name</code>/<code>-c</code> appears to accept the name of the VM itself.</p> Azure CLI <pre><code>az backup protection backup-now -g myResourceGroup -n myRecoveryServicesVault --container-name myVM\n    --item-name myVM\n    --retain-until 18-10-2017\n    --backup-management-type AzureIaasVM\n</code></pre>"},{"location":"Cloud/Azure/Azure-Backup/#list-containers","title":"List containers","text":"<p><code>-BackupManagementType</code> accepts the following values - <code>AzureVM</code> - <code>MARS</code> - <code>AzureWorkload</code> - <code>AzureStorage</code></p> <p><code>-ContainerType</code> accepts: - <code>AzureVM</code> - <code>Windows</code> - <code>AzureSQL</code> - <code>AzureStorage</code> - <code>AzureVMAppContainer</code></p> <pre><code>$v = Get-AzRecoveryServicesVault -ResourceGroupName $rg -Name vault\nGet-AzRecoveryServicesBackupContainer -ContainerType Windows -BackupManagementType MARS -VaultId $v.ID\n</code></pre> <p>This returns a list of JSON objects. <code>--backup-management-type</code> accepts the following values: - <code>AzureIaasVM</code> - <code>AzureStorage</code> - <code>AzureWorkload</code> <pre><code>az backup container list -g $g -v $v --backup-management-type AzureIaasVM\n</code></pre> Preserve only the \"name\" attribute of the first item, which itself is a semicolon-delimited string of values. (Start backup now) <pre><code>az backup container list -g $g -v $v --backup-management-type AzureIaasVM --query [0].name\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Backup/#sources","title":"Sources","text":"<ul> <li>AZ-103: <code>2.4</code></li> <li> <p>AZ-104: <code>5.2</code></p> </li> <li> <p>Azure Backup architecture and components</p> </li> <li>Azure Virtual Machine Agent overview</li> <li>Understanding and using the Azure Linux Agent</li> <li>Restore files from VM</li> <li>Back up a VM - Azure CLI, PowerShell</li> <li><code>Get-AzRecoveryServicesBackupContainer</code></li> <li><code>New-AzRecoveryServicesBackupProtectionPolicy</code></li> <li><code>az backup container list</code></li> </ul>"},{"location":"Cloud/Azure/Azure-File-Service/","title":"Azure File Service","text":""},{"location":"Cloud/Azure/Azure-File-Service/#mount-azure-file-share","title":"Mount Azure File Share","text":""},{"location":"Cloud/Azure/Azure-File-Service/#windows","title":"Windows","text":"<p>Connect to and mount an Azure File Share (Windows File Explorer)</p> <ol> <li>Right-click on This PC</li> <li>Click Map Network Drive option</li> <li>Specify drive letter to be used </li> <li>Specify folder: <code>\\\\&lt;storageAccount&gt;.files.core.windows.net\\&lt;shareName&gt;</code></li> <li>Click Finish</li> <li>In the dialog box that opens login with the username: <code>AZURE\\&lt;storageName&gt;</code></li> <li>Password should be access key for the storage account</li> </ol> <p><pre><code>net use x \\\\erstandard01.file.core.windows.net\\logs /u:AZURE\\erstandard01 &lt;accessKey&gt;\n</code></pre> Automatically reconnect after reboot in Windows <pre><code>cmdkey /add:storageAccountName.file.core.windows.net /user:AZURE\\storageAccountName /pass:storageAccountKey\n</code></pre></p> <pre><code>$storageKey = (Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageNAme).Value[0]\n$acctKey = ConvertTo-SecureString -String $storageKey -AsPlainText -Force\n$credential = New-Object System.Management.Automation.PSCredential -ArgumentList \"Azure\\$storageName\", $acctKey\nNew-PSDrive -Name \"Z\" -PSProvider FileSystem -Root \"\\\\$storageName.file.core.windows.net\\$shareName\" -Credential $credential\n</code></pre>"},{"location":"Cloud/Azure/Azure-File-Service/#linux","title":"Linux","text":"<p>Mounting to <code>/logs</code> <pre><code>sudo mount -t cifs //$storageAccount.file.core.windows.net/logs /logs -o \"vers=3.0,username=$storageAccount,password=$storageAccountKey,dir_mode=0777,file_mode=0777,sec=ntlmssp\"\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-File-Service/#sources","title":"Sources:","text":"<ul> <li>Deploy Azure File Sync</li> <li>AZ-103: p. 148</li> </ul>"},{"location":"Cloud/Azure/Azure-IAM/","title":"Azure IAM","text":"<p>Access to Azure resources is governed by role-Based Access Controls (RBAC) are supported only by Azure Portal and the ARM APIs.  RBAC is configured by selecting a role and associating it with a security principal, such as a user, group, or service identity.  Child reosurces inherit the roles of their parents (\"role inheritance\").</p> <p>Classic subscription administrators</p> <p>Classic subscription administrators have full access to a subcription.  They can access resources through Azure Portal, ARM APIs (PowerShell and CLI), and classic deployment model APIs.</p> <p>By default, the account that is used to sign up for a subscription is automatically set as both Account Administrator and Service Administrator. </p> <p>There can only be one Account Administrator per account and only 1 Service Administrator per subscription. </p> <p>Co-Administrators have the same access as Service Administrators, and there can be 200 of them per subscription, but cannot change the association of subscriptions to directories.</p>"},{"location":"Cloud/Azure/Azure-IAM/#roles","title":"Roles","text":"<p>Components of a role assignment include:</p> <ul> <li>Security principal: objects associated with a role definition and a scope to apply RBAC to azure resources (i.e. a user, group, service principal, or managed identity which is an application registration that is managed automatically by Azure and an Azure service)<ul> <li>User principal: identity associated with a user or group of users.</li> <li>Service principal: identity associated with an application.</li> </ul> </li> <li>Role definition: list of permissions which define what actions can or cannot be performed against a resource. In addition to the 4 foundational built-in roles, there are many other built-in roles and custom roles can be defined using a JSON file.</li> <li>Scope</li> </ul>"},{"location":"Cloud/Azure/Azure-IAM/#scopes","title":"Scopes","text":"<p>There are four scopes at which RBAC can be applied:</p> <ul> <li>Management group</li> <li>Subscriptions</li> <li>Resource groups</li> <li>Resources</li> </ul> <p></p> <p>Azure RBAC roles can be used to grant rights to 2 types of principals:</p> <ul> <li>User principal: identity associated with a user or group of users.</li> <li>Service principal: identity associated with an application.</li> </ul> <p>RBAC roles can also be applied to a subscription through Management Groups, which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted. </p>"},{"location":"Cloud/Azure/Azure-IAM/#role-assignments","title":"Role assignments","text":"<p>Current assignments for classic admins can be seen in the Properties blade of a subscription in Azure Portal. Co-Administrator assignments can be added by opening the Access Control (IAM) blade of a subscription, then clicking the Add co-administrator button.</p> <p>RBAC roles are supported only by Azure Portal and the ARM APIs.  Access policy is applied to a scope, which includes subscriptions, resource groups, or resources: a policy applied to a subscription is said to be at the \"subscription scope\".  Policy can also be applied to Management Groups, which is an additional scope above subscription.  In this way, several subscriptions can inherit a single policy through a Management Group.</p> <p>RBAC roles can also be applied to a subscription through Management Groups, which represent the recommended practice for ensuring consistent application of tenant-wide security.  Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls.  There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted. </p>"},{"location":"Cloud/Azure/Azure-IAM/#role-definitions","title":"Role definitions","text":"<p>Custom roles configure two types of privileges and are specified by two different properties of the definition JSON file: Management and Data. This provides safety from allowing unrestricted access to data.</p> <p>The values of these properties is an array of strings, each of which follows the format <code>Company.ProviderName/ResourceType/Action</code> where <code>action</code> can be of values <code>read</code>, <code>write</code>, <code>action</code>, <code>delete</code>, or <code>*</code>.</p> Privilege Property that defines allowed permissions Property that defines denied permissions Management <code>Actions</code> <code>NotActions</code> Data <code>DataActions</code> <code>NotDataActions</code> UnrestrictedNetwork resources (read only) <pre><code>\"Actions\": [\n  \"*\"\n]\n</code></pre> <pre><code>\"Actions\": [\n  \"Microsoft.Network/*/read\"\n]\n</code></pre> <p>Example role definitions:</p> Contributor <pre><code>{\n  \"Name\": \"Contributor\",\n  \"Id\": \"b24988ac-6180-42a0-ab88-20f7382dd24c\",\n  \"IsCustom\": false,\n  \"Description\": \"Lets you manage everything except access to resources.\",\n  \"Actions\": [\n    \"*\"\n  ],\n  \"NotActions\": [\n    \"Microsoft.Authorization/*/Delete\",\n    \"Microsoft.Authorization/*/Write\",\n    \"Microsoft.Authorization/elevateAccess/Action\",\n    \"Microsoft.Blueprint/blueprintAssignments/write\",\n    \"Microsoft.Blueprint/blueprintAssignments/delete\"\n  ],\n  \"DataActions\": [],\n  \"NotDataActions\": [],\n  \"AssignableScopes\": [\n    \"/\"\n  ]\n}\n</code></pre> <p>Some built-in roles:</p> <ul> <li>Owner has full access to all resources and can delegate access. Service Administrator and Co-Administrators are assigned this role at the subscription scope.</li> <li>Contributor can create and manage all resources (full read/write privileges), but cannot delegate access.</li> <li>Reader can view resources.</li> <li>[Cost Management Contributor][Cost Management Contributor]</li> <li>[Cost Management Reader][Cost Management Reader]</li> <li>[Resource Policy Contributor][Resource Policy Contributor]</li> <li>[User Administrator][User Administrator]</li> <li>[User Access Administrator][User Access Administrator]</li> </ul>"},{"location":"Cloud/Azure/Azure-IAM/#tasks","title":"Tasks","text":""},{"location":"Cloud/Azure/Azure-IAM/#create-assignment","title":"Create assignment","text":"<p>Assign the Owner role to a user at the subscription scope</p> <ul> <li>Navigate to resource group &gt; Access Control (IAM) &gt; Role Assignments tab &gt; Add &gt; Add Role Assignment</li> <li> <p>Open Subscription &gt; Access Control (IAM) &gt; Add Role Assignment&gt; select a Role &gt; Select target principal</p> </li> <li> <p>Access control (AIM) pane &gt; Add &gt; Add role assignment</p> </li> <li>Select a role in the Role dropdown and a user in the Select field. Then Save</li> </ul> Azure PortalPowerShellAzure CLI <p></p> <pre><code># Resource group scope\nNew-AzRoleAssignment \n    -SignInName \"rbacuser@example.com\" \n    -RoleDefinitionName \"Virtual Machine Contributor\" \n    -ResourceGroupName ExamRefRG\n\n# Subscription scope\nNew-AzRoleAssignment \n    -SignInName \"rbacuser@example.com\" \n    -RoleDefinitionName \"Owner\" \n    -Scope \"/subscriptions/$subId\"\n</code></pre> <pre><code># Resource group scope\naz role assignment create \n    --assignee \"rbacuser@example.com\" \n    --role \"Virtual Machine Contributor\" \n    --resource-group ExamRefRG\n\n# Subscription scope\naz role assignment create \n    --assignee \"rbacuser@example.com\" \n    --role \"Owner\" \n    --subscription $subId\n</code></pre>"},{"location":"Cloud/Azure/Azure-IAM/#delete-assignment","title":"Delete assignment","text":"<p>Navigate to resource group &gt; Access Control (IAM) &gt; Role Assignments tab &gt; Select one or more security principals &gt; Remove</p> <p>Remove RBAC assignments from a user</p> Azure PowerShell <pre><code>Remove-AzRoleAssignment -SignInName \"cloudadmin@opsgility.onmicrosoft.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG\n\nRemove-AzRoleAssignment -SignInName $u -ResourceGroupName $rgName -RoleDefinitionName \"Virtual Machine Contributor\" \n</code></pre> <p>Azure AD group <pre><code>$g = Get-AzADGroup -SearchString \"Cloud Admins\"\nRemove-AzRoleAssignment -ObjectId $g.Id -ResourceGroupName $rg -RoleDefinitionName \"Virtual Machine Contributor\" \n</code></pre></p> <p><pre><code>az role assignment delete --assignee $u --resource-group $rg --role \"Virtual Machine Contributor\" \n</code></pre> Azure AD group <pre><code>g=$(az ad group list --query \"[?displayName=='Cloud Admins'].objectId\" -o tsv)\naz role assignment delete --role \"Virtual Machine Contributor\" -\u2013assignee-object-id $g --resource-group $rg\n</code></pre></p> <p>Read assignment</p> Azure PowerShellAzure CLI <pre><code>Get-AzRoleDefinition -Name \"Virtual Machine Contributor\" | ConvertTo-Json\n</code></pre> <pre><code>az role definition list -n \"Virtual Machine Contributor\"\n</code></pre> <p>List custom roles available for assignment</p> Azure PowerShellAzure CLI <pre><code>Get-AzRoleDefinition | Where-Object { $_.IsCustom -eq $true }\n</code></pre> <pre><code>az role definition list --custom-role-only -o table\n</code></pre> <p>View all role assignments in a subscription <pre><code>az role assignment list --all\n</code></pre></p> <p>Create role definition</p> Azure PowerShell <pre><code>New-AzRoleDefinition -InputFile \"C:\\ARM_templates\\customrole1.json\"\n</code></pre>"},{"location":"Cloud/Azure/Azure-IAM/#configure-cost-center-quotas-and-tagging","title":"Configure cost center quotas and tagging","text":"<p>Grant an AD group RBAC rights</p> Azure PowerShell <pre><code>$group = Get-AzADGroup -SearchString \"Cloud Admins\"\nNew-AzRoleAssignment -ObjectId $group.Id -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG\n</code></pre> <p>Remove RBAC assignments from a group</p> Azure PowerShell <pre><code>$adGroup = Get-AzADGRoup -SearchString \"Cloud Admins\"\nRemove-AzRoleAssignment \n    -ResourceGroupName $rgName\n    -ObjectId $adGroup.Id \n    -RoleDefinitionName \"Virtual Machine Contributor\" \n</code></pre>"},{"location":"Cloud/Azure/Azure-IAM/#elevate-permissions","title":"Elevate permissions","text":"<p>For Azure AD Global Administrators who want to temporarily elevate permissions </p> <ol> <li>Sign into Azure portal as an Azure AD Global Administrator.<sup>?</sup></li> <li>Navigate to Azure Active Directory &gt; Properties. At the bottom of the page, under \"Access management for Azure resources\" click Yes then Save. </li> <li>Sign out and sign in again.</li> <li>Assign roles</li> <li>Revoke elevated access by returning to Azure Active Directory &gt; Properties and selecting No under \"Access management for Azure resources\".</li> </ol> <p>Sources</p> <ul> <li>Elevating global administrator access</li> <li>Understand Azure role definitions</li> </ul>"},{"location":"Cloud/Azure/Azure-IAM/#sspr","title":"SSPR","text":""},{"location":"Cloud/Azure/Azure-IaaS/","title":"Azure IaaS","text":""},{"location":"Cloud/Azure/Azure-IaaS/#high-availability","title":"High availability","text":"<p>These high-availability options are mutually exclusive:</p> <ul> <li>Availability zones protect from datacenter-level failures by providing physical and logical separation between VM instances. Zones have independent power sources, network, and cooling, and there are at least 3 zones in every region.</li> <li>Availability sets offer redundancy for VMs in the same application tier, ensuring at least one VM is available in the case of a host update or problem. Each VM is assigned a fault domain (representing separate physical racks in the datacenter) and an update domain (representing groups of VMs and underlying physical hardware that can be rebooted at the same time for host updates). Availability sets have a maximum of 20 update domains and 3 fault domains. </li> </ul> <p>VM scale sets (VMSS) support the ability to dynamically add and remove instances. By default, a VMSS supports up to 100 instances or up to 1000 instances if deployed with the property <code>singlePlacementGroup</code> set to false (called a large-scale set). A placement group is a concept similar to an availability set in that it assigns fault and upgrade domains. By default, a scale set consists of only a single placement group, but disabling this setting allows the scale set to be composed of multiple placement groups. If a custom image is used instead of one in the gallery, the limit is actually 300 instances.</p> Create availability set<pre><code>az vm availability-set create -n $n -g $g \n    --platform-fault-domain-count 3 \n    --platform-update-domain-count 10\n</code></pre>"},{"location":"Cloud/Azure/Azure-IaaS/#load-balancers","title":"Load balancers","text":"<p>Load balancers redistribute traffic from a frontend pool to a backend pool using rules.  Health probes determine the health of the VMs in the backend pool: if they don't respond then new connections won't be sent.  By default, Azure Load Balancer stores rules in a 5-tuple:</p> <ol> <li>Source IP address</li> <li>Source port</li> <li>Destination IP address</li> <li>Destination ports</li> <li>IP Protocol number</li> </ol> <p>Azure Load Balancers come in 2 pricing tiers:</p> <ol> <li>Basic which is free</li> <li>Standard which is charged based on the number of rules and the data that is processed.</li> </ol> <p>Both boot and guest OS diagnostics can be enabled on or after VM creation.</p> <p>Azure VMs have built-in extensions that enable configuration management. 2 most common extensions for configuration management:</p> <ul> <li>Windows PowerShell Desired State Configuration (DSC) allows you to define the state of a VM using PowerShell Desired State Configuration language </li> </ul> <p>A VM may have more than one Network Interface Card (NIC), but they must belong to the same region as the VM itself. All NICs on a VM must also be attached to the same VNet.</p>"},{"location":"Cloud/Azure/Azure-IaaS/#tasks","title":"Tasks","text":"<p>```sh title=</p>"},{"location":"Cloud/Azure/Azure-IaaS/#deploy-vm-from-image","title":"Deploy VM from image","text":"<p>az vm create -g $rg  -n $vmName -l $vmLocation      --image $imageName</p>"},{"location":"Cloud/Azure/Azure-IaaS/#specify-a-legacy-unmanaged-image","title":"Specify a legacy unmanaged image","text":"<p>az vm create -g $rg  -n $vmName -l $vmLocation      --image $osDiskUri      --generate-ssh-keys</p>"},{"location":"Cloud/Azure/Azure-IaaS/#windows-server-core","title":"Windows Server Core","text":"<p>az vm create -g $rg  -n $vmName -l $vmLocation      --image \"MicrosoftWindowsServer:WindowsServer:2016-Datacenter-Server-Core:2016.127.20190603\"      --admin-username aztestadmin     --admin-password $password     --nics socrates-nic <pre><code>```sh title=\"Display status of VMs\"\naz vm list -g $g -o table\n</code></pre></p> Invoke a command on a VM<pre><code># Enumerate available values for command-id\naz vm run-command list\n\naz vm run-command invoke -g $g -n $n\n    --command-id RunPowerShellScript \n    --scripts @script.ps1 \n    --parameters 'arg1=somefoo' 'arg2=somebar'\n</code></pre> <p>```sh title=\"</p>"},{"location":"Cloud/Azure/Azure-IaaS/#resize-vm","title":"Resize VM","text":"<p>az vm list-vm-resize-options      -g $g -n $n      --output table</p> <p>az vm resize -g $g -n $n      --size Standard_DS3_v2</p>"},{"location":"Cloud/Azure/Azure-IaaS/#create-container-registry","title":"Create container registry","text":"<p>az acr create -g $g -n $n     --sku Basic --admin-enabled true</p>"},{"location":"Cloud/Azure/Azure-IaaS/#add-nic","title":"Add NIC","text":"<p>az network nic create -g $g -n $n      --vnet-name $ExamRefVNET      --subnet $subnetName</p> <p>az vm nic add -g $g --vm-name $vmName --nics $nicName --primary-nic <pre><code>```sh\naz vm redeploy -g $g -n $n\n</code></pre></p> Create managed VM<pre><code>az vm deallocate -g $g --name $vmName\naz vm generalize -g $g --name $vmName\naz image create -g $g --name $imageName --source $vmName \n</code></pre> Create scale set<pre><code>az vmss create -g $g -n $n \n    --image UbuntuLTS \n    --authentication-type password \n    --admin-username $userName \n    --admin-password $password\n</code></pre>"},{"location":"Cloud/Azure/Azure-IaaS/#create","title":"Create","text":"<pre><code>$subnets = @()\n$subnet1Name = \"Subnet1\"\n$subnet2Name = \"Subnet2\"\n$subnet1AddressPrefix = \"10.0.0.0/24\"\n$subnet2AddressPrefix = \"10.0.1.0/24\"\n$vnetAddressSpace = \"10.0.0.0/16\"\n$vnetName = \"ExamRefVNET\"\n\nNew-AzResourceGroup -Name $resourceGroupName -Location $location\n</code></pre> <pre><code># Create a virtual network\n$subnets = @()\n$subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet1Name -AddressPrefix $subnet1AddressPrefix\n$subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet2Name -AddressPrefix $subnet2AddressPrefix\n$vnet = New-AzVirtualNetwork -Name $vnetName -Location $location -AddressPrefix $vnetAddressSpace -Subnet $subnets\n$pip = New-AzPublicIpAddress -Name $ipName -ResourceGroupName $g -Location $location -AllocationMethod Dynamic -DomainNameLabel $dnsName\n$nsgRules = @()\n$nsgRules += New-AzNetworkSecurityRuleConfig -Name \"RDP\" -Description \"RemoteDesktop\" -Protocol Tcp -SourcePortRange \"*\" -DestinationPortRange \"3389\" -SourceAddressPrefix \"*\" -DestinationAddressPrefix \"*\" -Access Allow -Priority 110 -Direction Inbound\n$nsg = New-AzNetworkSecurityGroup -ResourceGroupName $resourceGroupName -Name \"ExamREfNSG\" -SecurityRules $nsgRules -Location $location\n$nic = New-AzNetworkInterface -Name $nicNAme -ResourceGroupName $resourceGroupName -Location $location -SubnetId $vnet.Subnets[0].Id -PublicIpAddressId $pip.Id -NetworkSecurityGroupId $nsg.Id\nAdd-AzVMNetworkInterface -VM $vm -NetworkInterface $nic\n$vm = New-AzVMConfig -VMName $vmName -VMSize $vmSize\nSet-AzVMOperatingSystem -Windows -ComputerName $vmName -Credential $cred -ProvisionVMAgent -VM $vm\nSet-AzVMSourceImage -PublisherName $pubName -Offer $offerName -Skus $skuName -Version \"latest\" -VM $vm\nSet-AzVMOSDisk -CreateOption fromImage -VM $vm\nNew-AzVM -ResourceGroupName $resourceGroupName -Location $location -VM $vm\n</code></pre> <pre><code>az group create --name $g --location $location\nvmName=\"myUbuntuVM\"\nimageName=\"UbuntuLTS\"\naz vm create --resource-group $g --name $vmName --image $imageName --generate-ssh-keys\n</code></pre> <p>Create a virtual network</p> <pre><code>vnetName=\"ExamRefVNET\"\nvnetAddressPrefix=\"10.0.0.0/16\"\naz network vnet create --resource-group $g -n ExamRefVNET --address-prefixes $vnetAddressPrefix -l $location\ndnsRecord=\"examrefdns123123\"\nipName=\"ExamRefIP\"\naz network public-ip create -n $ipName -g $g --allocation-method Dynamic --dns-name $dnsRecord -l $location\nnsgName=\"webnsg\"\naz network nsg create -n $nsgName -g $g -l $location\n</code></pre> <p>Create a NSG rules to allow inbound SSH and HTTP</p> <pre><code>az network nsg rule create -n SSH --nsg-name ... --priority 100 -g ... --access Allow --description \"SSH Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 22 --source-address-prefix \"*\" --source-port-range \"*\"\naz network nsg rule create -n HTTP --nsg-name ... --priority 101 -g ... --access Allow --description \"Web Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 80 --source-address-prefix \"*\" --source-port-range \"* \n</code></pre> <p><pre><code># Create a network interface\nnicname=\"WebVMNic1\"\naz network nic create -n $nicname -g $g --subnet $Subnet1Name --network-security-group $nsgName --vnet-name $vnetName --public-ip-address $ipName -l $location\n</code></pre> <pre><code># Retrieve a list of marketplace images\naz vm image list --all\n</code></pre> <pre><code># Retrieve form factors available in each region\naz vm list-sizes --location ...\n</code></pre> <pre><code># Create the VM\nimageName=\"Canonical:UbuntuServer:16.04-LTS:latest\"\nvmSize=\"Standard_DS1_V2\"\nuser=demouser\nvmName=\"WebVM\"\naz vm create -n $vmName -g $g -l $location --size $vmSize --nics $nicname --image $imageName --generate-ssh-keys\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-IaaS/#vhd","title":"VHD","text":"<p>Add a new disk to a VM</p> PowerShellAzure CLI <pre><code>$dataDiskName = \"MyDataDisk\"\n$location=\"WestUS\"\n$diskConfig = New-AzDiskConfig -SkuName Premium_LRS -Location $location -CreateOption Empty -DiskSizeGB 128\n$dataDisk1 = New-AzDisk -DiskName $dataDiskName -Disk $diskConfig -ResourceGroupNAme ExamRefRG\n$vm = Get-AzVM -Name ExamRefVM -ResourceGroupName ExamRefRG\n$vm = Add-AzVMDataDisk -VM $vm -Name $dataDiskName -CreateOption Attach -ManagedDiskId $dataDisk1.Id -Lun 1\n\nUpdate-AzVM -VM $vm -ResourceGroupName ExamRefRG\n</code></pre> <pre><code>az vm disk attach -g ExamRefRG --vm-name ExamRefVM --name myDataDisk --new --size-gb 128 --sku Premium_LRS\n</code></pre> <p>Modify host cache setting</p> PowerShellAzure CLI <pre><code>$vm = Get-AzVM -ResourceGroupName $g -Name $vmName\nSet-AzVMDataDisk -VM $vm -Lun 0 -Caching ReadOnly\nUpdate-AzVM -ResourceGroupName $g -VM $vm\n</code></pre> <pre><code>az vm disk attach --vm-name $vmName --resource-group $g --size-gb 128 --disk $diskName --caching ReadWrite -new\naz vm unmanaged-disk attach\n</code></pre>"},{"location":"Cloud/Azure/Azure-IaaS/#diagnostics","title":"Diagnostics","text":""},{"location":"Cloud/Azure/Azure-IaaS/#enable-on-deployment","title":"Enable on deployment","text":""},{"location":"Cloud/Azure/Azure-IaaS/#enable-after-deployment","title":"Enable after deployment","text":"<p> <code>Set-AzVmDiagnosticsExtension</code></p> <p>Enable diagnostics using a storage account specified in a XML configuration file <pre><code>Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\"\n</code></pre> Providing storage account name absent in config, or overriding it if present <pre><code>Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup1\" -VMName \"VirtualMachine2\" -DiagnosticsConfigurationPath diagnostics_publicconfig.xml -StorageAccountName \"MyStorageAccount\"\n</code></pre> Explicitly providing storage account name and key <pre><code>Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" -StorageAccountName \"MyStorageAccount\" -StorageAccountKey $storage_key\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-IaaS/#arm-templates","title":"ARM templates","text":"<p>Deploy a named ARM template</p> PowerShellAzure CLI <pre><code>New-AzResourceGroupDeployment -Mode Complete -Name simpleVMDeployment -ResourceGroupName ExamRefRG\n</code></pre> <pre><code>az group deployment create --name simpleVMDeployment --mode Complete --resource-group ExamRefRG\n</code></pre> <p>Export a resource group to an ARM template</p> PowerShellAzure CLI <pre><code>Save-AzResourceGroupDeploymentTemplate -ResourceGroupName ExamRefRG -DeploymentName simpleVMDeployment\n</code></pre> <pre><code>az group deployment export --name simpleVMDeployment --resource-group ExamRefRG\n</code></pre> <p>Create from existing resource group</p> <pre><code>Export-AzResourceGroup -ResourceGroupName ExamRefRG\n</code></pre> <p>Pass a template file during deployment</p> <pre><code>New-AzResourceGroupDeployment -Name MyDeployment -ResourceGroupName ExamRefRG -TemplateFile C:\\MyTemplates\\AppTemplate.json\n</code></pre> <pre><code>az group export --name ExamRefRG\n</code></pre> <pre><code>az group deployment create --name MyDeployment --resource-group ExamRefRG --template-file AppTemplate.json --parameters @dev-env.json\n</code></pre>"},{"location":"Cloud/Azure/Azure-IaaS/#view-all-available-sizes-in-a-location","title":"View all available sizes in a location","text":"<p>Move a resource to another resource group or subscription (PowerShell) <pre><code>$resourceID = Get-AzResource -ResourceGroupName ExamRefRG | Format-Table -Property ResourceId\n\nMove-AzResource -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID\nMove-AzResource -DestinationSubscriptionId $subscriptionID -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID\n</code></pre></p> <p>Move a resource to another resource group or subscription (Azure CLI) <pre><code>az resource list -g ExamRefRG\naz resource move --destination-group ExamRefDestRG --ids $resourceID\naz resource move --destination-group ExamrefDestRG --destination-subscription-id $subscriptionID --ids $resourceID\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-IaaS/#dsc","title":"DSC","text":""},{"location":"Cloud/Azure/Azure-IaaS/#package","title":"Package","text":"<p>Package a DSC script into a zip file <pre><code>Publish-AzVMDscConfiguration -ConfigurationPath .\\ContosoWeb.ps1 -OutputArchivePath .\\ContosoWeb.zip\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-IaaS/#apply","title":"Apply","text":"<p>Publish a packaged DSC script to a storage account <pre><code>$g = \"ExamRefRG\"\n$location =- \"WestUS\"\n$vmName = \"ExamRefVM\"\n$storageName = \"dscstorageer1\"\n$configurationName = \"Main\"\n$archiveBlob = \"ContosoWeb.ps1.zip\"\n$configurationPath = \".\\ContosoWeb.ps1\"\n\nPublish-AzVMDscConfiguration -ConfigurationPath $configurationPath -ResourceGroupName $g -StorageAccountName $storageName\nSet-AzVmDscExtension -Version 2.76 -ResourceGroupName $g -VMName $vmName -ArchiveStorageAccountNAme $storageName -ArchiveBlobName $archiveBNlob -AutoUpdate:$false -ConfigurationName $configurationName\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-IaaS/#dedicated-host","title":"Dedicated host","text":""},{"location":"Cloud/Azure/Azure-Key-Vault/","title":"Azure Key Vault","text":""},{"location":"Cloud/Azure/Azure-Key-Vault/#azure-key-vault","title":"Azure Key Vault","text":"<ul> <li>Data protection</li> <li>Encryption models</li> <li>Application security</li> </ul> <p>Azure Key Vault is a managed secrets solution offered by Azure. Key Vault can store three types of secrets: keys, secrets, and certificates.</p> <p>Key Vault integrates with other Azure services</p> <ul> <li>Storage accounts</li> <li>Azure disk encryption (i.e. Bitlocker keys for encrypted Windows disks)</li> <li>Azure App Service (i.e. certificates or API keys)</li> <li>SQL Always Encrypted which ensures that data is stored encrypted even on the client</li> </ul> <p>Azure Key Vault likes x509 certificates in PFX format.</p> <p>Certificates are merged with existing CSRs .    </p>"},{"location":"Cloud/Azure/Azure-Monitoring/","title":"Azure Monitoring","text":"<p>A robust monitoring strategy implementing proactive notifications helps to increase uptime and optimize performance. Azure offers Azure Monitor and Azure Advisor.</p> <ul> <li>Azure Monitor brings a unified alerting experience to Azure, with a single pane of glass for interacting with metrics, the Activity Log, Log Analytics, service and resource health and service-specific insights. <ul> <li>Alerts can be filtered by subscription (maximum of 5), resource group (maximum of 1), resource type (available selections depend on resources deployed to selected group) time range (past hour, past day, past week, and past 30 days), and other criteria.</li> <li>Azure Monitor can create alert rules that are built on target resources or resource type and that proactively notify you of the health of resources and can also leverage action groups that automate actions to take in certain conditions.</li> </ul> </li> <li>Azure Advisor is a free, personalized guide to Azure best practices that provides recommendations to help you optimize resources.  Azure Advisor offers personalized recommendations across 4 domains: High availability, Security, Performance, and Cost</li> </ul> Feature Logs Metrics Retention Stored in Log Analytics (2 years) Stored in Monitor for 93 days, but metrics can be sent to Log Analytics and Storage accounts as well Properties Varying properties for each log, with support for rich data types such as date and time Fixed set of properties (or attributes): time, type, resource, value, and (optionally) dimensions. Data availability Triggered by an event, requiring time to process before they are available for querying Gathered at intervals and available for immediate querying. <p>Virtual machines can be one of the most expensive resources in a cloud implementation, and there are several ways to reduce their cost</p> <ul> <li>Deallocate compute when not needed</li> <li>Delete unused virtual machines and allocate them only on demand</li> <li>Right-size VMs so that you don't overuse resources</li> </ul> <p>Advisor can also identify</p> <ul> <li>ExpressRoute circuits that have been \"Not Provisioned\" for more than 30 days</li> <li>Gateways that have been idle for more than 90 days</li> </ul> <p>There are two monitoring data streams:</p> <ul> <li>Metrics are the numerical time series data produced by resources and services within Azure. They are collected at 1-minute intervals, identified by a metric name and namespace (category). Metrics can be one-dimensional or have up to 10 dimensions.<ul> <li>Metrics have the following properties:<ul> <li>Time the value was collected</li> <li>Type of measurement made</li> <li>Resource associated with value</li> <li>Value</li> </ul> </li> <li>Metrics can be stored in:<ul> <li>Azure Monitor for 93 days</li> <li>Log Analytics for 2 years</li> <li>Storage account, where they are treated according to the retention policy and storage limits of the account.</li> </ul> </li> </ul> </li> <li>Logs come in various types</li> <li>Diagnostics logs (including resource logs and tenant logs) are a type of log data that can be configured to send data to other locations, such as a Storage account or Log Analytics workspace. Diagnostics logs have to be enabled for each resource to be monitored through the Portal by enabling Diagnostics Settings, and not all resource types support diagnostic logs. Of those that do, not all resources support a retention policy or sending metric data.</li> <li>Azure Activity logs is a subscription level log that captures events that range from operational data (i.e. resource creation, deletion) to service health events for a subscription, but lacks resource-level detail..</li> <li>Guest telemetry can relay logs from VMs with the use of diagnostics agents</li> </ul>"},{"location":"Cloud/Azure/Azure-Monitoring/#log-analytics","title":"Log Analytics","text":"<p>A Log Analytics workspace is a form of abstracting the process of log collection and is used to collect and aggregate logs. Like any other resource, it must be associated with a location and a resource group. Any Azure resource can only report logs to a single workspace, but Azure Monitor allows multiple workspaces to be queried simultaneously. The logs can be queried through Log Analytics or Monitor. Because a workspace is a resource, RBAC can be applied to control access to it. </p> <p>Log Analytics is based on Azure Data Explorer and uses Kusto.</p> <p>Log Analytics pricing is divided into data ingestion and data retention: - Under Pay-As-You-Go data ingestion the first 5 GB per month are free and further data is charged at a rate of $2.76/GB/month - Capacity Reservations offer a discount on Pay-as-you-go by charging a fixed amount per day ($219.52/day for 100 GB), with further discounts at higher tiers - Data retention is free for any amount of data up to 31 days, and 90 days for Azure Sentinel-enabled workspaces</p>"},{"location":"Cloud/Azure/Azure-Monitoring/#operational-insights","title":"Operational Insights","text":"<p>Log Analytics was previously named Operational Insights, which was named System Center Advisor prior to 2014.</p>"},{"location":"Cloud/Azure/Azure-Monitoring/#application-insights","title":"Application Insights","text":"<p>Application Insights is a platfrom separate from Log Analytics which is intended to monitor web applications.</p>"},{"location":"Cloud/Azure/Azure-Monitoring/#alerts","title":"Alerts","text":"<p>Alerts can be created from the Alerts pane in the Monitor blade: </p> <p>Most resource blades also have Alerts in the resource menu under Monitoring.</p> <p>Alert rules, which are used to generate alerts, contain - Target resource, any Azure resource that generates signals, defines the scope and signals available for the alert. - Signal (i.e. metric or Activity Log) emitted by target resource. Signals are of 3 types:    1. Metrics   2. Log search queries    3. Activity logs - Conditional logic for alert combines the signal and a logical test to trigger alert. - Action Group determines what will happen when the alert is trigged. Action groups are themselves resources, and thus located in a subscription and resource group, and have:   - Name   - Short name is used to identify the Action Group in emails and notifications and is limited to 12 characters   - Actions define the configuration for a specific action type.  - Severity (0-4)</p> <p>Alerts can have 3 states:</p> <ul> <li>New and not reviewed</li> <li>Acknowledged issue is being actioned by an admin</li> <li>Closed issue that generated the alerts has been resolved and the alert has been marked as closedAlerts have many notification options, including email, SMS, mobile app, voice, and integration with automation. </li> </ul>"},{"location":"Cloud/Azure/Azure-Monitoring/#actions","title":"Actions","text":"<p>A single action group can trigger multiple actions. Available types include: Email/SMS/Push/Voice, Azure Function, Logic App, Webhook, ITSM, and Automation Runbook</p> <ul> <li>IT Service Management Connector up to 10 ITSM actions can be configured with an ITSM connection</li> <li>Supported providers include ServiceNow, System Center Service Manager, Provance, and Cherwell  Connect Azure to ITSM tools using ITSMC</li> <li>Webhooks</li> <li>Runbook runs in Azure Automation Service Runbooks</li> </ul> <p>The maximum number of alert notifications per hour: - Email: 60 - Voice: 12 - SMS: 12</p>"},{"location":"Cloud/Azure/Azure-Monitoring/#vms","title":"VMs","text":"<p>\"Virtual Machine Insights\" (or \"Azure Monitor for VMs\") is the successor to older monitoring workflow that used \"guest OS diagnostics\" in conjunction with Metrics Explorer. It requires a log analytics workspace. Diagnostic settings, conventionally, was the feature that would be enabled to allow Azure to collect metrics and logs from VMs, including event logs and performance counters.</p> <p>Two primary views: - Performance is a successor to the old Metrics Explorer - Map (originally \"Service Map\")</p>"},{"location":"Cloud/Azure/Azure-Monitoring/#tasks","title":"Tasks","text":""},{"location":"Cloud/Azure/Azure-Monitoring/#enable-diagnostics-on-a-vm","title":"Enable diagnostics on a VM","text":"<p>Sources: - <code>az vm diagnostics set</code></p> <pre><code>\n</code></pre>"},{"location":"Cloud/Azure/Azure-Monitoring/#diagnostics-log-collection-with-a-storage-account","title":"Diagnostics log collection with a storage account","text":"<p>Browse to the resource itself. Alternatively, open Azure Monitor and then the Diagnostics Settings blade. From there you can view all eligible resouce types and view status of log collection.  <pre><code>$resource = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName\n$storage = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName\nSet-AzDiagnosticSetting -ResourceId $resource.ResourceId -StorageAccountId $storage.ResourceId -Enabled $true\n</code></pre> <pre><code>resourceId=$(az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv)\naz monitor diagnostic-settings create --name $diagnosticName --storage-account $storageId --resource $resouceId --resource-group $resourceGroup \\\n  --logs '[ {\n    \"category\": &lt;categoryName&gt;,\n    \"enabled\": true,\n    \"retentionPolicy\": {\n      \"days\": &lt;numberOfDays&gt;,\n      \"enabled\": true } } ] '\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Monitoring/#diagnostics-log-streaming-to-an-event-hub","title":"Diagnostics log streaming to an Event Hub","text":"<p><pre><code>$rule = Get-AzServiceBusRule -ResourceGroup $resourceGroupName -Namespace $namespace -Topic $topic -Subscription $subscription -Name $ruleName\nSet-AzDiagnosticSetting -ResourceId $resource.ResourceId -ServiceBusRuleId $rule.Id -Enabled $true\n</code></pre> <pre><code>resourceId=$(az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv)\naz monitor diagnostic-settings create --name $diagnosticName --event-hub $eventHubName --event-hub-rule $eventHubRuleId --resource $resourceId \\\n  --logs '[{\n    \"category\": &lt;categoryName&gt;,\n    \"enabled\": true }]'\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Monitoring/#diagnostics-log-collection-in-a-log-analytics-workspace","title":"Diagnostics log collection in a Log Analytics workspace","text":"<p>The PowerShell module that allows interaction with Log Analytics still refers to the service's old name.</p> <p><pre><code>$workspace = Get-AzOperationalInsightsWorkspace -Name $logAnalyticsName -ResourceGroupName $g\nSet-AzDiagnosticSetting -ResourceId $r.ResourceId -WorkspaceId $workspace.ResourceId -Enabled $true\n</code></pre> <pre><code>az monitor diagnostic-settings create --name $diagnosticName --workspace $logAnalyticsName --resource $rid --resouce-group $g \\\n  --logs '[{\n    \"category\": &lt;categoryName&gt;,\n    \"enabled\": true }]'\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Monitoring/#create-an-alert-rule","title":"Create an alert rule","text":"<p>Sources: - Create, view, and manage activity log alerts by using Azure Monitor</p> <p></p> <p></p>"},{"location":"Cloud/Azure/Azure-Monitoring/#create-log-analytics-workspace","title":"Create Log Analytics workspace","text":""},{"location":"Cloud/Azure/Azure-Monitoring/#sources","title":"Sources","text":"<ul> <li>AZ-103: <code>1.2</code></li> <li> <p>AZ-104: <code>5.1</code></p> </li> <li> <p>Azure Monitor for VMs</p> </li> </ul>"},{"location":"Cloud/Azure/Azure-Policy/","title":"Azure Policy","text":"<p>Azure Policy is a service that can create, assign, and manage policies to enforce governance.  RBAC roles deny by default and allow explicitly. But Azure Policy allows by default and denies explicitly.</p> <p>To implement policy, a policy definition is created first, then a policy assignment assigns it to a scope. </p> <pre><code>az policy definition create \n    --name 'allowedVMs' --description 'Only allow virtual machines in the defined SKUs' --mode ALL --rules '{...}' --params '{...}'\n\naz policy assignment create \n    --policy 'allowedVMs' --name 'deny-non-compliant-vms' --scope '/subscriptions/&lt;Subscription ID&gt;' -p # (1)\n\naz policy assignment delete --name 'deny-non-compliant-vms'\n</code></pre> <ol> <li>A scope can be a management group, subscription, or resource group, with all child resources and resource groups being affected.</li> </ol> <p>Policy definitions can be packaged together using initiative definitions and applied to a scope using initiative assignments.</p> <p>Every policy definition has a single effect, which includes:</p> <ul> <li>Audit: create a warning event in the log</li> <li>Modify: used to add, update, or remove properties or tags on a resource during creation or update.</li> <li>Append</li> <li>AuditIfNotExists</li> <li>Deny</li> <li>DeployIfNotExists</li> <li>Disabled</li> </ul> <p>The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\")</p>"},{"location":"Cloud/Azure/Azure-Storage/","title":"Azure Storage","text":""},{"location":"Cloud/Azure/Azure-Storage/#dns","title":"DNS","text":""},{"location":"Cloud/Azure/Azure-Storage/#storage-account-access","title":"Storage account access","text":""},{"location":"Cloud/Azure/Azure-Storage/#sas-token","title":"SAS token","text":"<p>SAS tokens are generated from a storage account key; if the key is invalidated then so are all SAS tokens generated from it. The user delegation SAS token itself is meant to be appended to the end of the blob's URI.<sup>CloudSkills: 40:00</sup></p>"},{"location":"Cloud/Azure/Azure-Storage/#tasks","title":"Tasks","text":""},{"location":"Cloud/Azure/Azure-Storage/#add-endpoints-to-azure-file-sync-group","title":"Add endpoints to Azure File Sync Group","text":"<ol> <li>Register a server to the sync group by installing Azure File Sync agent on each server. When installing, you sign in with your subscription's credentials, then register the server by providing the Subscription, Resource Group, and Storage Sync Service names.</li> <li>Click Add Server Endpoint. This will display a dropdown of all servers with the agent installed and associated with the sync service.</li> </ol> Upload blob<pre><code>az storage blob upload --container-name $containerName --account-name $accountName --account-key $accountKey --file $file --name $blobName\n\nAzCopy copy localFilePath https://storageAccount.blob.core.windows.net/destinationContainer/path/to/blob?SASToken\n</code></pre> <pre><code>:: Download a blob from a container\nAzCopy copy https://storageAccount.blob.core.windows.net/sourceContainer/path/to/blob?SASToken localFilePath\n\n:: Copy a blob from one container to another\nAzCopy /Source:https://sourceblob.blob.core.windows.net/sourcecontainer/ \n    /Dest:https://deststorage.blob.core.windows.net/destcontainer/ \n    /SourceKey:sourcekey /DestKey:destkey /Pattern:disk1.vhd\n</code></pre> <p><pre><code>$blobCopyState = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $vhdName -DestContext $destContext\n$srcStorageKey = Get-AzStorageAccountKey -ResourceGroupName $sourceg -Name $srcStorageAccount\n$destStorageKey = Get-AzStorageAccountKey -ResourceGroupName $destg -Name $destStorageAccount\n$srcContext = New-AzStorageContext -StorageAccountName $srcStorageAccount -StorageAccountKey $srcStorageKey.Value[0]\n$destContext = New-AzStorageContext -StorageAccountNAme $destStorageAccount -StorageAccountKey $destStorageKey.Value[0]\n\n# Create new container in destination account\nNew-AzStorageContainer -Name $destContainer -Context $destContext\n\n# Make the copy\n$copiedBlob = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $blobName -DestContext $destContext\n</code></pre> <pre><code>az storage blob copy start --account-name $destStorageAccount --account-key $destStorageKey --destination-blob $blobName --source-account-name $srcStorageAccount --source-container $srcContainer --source-blob $blobName --source-account-key $srcStorageKey\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Storage/#monitor-progress-of-the-async-blob-copy","title":"Monitor progress of the async blob copy","text":"<p><pre><code>$copiedBlob | Get-AzStorageBlobCopyState\n</code></pre> <pre><code>az storage blob show --account-name $destStorageAccount --account-key $destStorageKey --container-name $destContainer --name $blobName\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Storage/#create-sas-token","title":"Create SAS token","text":"<p><pre><code>$storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $accountName\n$context = New-AzStorageContext -StorageAccountName $accountName -StorageAccountKey $storageKey[0].Value\n$startTime = Get-Date\n$endTime = $startTime.AddHours(4)\n\nNew-AzStorageBlobSASToken -Container $container -Blob $blob -Permission \"rwd\" -StartTime $startTime -ExpiryTime $startTime.AddHours(4) -Context $context\n</code></pre> <pre><code>az storage blob generate-sas --account-name \"storageAccount\" --account-key $storageAccountKey --container-name $container --name $blobName --permissions r --expiry \"2019-05-31\"\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Storage/#create-storage-container","title":"Create storage container","text":"<pre><code>$storageKey = Get-AzStorageAccountKey -Name $storageAccount -ResourceGroupName $resourceGroup\n$context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey.Value[0]\nSet-AzCurrentStorageAccount -Context $context\n\nNew-AzStorageContainer -Name $container -Permission Off\n</code></pre> <p>Upload file as blob to new container</p> Azure PowerShellAzure CLI <pre><code>Set-AzStorageBlobContent -File $localFile -Container $container -Blob $blobName\n</code></pre> <pre><code>az storage container create \\\n    --account-name $STORAGE_ACCOUNT --name $CONTAINER_NAME --public-access off\n</code></pre>"},{"location":"Cloud/Azure/Azure-Storage/#ensure-app-services-backup-vault-and-event-hub-have-access-to-a-storage-account","title":"Ensure App Services, backup vault, and event hub have access to a storage account","text":"<pre><code>Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 |\n    Set-AzVirtualNetworkSubnetConfig -Name VSUBNET01 -AddressPrefix 10.0.0.0/24 -ServiceEndpoint Microsoft.Storage |\n    Set-AzVirtualNetwork\n\n$subnet = Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 |\nGet-AzVirtualNetworkSubnetConfig -Name VSUBNET01\nAdd-AzStorageAccountNetworkRule -ResourceGroupName VNET01 -Name Storage01 -VirtualNetworkResourceId $subnet.Id\nUpdate-AzStorageAccountNetworkRuleSet -ResourceGroupName RG01 -Name STORAGE01 -Bypass Azure.Services\n</code></pre>"},{"location":"Cloud/Azure/Azure-Storage/#troubleshoot-azure-file-sync","title":"Troubleshoot Azure File Sync","text":"<p>Several procedures to be used when Azure File Sync is having issues</p> <p>Collect logs to troubleshoot issues with Azure File Sync agent installation <pre><code>StorageSyncAgent.msi /l*v AFSInstaller.log\n</code></pre> Remove the server from registered sync group Error message \"This server is already registered during registration\" <pre><code>Import-Module \"C:\\Program Files\\Azure\\StorageSyncAgent\\StorageSync.Management.ServerCmdlets.dll\"\nReset-StorageSyncServer\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-Storage/#monitoring-using-log-analytics","title":"Monitoring using Log Analytics","text":"<p>Access Activity Log data (Portal) 1. Find Management + Governance in All Services 2. Open Activity Log 3. Click Logs icon at top of Activity Log view to select an existing Log Analytics (OMS) workspace or create a new one</p>"},{"location":"Cloud/Azure/Azure-Storage/#storage-account-endpoints","title":"Storage account endpoints","text":""},{"location":"Cloud/Azure/Azure-Storage/#virtual-network-service-endpoint","title":"Virtual network service endpoint","text":"<p>Sources - AZ-103 p. 112 - Configure Azure Storage firewalls and virtual networks</p> <p></p> <ol> <li>Specify <code>Microsoft.Storage</code> in the service endpoint settings of the VNet subnet</li> <li>Configure which VNets can access a particular storage account</li> </ol> <p>Display virtual network rules</p> Azure CLI Azure PowerShell <pre><code>az storage account network-rule list -g $rgName -n $n --query virtualNetworkRules\n</code></pre> <pre><code>Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object VirtualNetworkRules\n</code></pre> <p>Enable service endpoint for Azure Storage on an existing virtual network and subnet.</p>  Azure PowerShellAzure CLI <pre><code>Get-AzVirtualNetwork -ResourceGroupName $rgName -Name $n | Set-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" -AddressPrefix \"10.0.0.0/24\" -ServiceEndpoint \"Microsoft.Storage\" |   Set-AzVirtualNetwork\n</code></pre> <pre><code>az network vnet subnet update -g $rgName --vnet-name $n --name \"mysubnet\" --service-endpoints \"Microsoft.Storage\"\n</code></pre> <p>Add network rule for VNet and subnet</p>  Azure PowerShellAzure CLI <pre><code>$subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\"\n\nAdd-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet.Id\n</code></pre> <pre><code>subnetid=$(az network vnet subnet show -g $ng --vnet-name $nn -n \"mysubnet\" --query id --output tsv)\naz storage account network-rule add -g $sg -n $sn --subnet $subnetid\n</code></pre> <p>Remove network rule</p> <pre><code>```powershell\n$subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | \n    Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\"\n\nRemove-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet.Id\n```\n</code></pre> <p>Bypass network rules to allow access for Azure services like Event Hub and Recovery Services Vault</p> Azure PowerShellAzure CLI <pre><code># Display exceptions for the storage account network rules\nGet-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n | Select-Object Bypass\n# Configure exceptions to storage account network rules\nUpdate-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -Bypass AzureServices,Metrics,Logging\n</code></pre> <pre><code># Display exceptions for the storage account network rules\naz storage account show -g $g -n $n --query networkRuleSet.bypass\n# Configure exceptions to storage account network rules\naz storage account update -g $g -n $n --bypass Logging Metrics AzureServices\n</code></pre> <ul> <li>Configure Azure Storage firewalls and virtual networks</li> <li>AZ-103: p. 107, 114, 127</li> </ul>"},{"location":"Cloud/Azure/Azure-VPN/","title":"Azure VPN","text":""},{"location":"Cloud/Azure/Azure-VPN/#authentication","title":"Authentication","text":"<p>Azure P2S VPN connections support several authentication methods:</p> <ul> <li>Azure AD authentication (Windows 10 only)</li> <li>RADIUS server</li> <li>VPN Gateway native certificate authentication</li> </ul> <p>The VPN gateway acts as a pass-through forwarding authentication messages between the connecting device and the RADIUS server. The RADIUS server can be deployed on-premises or in the Azure VNet, and two such servers can be deployed for high availability.</p> <ul> <li>If deployed on-premises, a S2S VPN to the site is required, and ExpressRoute is not usable.</li> <li>AD domain authentication requires a RADIUS server that integrates with the AD server.</li> </ul>"},{"location":"Cloud/Azure/Azure-VPN/#tasks","title":"Tasks","text":""},{"location":"Cloud/Azure/Azure-VPN/#create-local-network-gateway","title":"Create local network gateway","text":"<pre><code>$localnw = New-AzLocalNetworkGateway -Name LocalNetGW -ResourceGroupName ExamRefRG -Location \"West Europe\" -GatewayIpAddress \"53.50.123.195\" -AddressPrefix \"10.5.0.0/16\" \n</code></pre> Create VPN connection<pre><code>$gateway = Get-AzVirtualNetworkGateway -Name VPNGW1 -ResourceGroupName ExamRefRG\n$conn = New-AzVirtualNetworkGatewayConnection \n    -Name OnPremConnection -ResourceGroupName ExamRefRG -Location 'West Europe' \n    -VirtualNetworkGateway1 $gateway \n    -LocalNetworkGateway2 $localnw \n    -ConnectionType IPsec \n    -SharedKey \"abc123\"\n</code></pre>"},{"location":"Cloud/Azure/Azure-VPN/#create-a-vpn-gateway","title":"Create a VPN Gateway","text":"<p><pre><code>$rg = ExamRefRG\n</code></pre> Create gateway subnet in VNet1 Gateway subnets are normal subnets with the name \"GatewaySubnet\" <pre><code>$vnet1 = Get-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rg\n$vnet1.Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10.1.1.0/27\n$vnet1 = Set-AzVirtualNetwork -VirtualNetwork $vnet1\n</code></pre> Create VPN gateway in VNet1 <pre><code>$gwpip = New-AzPublicIpAddress -Name VNet1-GW-IP -ResourceGroupName $rg -Location 'North Europe' -AllocationMethod Dynamic\n$gwsubnet = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet1\n$gwipconf = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf -Subnet $gwsubnet -PublicIpAddress $gwpip\n$vnet1gw = New-AzVirtualNetworkGateway -Name VNet1-GW -ResourceGroupName $rg -Location 'North Europe' -IpConfigurations $gwipconf -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1\n</code></pre></p> <p>Create gateway subnets in VNet2 and VNet3 <pre><code>az network vnet subnet create --name GatewaySubnet --vnet-name VNet1 --resource-group ExamRefRG --address-prefixes 10.1.1.0/27\naz network public-ip create --name VNet1-GW-IP --resource-group ExamRefRG --location NorthEurope\naz network vnet-gateway create --name VNet1-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet1 --public-ip-addresses VNet1-GW-IP --location NorthEurope\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-VPN/#create-a-vpn-gateway-and-vnet-peering","title":"Create a VPN gateway and VNet peering","text":"<p>Create gateway subnets in VNet2 and VNet3 <pre><code>$vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG\n$vnet2.Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10.2.1.0/27\n$vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2\n$vnet3 = Get-AzVirtualNetwork -Name VNet3 -ResourceGroupName ExamRefRG\n$vnet3.Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10.3.1.0/27\n$vnet3 = Set-AzVirtualNetwork -VirtualNetwork $vnet3\n</code></pre> Create VPN gateway in VNet2 <pre><code>$gwpip2 = New-AzPublicIpAddress -Name VNet2-GW-IP -ResourceGroupName ExamRefRG -Location $vnet2.Location -AllocationMethod Dynamic\n$gwsubnet2 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet2\n$gwipconf2 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf2 -Subnet $gwsubnet2 -PublicIpAddress $gwpip2\n$vnet2gw = New-AzVirtualNetworkGateway -Name VNet2-GW -ResourceGroupNAme ExamRefR -Location $vnet2.Location -IpConfigurations $gwipconf2 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1\n</code></pre> Create VPN gateway in VNet3 <pre><code>$gwpip3 = New-AzPublicIpAddress -Name VNet3-GW-IP -ResourceGroupName ExamRefR -Location $vnet3.Location -AllocationMethod Dynamic\n$gwsubnet3 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet3\n$gwipconf3 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf3 -Subnet $gwsubnet3 -PublicIpAddress $gwpip3\n$vnet3gw = New-AzVirtualNetworkGateway -Name VNet3-GW -ResourceGroupNAme ExamRefRG -Location $vnet3.Location -IpConfigurations $gwipconf3 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1\n</code></pre> Create connections <pre><code>New-AzVirtualNetworkGatewayConnection -Name VNet2-to-VNet3 -ResourceGroupName ExamRefRG -Location $vnet2.Location -VirtualNetworkGateway1 $vnet2gw -VirtualNetworkGateway2 $vnet3gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\"\nNew-AzVirtualNetworkGatewayConnection -Name VNet3-to-VNet2 -ResourceGroupName ExamRefRG -Location $vnet3.Location -VirtualNetworkGateway1 $vnet3gw -VirtualNetworkGateway2 $vnet2gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\"\n</code></pre></p> <p>Create gateway subnets in VNet2 and VNet3 <pre><code>az network vnet subnet create --name GatewaySubnet --vnet-name VNet2 --resource-group ExamRefRG --address-prefixes 10.2.1.0/27\naz network vnet subnet create --name GatewaySubnet --vnet-name VNet3 --resource-group ExamRefRG --address-prefixes 10.3.1.0/27\n</code></pre> Create public IP addresses for use by VPN gateways <pre><code>az network public-ip create --name VNet2-GW-IP --resource-group ExamRefRG --location NorthEurope\naz network public-ip create --name VNet3-GW-IP --resource-group ExamRefRG --location WestEurope\n</code></pre> Create VPN gateways in VNet2 and VNet 3 <pre><code>az network vnet-gateway create \n    --name VNet2-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet2 --public-ip-addresses VNet2-GW-IP --location NorthEurope\naz network vnet-gateway create \n    --name VNet3-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet3 --public-ip-addresses VNet3-GW-IP --location WestEurope\n</code></pre> Create connections between VPN gateways <pre><code>az network vpn-connection create --name VNet2-to-VNet3 --resource-group ExamRefRG --vnet-gateway1 VNet2-GW --vnet-gateway2 VNet3-GW --shared-key secretkey123 --location NorthEurope\naz network vpn-connection create --name VNet3-to-VNet2 --resource-group ExamRefRG --vnet-gateway1 VNet3-GW --vnet-gateway2 VNet2-GW --shared-key secretkey123 --location WestEurope\n</code></pre></p>"},{"location":"Cloud/Azure/Azure-VPN/#use-vpn-troubleshoot","title":"Use VPN Troubleshoot","text":"<p>Get the Network Watcher resource <pre><code>$nw = Get-AzResource | Where ResourceType -eq Microsoft.Network/networkWatchers -and Location -eq WestEurope\n$networkWatcher = Get-AzNetworkWatcher -Name $nw.Name -ResourceGroupName $nw.ResourceGroupName\n</code></pre></p> <p>Get the connection to troubleshoot <pre><code>$connection = Get-AzVirtualNetworkGatewayConnection -Name Vnet1-to-Vnet2 -ResourceGroupName ExamRefRG\n</code></pre></p> <p>Start VPN Troubleshoot</p> <pre><code>Start-AzNetworkWatcherResourceTroubleshooting -NetworkWatcher $networkWatcher -TargetResourceId $connection.Id -StorageId $sa.Id -StoragePath \"$($sa.PrimaryEndpoints.Blob)$($sc.name)\"\n</code></pre> <p>Create a storage account and container for logs</p> <pre><code>az storage account create \n    --name examrefstorage --location westeurope --resource-group ExamRefRG \n    --sku Standard_LRS\n\naz storage account keys list \n    --resource-group ExamRefRG --account-name examrefstorage\n\naz storage container create \n    --account-name examrefstorage --account-key {storageAccountKey} --name logs\n</code></pre> Start VPN Troubleshoot<pre><code>az network watcher troubleshooting start \n    --resource-group ExamRefRG --resource Vnet1-to-Vnet2 \n    --resource-type vpnConnection \n    --storage-account examrefstorage \n    --storage-path https://examrefstorage.blob.core.windows.net/logs \n    --output json\n</code></pre>"},{"location":"Cloud/Azure/Azure-VPN/#_1","title":"Azure VPN","text":"<p>AZ-103: 395</p> Create a Site-to-Site VPN <p>Sources</p> <ul> <li>VPN Gateway design</li> <li>Connect Azure VPN gateways to multiple on-premises policy-based VPN devices</li> <li>About VPN Gateway configuration settings</li> <li>Highly available cross-premises and VNet-to-VNet connectivity</li> <li>ExpressRoute connectivity models</li> <li>Connect a computer to a virtual network using P2S and RADIUS authentication: PowerShell</li> </ul>"},{"location":"Cloud/Azure/Kusto/","title":"Kusto","text":"<p>In Kusto documentation <code>T</code> typically refers to the Table being queried:</p> <pre><code>T\n| where Predicate\n</code></pre> <p><code>&lt;&gt;</code> is equivalent to <code>!=</code> <pre><code>SecurityEvent\n| where Level &lt;&gt; 8\n| where EventID==4672\n</code></pre></p> <p>Select columns to include, rename, or drop</p> <pre><code>T\n| project\n    X=C,                       // Rename column C to X\n    A=2*B,                     // Calculate a new column A from the old B\n    C=strcat(\"-\",tostring(C)), // Calculate a new column C from the old C\n    B=2*B                      // Calculate a new column B from the old B\n</code></pre> <pre><code>StormEvents\n| extend label = case (\n  DamageProperty &lt; 1000, \"Storm\",\n  DamageProperty &gt; 1000 and DamageProperty &lt; 10000, \"Disaster\",\n  \"Catastrophe\" )\n| summarize count() by label\n</code></pre> <p>The SecurityEvent table provided as part of the [Log Analytics][Log Analytics] workspace trainingg dataset contains event viewer logs typical of what a security analyst would analyze, with the following columns:</p> <ul> <li>TimeGenerated</li> <li>Account</li> <li>AccountType (Machine or User)</li> <li>Computer</li> <li>EventSourceName</li> <li>Channel</li> <li>CommandLine</li> </ul> <p>Find logons, producing number of logins per Computer for computers with names beginning with \"App\"</p> <pre><code>SecurityEvent\n| where TimeGenerated between (ago(14d)..ago(7d))\n| where EventID == 4624\n| where Computer startswith \"App\"\n| summarize count() by Computer\n</code></pre> <pre><code>SecurityEvent\n| where EventID == 4688\n| summarize count() by CommandLine, Computer\n</code></pre> id title director year length_minutes 1 Toy Story John Lasseter 1995 81 2 A Bug's Life John Lasseter 1998 95 3 Toy Story 2 John Lasseter 1999 93 4 Monsters, Inc. Pete Docter 2001 92 5 Finding Nemo Andrew Stanton 2003 107 6 The Incredibles Brad Bird 2004 116 7 Cars John Lasseter 2006 117 8 Ratatouille Brad Bird 2007 115 9 WALL-E Andrew Stanton 2008 104 10 Up Pete Docter 2009 101 11 Toy Story 3 Lee Unkrich 2010 103 12 Cars 2 John Lasseter 2011 120 13 Brave Brenda Chapman 2012 102 14 Monsters University Dan Scanlon 2013 110 <p>Find the title of each film</p> <pre><code>Movies \n| project title\n</code></pre> <p>Number of reporting computers each hour <sup>AZ-103: 53</sup></p> <pre><code>Heartbeat \n| summarize dcount(ComputerIP) by bin(TimeGenerated, 1h) \n| render timechart\n</code></pre> <p>List top 10 VMs with most error events over the past day <sup>MeasureUp</sup></p> <pre><code>Event\n| where (EventLevelName == \"Error\")\n| where (TimeGenerated &gt; ago(1days))\n| summarize ErrorCount = count() by Computer\n| top 10 by ErrorCount desc\n</code></pre> <p>Render a SQL query as KQL</p> <pre><code>EXPLAIN\nSELECT name FROM greeks;\n</code></pre> <p>Count instances of a value</p> <pre><code>movies | summarize movies_directed = count() by director\n</code></pre> <p>Create a new column dynamically from others</p> <pre><code>movies | extend age = 2020 - year | project name, age;\n</code></pre> <p>Hide secrets from the queries log</p> <pre><code>print h\"Hello world!\";\n.show \n</code></pre> <p>Kusto clusters can be provisioned and Kusto databases created and manipulated using both PowerShell and Azure CLI.</p> <p>The Azure CLI <code>kusto</code> module will not be supported after 01/01/2021.</p> <pre><code>az extension add -n kusto\n</code></pre> <p>Create cluster</p> Azure PowerShellAzure CLI <pre><code>New-AzKustoCluster -ResourceGroupName testrg -Name testnewkustocluster -Location 'East US' -SkuName Standard_D11_v2 -SkuTier Standard -EnableDoubleEncryption true\n</code></pre> <pre><code>az kusto cluster create --name --resource-group --sku\n</code></pre> <pre><code>az kusto database create\n</code></pre> <p>Create table</p> <p>Connect to database <pre><code>#connect cluster('jasper.eastus').database('test');\n</code></pre></p> <p>Create table <pre><code>.create table starships (Name:string, Registry:string, Class:string, Crew:int32)\n</code></pre></p> <p>Ingest data <pre><code>.ingest into table T h'https://raw.githubusercontent.com/jasper-zanjani/dogfood/master/csv/greeks.csv' with (ignoreFirstRecord=true)\n</code></pre> Alternatively, define a new <code>datatable</code> inline</p> <p><pre><code>let starships = datatable(Name:string,Class:string,Registry:string,Crew:int)\n[\n  \"USS Enterprise\", \"Constitution\",\"NCC-1701\",203,\n  \"USS Constitution\",\"Constitution\",\"NCC-1700\",204,\n  \"USS Defiant\",\"Defiant\",\"NX-74205\",50,\n  \"USS Voyager\",\"Intrepid\",\"NCC-74656\",141,\n  \"USS Enterprise\",\"Galaxy\",\"NCC-1701-D\",6000,\n  \"USS Reliant\",\"Miranda\",\"NCC-1864\",35\n];\n</code></pre> Search for a word</p> <p><pre><code>starships | search  \"enterprise\";\n</code></pre> <pre><code>search in (SecurityEvent) \"Cryptographic\"\n| take 10;\n</code></pre></p> <p>Datetime <sup></sup> values support a menagerie of functions</p> <pre><code>print datetime(2015-01-01) # 2015-01-01 00:00:00.0000\nprint format_datetime(datetime(2015-01-01), \"yyyy\") # 2015\n</code></pre> <p>Concatenate values from other columns.</p> <pre><code>StormEvents\n| project EpisodeId, where_storm = strcat(EventType, \" in \", State);\n</code></pre> <p>Export data <pre><code>.export async to sql ['dbo.StormEventTypeTable']\n</code></pre></p> <p>Sources</p> <ul> <li>Azure Data Explorer documentation <sup></sup></li> <li>How to start with Microsoft Azure Data Explorer  <sup></sup></li> <li>KQL quick reference <sup></sup></li> <li>SQL to Kusto cheat sheet  <sup></sup></li> <li>Kusto.Explorer <sup></sup></li> <li>Azure Sentinel webinar parts 1, 2  <sup></sup>, 3</li> <li>KQL syntax: <code>count</code>, <code>take</code></li> </ul>"},{"location":"Cloud/Azure/appservice/","title":"Appservice","text":"<p>Web applications must be organized under an App Service plan, which defines compute resources for a web app. (1)</p> <ol> <li> <p>Links</p> <ul> <li> <p>What are Azure App Service plans?</p> </li> <li> <p>Host a web application with Azure App Service</p> </li> </ul> <p>Pricing tiers</p> <ul> <li> <p>Free/Shared</p> <p>Uses a shared infrastructure with minimal storage. No options for deploying different staged versions, routing of traffic, or backups</p> </li> <li> <p>Basic</p> <p>Dedicated compute for app, including avaiilability of SSL and manual scaling of app instance number.</p> </li> <li> <p>Standard</p> <p>Daily backups, automatic scaling of app instances, deployment slots, and user routing with Traffic Manager</p> </li> <li> <p>Premium</p> <p>More frequent backups, increased storage, and greater number of deployment slots and instance scaling options.</p> </li> </ul> </li> </ol>"},{"location":"Cloud/Azure/appservice/#tasks","title":"Tasks","text":"<ul> <li>Static website hosting in Azure Storage: actually appears to deal with mostly Azure storage account</li> </ul> <pre><code>az appservice plan create -g $RG_NAME -n $p\n    --is-linux\n\naz webapp list-runtimes --linux\n\naz webapp create -n $n -g $RG_NAME \n    --plan $p\n</code></pre> Deploy from GitHub<pre><code>az appservice plan create -g $rg -n $p\n    --sku FREE\n\naz webapp create -g $rg -n $webappname \n    --plan $p\n\naz webapp deployment source config -g $rg -n $webappname \n    --repo-url $gitrepo \n    --branch master \n    --manual-integration\n\n# Web app will be available at http://$webappname.azurewebsites.net\n</code></pre> Deploy image from Azure Container Registry<pre><code>az webapp create -g $RG_NAME -n $n\n    --plan $p \n    --deployment-container-image-name $registry.azurecr.io/$image:latest\n\naz webapp config appsettings set -g $RG_NAME -n $n \n    --settings \"WEBSITES_PORT=8000\"\n\n# Service principal ID\n$p = az webapp identity assign -g $RG_NAME -n $n -o tsv\n    --query principalId \n\n$s = az account show -o tsv\n    --query id \n\n# Grant web app permission to access the container registry\naz role assignment create \n    --assignee $p \n    --scope /subscriptions/$s/resourceGroups/$g/provides/Microsoft.ContainerRegistry/registries/$registry\n\n# Deploy image\naz webapp config container set -g $RG_NAME -n $n \n    --docker-custom-image-name $registry.azurecr.io/$image:latest \n    --docker-registry-server-url https://$registry.azurecr.io\n\naz webapp restart -n $n -g $g\n</code></pre>"},{"location":"Cloud/Azure/entra-id/","title":"Entra id","text":""},{"location":"Cloud/Azure/entra-id/#entra-id","title":"Entra ID","text":"<p>Entra ID (formerly Azure Active Directory) has its own set of roles which apply to Azure AD resources and which are distinct from those of Azure RBAC.</p> <p>A tenant refers to an instance of Azure AD that is tied to a subscription, and refers to the organization. Each tenant is associated with a dedicated and trusted directory that includes the tenant's users, groups, and apps.</p> <p>Roles:</p>"},{"location":"Cloud/Azure/entra-id/#licenses","title":"Licenses","text":"<ul> <li>Microsoft Entra ID Free</li> <li>Microsoft Entra ID P1</li> <li>Microsoft Entra ID P2 <ul> <li>Microsoft Entra ID Protection</li> <li>Privileged Identity Management</li> </ul> </li> </ul> <p>Note: The user to be licensed must first have a Usage location set.</p> <p>Use the ISO 3166-1 A2 two-letter country or region code to set this value in PowerShell</p> <pre><code>Set-AzureADUser -UsageLocation 'US'\n</code></pre> <ul> <li>Assign or remove licenses in the Azure Active Directory Portal</li> <li>Configure Microsoft 365 user account properties with PowerShell</li> </ul>"},{"location":"Cloud/Azure/entra-id/#roles","title":"Roles","text":"<ul> <li> <p>Global Administrator can manage access to administrative features in AAD and can grant administrator roles to other users. An AAD Global Administrator can also temporarily elevate their own access to the Azure RBAC role of User Access Administrator in order to manage all Azure subscriptions and management groups. Whoever signs up for the directory is automatically assigned this role.</p> </li> <li> <p>Device administrator</p> </li> </ul>"},{"location":"Cloud/Azure/entra-id/#features","title":"Features","text":"<ul> <li>Enterprise State Roaming allows users to securely synchronize user settings and application settings to Azure.<ul> <li>required for AD users to be able to change their password on-prem or in the clooud.</li> </ul> </li> </ul>"},{"location":"Cloud/Azure/entra-id/#tasks","title":"Tasks","text":"Self-service password reset <p>Self-Service Password Reset (SSPR) is supported for all users.  SSPR registration can be configured by group or for all domain users, but not individual users.</p> <p>Administrator accounts are treated differently from other user accounts for SSPR and have a \"strong default two-gate password reset policy\", which requires two pieces of authentication data and foregoes the use of security questions.</p> Bulk user creation <p>Users can be imported or created in bulk using CSV files.</p> Joining a device <p>When you join a device to an Azure AD tenant's domain, Azure AD creates local administrator accounts on the device for:</p> <ul> <li>The user joining the device</li> <li>The Azure AD global administrator</li> <li>The Azure AD device administrator</li> </ul> Enable MFA <p>User sign-in can be secured with Microsoft Entra multifactor authentication.</p> <ul> <li>Create a Conditional Access policy to enforce MFA with specified users.</li> </ul> <ul> <li>Tutorial: Secure user sign-in events with Azure Multi-Factor Authentication</li> <li>How to manage the local administrators group on Azure AD joined devices</li> <li>Password policies and account restrictions in Azure AD</li> </ul>"},{"location":"Cloud/Azure/entra-id/#b2b","title":"B2B","text":"<p>Business-to-business (B2B) collaboration allows you to invite guest users into your own (What is guest user access in Azure Active Directory B2B?)</p>"},{"location":"Cloud/IBM/","title":"IBM Cloud","text":"<p>IBM Cloud CLI</p> accountiamisloginpipluginregionsresourcesltarget Config Links <pre><code># Get details of current account\nibmcloud account show\n\n# List accounts\nibmcloud account list\n</code></pre> <pre><code># Create an API key, saved to the current working directory\nibmcloud iam api-key-create $NAME --file $FILENAME --action-if-leaked DELETE\n\n# List API keys\nibmcloud iam api-keys\n</code></pre> <p>Virtual Private Cloud infrastructure service</p> subnetvpc <pre><code># List subnets\nibmcloud is subnets\n\n# Create subnet\nibmcloud is subnet-create\n</code></pre> <pre><code>ibmcloud is vpcs\n\nibmcloud is vpc-create --address-prefix-management auto\n\n# List address prefixes of a VPC\nibmcloud is vpc-addrs $VPC\n</code></pre> <pre><code>ibmcloud login\n\n# Logging in via one-time passcode is available from within the management portal\nibmcloud login -a https://cloud.ibm.com -u passcode -p wP9SrQx476\n</code></pre> iamimginsvolws <pre><code># Create an API key, saved to the current working directory\nibmcloud iam api-key-create $NAME --file $FILENAME --action-if-leaked DELETE\n\n# List API keys\nibmcloud iam api-keys\n</code></pre> <pre><code># List images in a workspace\nibmcloud pi img ls\n\n# List images available in the regional image catalog\nibmcloud pi img lc\n</code></pre> <pre><code># List instances (note that a workspace target must be set)\nibmcloud pi instance ls\n\n# Create an instance\n...\n</code></pre> Virtual machine operations<pre><code>\n</code></pre> <pre><code># List all storage volumes in a workspace\nibmcloud pi vol ls\n\n# Create a storage volume\n# Default values:\n#   --storage-pool: General-Flash-8\n#   --storage-tier: tier3\nibmcloud pi vol cr --size 1\n\n# Delete a storage volume\nibmcloud pi vol del $VOLUME_ID\n</code></pre> Related<pre><code># List storage pools for the targeted region\nibmcloud pi spools\n\n# List storage tiers for the targeted region\nibmcloud pi stiers\n</code></pre> <pre><code># List Power Virtual Server workspaces\nibmcloud pi ws ls\n\n# Create a workspace\nibmcloud pi ws cr -d $DATACENTER -p public -g $RG_ID # (1)\n\n# Extract workspace ID of first displayed workspace\nWORKSPACE_ID=$(ibmcloud pi ws ls --json | jq '.Payload.workspaces.[0].id' -r)\n\n# Get details of a workspace\nibmcloud pi ws get $WORKSPACE_ID\n\n# Target a workspace (requires a Cloud Resource Name (2) )\nibmcloud pi ws tg $WORKSPACE_CRN\n\n# Delete a workspace\nibmcloud pi ws del $WORKSPACE_ID\n</code></pre> <ol> <li> <p>This command requires the ID, not name, of the resource group:</p> <pre><code># Display ID of a named resource group\nibmcloud resource group $RG --id\n</code></pre> datacenter<pre><code># List datacenters\nibmcloud pi dat ls\n</code></pre> </li> <li> <p>For some commands, a cloud ID is not accepted but only a Cloud Resource Name (CRN).     These can be found from within the Resource List page of the IBM Cloud console.</p> </li> </ol> <pre><code># Install IBM Power Virtual Server CLI plug-in\nibmcloud plugin install power-iaas\n\n# Install the Classic Infrastructure Services (CIS) plugin\nibmcloud plugin install sl\n\n# Confirm\nibmcloud plugin show power-iaas\n</code></pre> <pre><code># List available regions\nibmcloud regions\n</code></pre> <pre><code># List resource groups\nibmcloud resource groups\n\n# Show details of a resource group\nibmcloud resource group $GROUP\n\n# Create a resource group\nibmcloud resource group-create $GROUP\n</code></pre> <pre><code>\n</code></pre> <pre><code># View current account and region\nibmcloud target\n</code></pre> <p>Configuration data for the ibmcloud CLI application is at $HOME/.bluemix/config.json (IBM Cloud used to be known as Bluemix). Plugins are also installed under this directory.</p> <ul> <li> <p>IBM Cloud CLI Quick Reference (pdf)</p> </li> <li> <p>Documentation</p> </li> </ul>"},{"location":"Cloud/IBM/#terraform-provider","title":"Terraform provider","text":"<p>The  provider for Terraform.</p> <pre><code>terraform {\n    required_providers {\n        ibm = {\n            source = \"IBM-Cloud/ibm\"\n        }\n    }\n}\n\nvariable \"ibmcloud_api_key\" {}\n\nprovider \"ibm\" {\n    ibmcloud_api_key = var.ibmcloud_api_key\n    region = \"us-east\"\n}\n</code></pre>"},{"location":"Cloud/IBM/#tasks","title":"Tasks","text":"API keys<pre><code># Create an API key, saved to the current working directory\nibmcloud iam api-key-create $NAME --file $FILENAME --action-if-leaked DELETE\n\n# List API keys\nibmcloud iam api-keys\n</code></pre> <p>Creating an instance requires:</p> <ul> <li>Image ID</li> <li>Subnet ID</li> <li>Volume ID</li> <li>System type (\"s922\", \"s1022\", \"e980\", or \"e1080\")</li> </ul> <pre><code>IMAGE_ID=$(ibmcloud pi images ls --json | jq -r '.images.[0].imageID')\nSUBNET_ID=$(ibmcloud pi snet ls --json | jq -r '.networks.[0].networkID')\n</code></pre> <p>Other values have given defaults:</p> <ul> <li>Memory: 2 GB</li> <li>Processor type and number: 1 dedicated core</li> <li>Storage tier: \"tier3\"</li> </ul> Virtual machine operations<pre><code>\n</code></pre>"},{"location":"Kubernetes/","title":"Kubernetes","text":"<p>kubectl</p> <p>Kubernetes (Greek for \"helmsman\", \"pilot\", or \"captain\" and \"k8s\" for short) has emerged as the leading container orchestrator in the industry since 2018.  It provides a layer that abstracts infrastructure, including computers, networks, and other computers, for applications deployed on top.</p> <p>Kubernetes can be visualized as a system built from layers, with each higher layer abstracting the complexity of the lower levels. One server serves as the master, exposing an API for users and clients, assigning workloads, and orchestrating communication between other components. The processes on the master node are also called the control plane. Other machines in the cluster are called nodes or workers and accept and run workloads using available resources.  A Kubernetes configuration files is called a kubeconfig.</p> <p>A pod is the most atomic unit of work which encompasses one or more tightly-coupled containers that will be deployed together on the same node. All containers in a pod share the same Linux namespace, hostname, IP address, network interfaces, and port space. This means containers in a pod can communicate with each other over localhost, although care must be taken to ensure individual containers do not attempt to use the same port. However their filesystems are isolated from one another unless they share a volume.</p> <p>Every Pod occupies one address in a shared range, so communication between Pods is simple.</p> <ul> <li>replica: an instance of a pod</li> <li>service: an abstraction over a logical set of Pods and a policy by which to access them, i.e. a microservice. Because Pods are mortal, the Service controller keeps track of Pod addresses and publishes this information to the consumers of Services, a function called service discovery.</li> <li>worker see node</li> </ul>"},{"location":"Kubernetes/#object","title":"Object","text":"<p>Every persistent entity which represents the state of the Kubernetes system is called an object. Objects describe the desired state of the cluster's workload, and the Kubernetes constantly strives to achieve and maintain desired state.</p> <p>Almost all Kubernetes objects contain two nested fields: spec, which describes desired state, and status, which describes current state. Every object type in the Kubernetes API has a controller (i.e. deployment controller, etc.) that reads desired state from the spec and reports its actual state by writing to the Status section. Spec is most often provided by means of a YAML-format file known as a manifest.</p> <p>Resource and object are often used interchangeably, but more precisely the resource refers to the URL path that points to the object, and an object may be accessible through multiple resources.</p> <p>Links</p> <ul> <li>Kubernetes Spec Explorer</li> </ul>"},{"location":"Kubernetes/#concepts","title":"Concepts","text":""},{"location":"Kubernetes/#configuration","title":"Configuration","text":"<p>Kubeconfig files allows kubectl to choose a cluster and communicate with its API server They are conventionally stored at ~/.kube/config, but alternative locations can be specified using the $KUBECONFIG environment variable and the --kubeconfig option.</p> <pre><code># View merged kubeconfig settings\nkubectl config view\n</code></pre> <p>Within a kubeconfig, a context element is used to group access parameters under a convenient name.</p> <pre><code>\n</code></pre>"},{"location":"Kubernetes/#namespaces","title":"Namespaces","text":"<p>Kubernetes namespaces provide a mechanism for logically grouping and isolating resources within a cluster.</p> <p>Various namespaces exist by default, called initial namespaces. (1)</p> <ol> <li> <ul> <li>default</li> <li>kube-node-lease</li> <li>kube-public</li> <li>kube-system</li> </ul> </li> </ol>"},{"location":"Kubernetes/#history","title":"History","text":"<p>Kubernetes was first announced by Google in mid-2014.  It had been developed by Google after deciding to open-source the Borg system, a cluster and container management system that formed the automation infrastructure that powered the entire Google enterprise. Kubernetes coalesced from a fusion between developers working on Borg and Google Compute Engine.  Borg eventually evolved into Omega.</p> <p>By that time, Amazon had established a market advantage and the developers decided to change their approach by introducing a disruptive technology to drive the relevance of the Compute platform they had built.  They created a ubiquitous abstraction that could run better than anyone else.</p> <p>At the time, Google had been trying to engage the Linux kernel team and trying to overcome their skepticism.  Internally, the project was framed as offering \"Borg as a Service\", although there were concerns that Google was in danger of revealing trade secrets.</p> <p>Google ultimately donated Kubernetes to the Cloud Native Computing Foundation.</p> <p>Kubernetes's heptagonal logo is an allusion to when it was called \"Project 7\" as a reference to Star Trek's Borg character 7 of 9.</p>"},{"location":"Kubernetes/#manifest","title":"Manifest","text":"<p>A manifest is a (usually) YAML-format file that represents the declarative configuration of a Kubernetes object (although it can also be in JSON, and the kubectl utility will convert the YAML to JSON when executing API calls).</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2 # tells deployment to run 2 pods matching the template\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre> <p>A manifest contains various fields:</p> <ul> <li>apiVersion usually \"v1\"</li> <li>kind type of object</li> <li>metadata including name</li> <li>spec: desired state</li> </ul> <p>An explanation of each field available in the API of any object type can be displayed on the command-line</p> <pre><code>kubectl explain nodes\nkubectl explain no.spec\n</code></pre> Display the manifest of a node<pre><code>kubectl get node $NODE -o yaml\nkubectl describe node kind-worker-2\n</code></pre> <p>Compute resources of containers can be limited at pod.spec.containers.resources.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    resources:\n    requests:\n      memory: \"64Mi\"\n      cpu: \"500m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre> <p>Volumes are declared in .spec.volumes and mounted into containers in .spec.containers[*].volumeMounts.</p> emptyDirhostPathgcePersistentDisk <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: alpine\nspec:\nvolumes:\n- name: data\n  emptyDir:\ncontainers:\n- name: alpine\n  image: alpine\n  volumeMounts:\n  - mountPath: \"/data\"\n    name: \"data\"\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: alpine\nspec:\nvolumes:\n    - name: data\n    hostPath:\n        path: /var/data\n\ncontainers:\n- name: alpine\n    image: alpine\n    volumeMounts:\n    - mountPath: \"/data\"\n        name: \"data\"\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: alpine\nspec:\nvolumes:\n    - name: data\n    gcePersistentDisk:\n        pdName: my-disk\n        fsType: ext4\ncontainers:\n- name: alpine\n    image: alpine\n    volumeMounts:\n    - mountPath: \"/data\"\n        name: \"data\"\n</code></pre>"},{"location":"Kubernetes/#probes","title":"Probes","text":"<p>Kubernetes can monitor Pod health by using probes, which can be categorized by how they measure health:</p> <ul> <li>Readiness: i.e. Is the container ready to serve user requests?</li> <li>Liveness: i.e. Is the container running as intended?</li> </ul> <p>In Kubernetes, Volumes are an abstraction of file systems accessible from within a Pod's containers.</p> <ul> <li>Network storage device, such as gcePersistentVolume</li> <li>emptyDir, where the data is stored in RAM using Docker's tmpfs file system</li> <li>hostPath, where the volume is located within the node's file system. Because pods are expected to be created and destroyed on any node (which may themselves be destroyed and recreated), hostPath volumes are discommended.</li> </ul>"},{"location":"Kubernetes/#selector","title":"Selector","text":"<p>A label selector provides a way to identify a set of objects and is the core grouping primitive supported by Kubernetes. It can be made of multiple requirements that are comma-separated, all of which must be satisfied.</p> <p>There are two types of selector:</p> <ul> <li>Equality-based  admits the operators =, !=, and ==.</li> <li>Set-based  admits the operators in, notin, and exists.</li> </ul> Equality-based selectorSet-based selector <pre><code>environment = production\ntier != frontend\n</code></pre> <pre><code>environment in (production, qa)\ntier notin (frontend, backend)\npartition\n!partition\n</code></pre>"},{"location":"Kubernetes/helm/","title":"Helm","text":"<pre><code>helm repo add opsmx https://helmcharts.opsmx.com/\n\n# Confirm\nhelm repo ls\n\n# Install helm chart\nhelm install my-hello-kubernetes opsmx/hello-kubernetes --version 1.0.3\n\n# Confirm\nhelm list\n\n# New pods, deployments, and services\nkubectl get pods\nkubectl get deployments\nkubectl get services\n</code></pre>"},{"location":"Terraform/","title":"Terraform","text":"<p>terraform</p> Commands<pre><code># Download modules specified in provider block\nterraform init\n\n# Validate configuration\nterraform validate\n\n# Dry-run\nterraform plan\n\n# Create resources\nterraform apply\n\n# Skip approval prompt with auto-approve\nterraform apply -auto-approve\n\n# Remove resources\nterraform destroy\n</code></pre> <p>Terraform configurations are written most commonly in HCL, or less commonly in JSON.</p> Syntax<pre><code>block_type \"label\" \"name_label\" {\n    key = \"value\"\n    nested_block {\n        key = \"value\"\n    }\n}\n</code></pre> Hello, World!<pre><code>resource \"local_file\" \"hello-world\" {\n  filename     = \"message\"\n  content  = \"Hello, World!\"\n}\n</code></pre> <p>The most basic Terraform file uses an API key to connect to the provider service. Many providers are available for Terraform, but most of them are cloud-based.</p> IBM Cloud<pre><code>terraform {\n    required_providers {\n        ibm = {\n            source = \"IBM-Cloud/ibm\"\n        }\n    }\n}\n\nvariable \"ibmcloud_api_key\" {}\n\nprovider \"ibm\" {\n    ibmcloud_api_key = var.ibmcloud_api_key\n    region = \"us-east\"\n}\n</code></pre> Azure<pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"&gt;4\"\n    }\n  }\n}\n\nvariable \"azure_subscription_id\" {}\n\nprovider \"azurerm\" {\n  subscription_id = var.azure_subscription_id\n}\n</code></pre> <p>Variables allow values to be substituted without altering a module's source code. Variables are declared in a variable block (1) and values can be provided in various ways:</p> <ol> <li> Variable definition<pre><code>variable \"name_label\" {\n    type = value\n    description = \"string\"\n    default = value\n    sensitive = bool\n}\n</code></pre> </li> </ol> <ul> <li> <p>Variable definition files using -var-file. Files named terraform.tfvars and .auto.tfvars are automatically parsed.</p> </li> <li> <p>Environment variables prefixed with <code>TF_VAR_</code></p> </li> <li> <p>From the command-line using -var</p> </li> </ul> <p>Placing sensitive values in variable definition files allows sensitive data, such as passwords, API keys, or tokens, to be kept secret.</p> <p>Terraform configurations mostly contain three types of object:</p> <ul> <li>Providers provide information about a provider plugin, i.e. AWS. There are various tiers of providers: official, partner, and community.  </li> <li>Resources are what is created within a target environment, i.e. a VM or a database</li> <li>Data sources are ways to query information from a provider.</li> </ul>"},{"location":"Terraform/#provisioners","title":"Provisioners","text":"<p>Provisioners model specific actions in order to prepare servers or other infrastructure objects for service. Provisioners are described as a last resort in Terraform documentation because their actions cannot be modeled as part of a plan.</p> <p>local-exec invokes a local executable.</p> <pre><code>provisioner \"local-exec\" {\n    command = \"echo ${self.private_ip} &gt;&gt; private_ips.txt\"\n}\n</code></pre> <p>There is also a null resource provisioner</p>"},{"location":"Terraform/#outputs","title":"Outputs","text":"<p>Output values make information about infrastructure available on the command-line. Outputs are displayed after a configuration run and are also stored in the state data.</p> <pre><code>terraform output region\n\n# Omit quotes and final newline, outputting only the string value itself (useful for scripting) (1)\nterraform output -raw region\n</code></pre> <ol> <li><pre><code># Add new context to .kube/config\naws eks update-kubeconfig \\\n    --region $(terraform output -raw region) \\\n    --name $(terraform output -raw cluster_name)\n</code></pre></li> </ol>"},{"location":"Terraform/Tasks/AWS/create-user/","title":"Create user","text":"<pre><code>provider \"aws\" { }\n\nresource \"aws_iam_user\" \"bob\" {\n  name = \"Bob\"\n}\n\nresource \"aws_iam_group\" \"admins\" {\n  name = \"Admins\"\n  path = \"/Groups/\"\n}\n\nresource \"aws_iam_group_policy_attachment\" \"adminaccess-attach\" {\n    group = aws_iam_group.admins.name\n    policy_arn =\"arn:aws:iam::aws:policy/AdministratorAccess\"\n}\n\nresource \"aws_iam_user_group_membership\" \"bob-admins\" {\n    user = \"Bob\"\n    groups = [\n        aws_iam_group.admins.name\n    ]\n}\n</code></pre>"},{"location":"Terraform/Tasks/AWS/deploy-vm/","title":"Deploy VM","text":"<pre><code># Use environment variables or a credentials file to store credentials\n\nvariable \"secret_key\" { }\n\nprovider \"aws\" {\n    access_key = \"AKIAZ2G54UZXCEIHB3Y4\"\n    secret_key = var.secret_key\n    region = \"us-east-1\"\n}\n\ndata \"aws_ssm_parameter\" \"amzn2_linux\" {\n    name= \"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\"\n}\n\nresource \"aws_vpc\" \"app\" {\n    cidr_block = \"10.0.0.0/16\"\n    enable_dns_hostnames = true\n}\n\nresource \"aws_internet_gateway\" \"app\" {\n    vpc_id = aws_vpc.app.id\n}\n\nresource \"aws_subnet\" \"public_subnet1\" {\n    cidr_block = \"10.0.0.0/24\"\n    vpc_id = aws_vpc.app.id\n    map_public_ip_on_launch = true\n}\n\nresource \"aws_route_table\" \"app\" {\n    vpc_id = aws_vpc.app.id\n\n    route {\n        cidr_block = \"0.0.0.0/10\"\n        gateway_id = aws_internet_gateway.app.id\n    }\n}\n\nresource \"aws_route_table_association\" \"app_subnet1\" {\n    subnet_id = aws_subnet.public_subnet1.id\n    route_table_id = aws_route_table.app.id\n}\n\nresource \"aws_security_group\" \"nginx_sg\" {\n    name = \"nginx_sg\"\n    vpc_id = aws_vpc.app.id\n\n    ingress {\n        from_port = 80\n        to_port = 80\n        protocol = \"tcp\"\n        cidr_blocks = [\"0.0.0.0/0\"]\n    }\n\n    egress {\n        from_port = 0\n        to_port = 0\n        protocol = \"-1\"\n        cidr_blocks = [\"0.0.0.0/0\"]\n    }\n}\n\nresource \"aws_instance\" \"nginx1\" {\n    ami = nonsensitive(data.aws_ssm_parameter.amzn2_linux.value)\n    instance_type = \"t2.micro\"\n    subnet_id = aws_subnet.public_subnet1.id\n    vpc_security_group_ids = [aws_security_group.nginx_sg.id]\n\n    user_data = &lt;&lt;EOF\n#! /bin/bash\nsudo amazon-linux-extras install -y nginx1\nsudo service nginx start\nsudo rm /usr/share/nginx/html/index.html\necho '&lt;html&gt;&lt;body&gt;&lt;h1&gt;Taco Team Server&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\"\nEOF\n\n}\n</code></pre>"},{"location":"Terraform/Tasks/AWS/eks/","title":"EKS","text":"main.tf<pre><code>provider \"aws\" {\n  region = var.region\n}\n\n# Filter out local zones, which are not currently supported \n# with managed node groups\ndata \"aws_availability_zones\" \"available\" {\n  filter {\n    name   = \"opt-in-status\"\n    values = [\"opt-in-not-required\"]\n  }\n}\n\nlocals {\n  cluster_name = \"awsdemo\"\n}\n\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"5.1.2\"\n\n  name = \"kube-vpc\"\n\n  cidr = \"10.0.0.0/16\"\n  azs  = slice(data.aws_availability_zones.available.names, 0, 3)\n\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n\n  enable_nat_gateway   = true\n  single_nat_gateway   = true\n  enable_dns_hostnames = true\n\n  public_subnet_tags = {\n    \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/elb\"                      = 1\n  }\n\n  private_subnet_tags = {\n    \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/internal-elb\"             = 1\n  }\n}\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"19.16.0\"\n\n  cluster_name    = local.cluster_name\n  cluster_version = \"1.27\"\n  cluster_endpoint_public_access = true\n\n  vpc_id                         = module.vpc.vpc_id\n  subnet_ids                     = module.vpc.private_subnets\n\n  eks_managed_node_group_defaults = {\n    ami_type = \"AL2_x86_64\"\n  }\n\n  eks_managed_node_groups = {\n    one = {\n      name = \"node-group-1\"\n\n      instance_types = [\"t3.small\"]\n\n      min_size     = 1\n      max_size     = 3\n      desired_size = 2\n    }\n\n    two = {\n      name = \"node-group-2\"\n\n      instance_types = [\"t3.medium\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 1\n    }\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", local.cluster_name]\n      command     = \"aws\"\n    }\n  }\n}\n\nresource \"helm_release\" \"hello_kubernetes\" {\n  name       = \"my-hello-kubernetes\"\n  repository = \"https://helmcharts.opsmx.com/\"\n  chart      = \"hello-kubernetes\"\n}\n</code></pre> terraform.tf<pre><code>terraform {\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.7.0\"\n    }\n    helm = {\n      source = \"hashicorp/helm\"\n      version = \"~&gt; 2.11.0\"\n    }\n  }\n  required_version = \"~&gt; 1.3\"\n}\n</code></pre> outputs.tf<pre><code>output \"region\" {\n  description = \"AWS region\"\n  value       = var.region\n}\n\noutput \"cluster_name\" {\n  description = \"Kubernetes Cluster Name\"\n  value       = module.eks.cluster_name\n}\n</code></pre> variables.tf<pre><code>variable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n</code></pre>"},{"location":"Terraform/Tasks/Azure/00-provider/","title":"provider","text":"<pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"&gt;4\"\n    }\n  }\n}\n\nvariable \"azure_subscription_id\" {}\n\nprovider \"azurerm\" {\n  subscription_id = var.azure_subscription_id\n}\n</code></pre>"},{"location":"Terraform/Tasks/Azure/01-group/","title":"azurerm_resource_group","text":"<pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"&gt;4\"\n    }\n  }\n}\n\nvariable \"azure_subscription_id\" {}\n\nprovider \"azurerm\" {\n  subscription_id = var.azure_subscription_id\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"rg-tf\" {\n  name = \"rg-tf\"\n  location = \"eastus\"\n}\n</code></pre>"},{"location":"Terraform/Tasks/Azure/10-vnet/","title":"azurerm_virtual_network","text":"<ul> <li><code>azurerm_virtual_network</code> resource</li> </ul> Deploy a vnet and three associated subnets in the 10.0.0.0/24 address space<pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"&gt;4\"\n    }\n  }\n}\n\nvariable \"azure_subscription_id\" {}\n\nprovider \"azurerm\" {\n  subscription_id = var.azure_subscription_id\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"rg-tf\" {\n  name = \"rg-tf\"\n  location = \"eastus\"\n}\n\nresource \"azurerm_virtual_network\" \"vnet-tf\" {\n  name = \"vnet-tf\"\n  resource_group_name = azurerm_resource_group.rg-tf.name\n  location = azurerm_resource_group.rg-tf.location\n  address_space = [\"10.0.0.0/16\"]\n\n  subnet {\n    name = \"sn0\"\n    address_prefixes = [\"10.0.0.0/24\"]\n  }\n  subnet {\n    name = \"sn1\"\n    address_prefixes = [\"10.0.1.0/24\"]\n  }\n  subnet {\n    name = \"sn2\"\n    address_prefixes = [\"10.0.2.0/24\"]\n  }\n}\n</code></pre>"},{"location":"Terraform/Tasks/Basics/","title":"Hello, World!","text":""},{"location":"Terraform/Tasks/IBM/","title":"IBM Cloud tasks","text":"<pre><code>terraform {\n    required_providers {\n        ibm = {\n            source = \"IBM-Cloud/ibm\"\n        }\n    }\n}\n\nvariable \"ibmcloud_api_key\" {}\n\nprovider \"ibm\" {\n    ibmcloud_api_key = var.ibmcloud_api_key\n    region = \"us-east\"\n}\n</code></pre>"}]}